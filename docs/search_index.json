[["index.html", "Help! What Statistical Model Should I Use? 2024 Workshop Info Schedule Pre-work", " Help! What Statistical Model Should I Use? 2024 Faculty: Patrick McMillan, Alysha Cooper, Jacqueline May August 19, 2024 - August 21, 2024 Workshop Info Welcome to the 2024 Help! What Statistical Model Should I Use? Canadian Bioinformatics Workshop webpage! Schedule Time (Eastern) August 19 Time (Eastern) August 20 Time (Eastern) August 21 8:30 Arrivals &amp; Check-in 8:30 Arrivals 8:30 Arrivals 9:00 Welcome (Nia Hughes) 9:00 Module 3: Modeling Part 1 (Alysha Cooper) 9:00 Module 5: Putting it all Together (Patrick McMillan) 9:30 Module 1: Data cleaning and exploration review (Dr. Jacqueline May) 10:00 Break (30 min) 10:00 Break (30 min) 10:30 Break (30 min) 10:30 Module 3 Lab: Modeling Part 1 (Alysha Cooper) 10:30 Module 5 Lab: Putting it all Together (Patrick McMillan) 11:00 Module 1 Lab: Data cleaning and exploration review (Dr. Jacqueline May) 12:30 Class photo + Lunch (1h) 12:30 Lunch (1h) 13:00 Lunch (1h) 13:30 Module 4: Modeling Part 2 (Patrick McMillan) 13:30 Module 6: Causal Inference Introduction (Alysha Cooper) 14:00 Module 2: Dealing with Missingness (Dr. Jacqueline May) 14:30 Break (30 min) 14:30 Break (30 min) 15:00 Break (30 min) 15:00 Module 4 Lab: Modeling Part 2 (Patrick McMillan) 15:00 Module 6 Lab: Causal Inference Introduction (Alysha Cooper) 15:30 Module 2 Lab: Dealing with Missingness (Dr. Jacqueline May) 17:00 17:00 Survey &amp; Closing Remarks 17:30 Finished 17:30 Finished 17:30 Finished Pre-work You can find your pre-work here. "],["meet-your-faculty.html", "Meet Your Faculty", " Meet Your Faculty "],["data-and-compute-setup.html", "Data and Compute Setup", " Data and Compute Setup Course data downloads Coming soon! Compute setup Coming soon! "],["module-1-data-cleaning-and-exploration-review.html", "Module 1: Data Cleaning and Exploration Review Lecture Lab", " Module 1: Data Cleaning and Exploration Review Lecture Lab Load Libraries —- To install a package: install.packages(&quot;tidyverse&quot;) ## ## The downloaded binary packages are in ## /var/folders/f6/jq59skk16rnfmjt0_j71lg_80000gp/T//RtmpQnb4ft/downloaded_packages To load a library: library(tidyverse) library(rfishbase) library(factoextra) Read in Data —- Get your current working directory. getwd() Download csv file from this link: https://doi.org/10.5061/dryad.f6t39kj Place it into your current working directory. Read in data. dfReptiles &lt;- read.csv(&#39;Appendix S1 - Lizard data version 1.0.csv&#39;) Looking at your data. —- # What class is it? class(dfReptiles) ## [1] &quot;data.frame&quot; # How many rows and columns? dim(dfReptiles) ## [1] 6662 50 # Look at the column names. colnames(dfReptiles) ## [1] &quot;Binomial&quot; ## [2] &quot;Genus&quot; ## [3] &quot;epithet&quot; ## [4] &quot;valid..reptile.database..February.2018.&quot; ## [5] &quot;year.of.description&quot; ## [6] &quot;country.described.from&quot; ## [7] &quot;main.biogeographic.Realm&quot; ## [8] &quot;Geographic.Range&quot; ## [9] &quot;known.only.from.the.only.type.&quot; ## [10] &quot;Latitude.centroid..from.Roll.et.al..2017.&quot; ## [11] &quot;Longitude.centroid..from.Roll.et.al..2017.&quot; ## [12] &quot;insular.endemic&quot; ## [13] &quot;maximum.SVL&quot; ## [14] &quot;female.SVL&quot; ## [15] &quot;hatchling.neonate.SVL&quot; ## [16] &quot;Leg.development&quot; ## [17] &quot;mass_equation..Feldman.et.al..2016.unless.stated.&quot; ## [18] &quot;intercept&quot; ## [19] &quot;slope&quot; ## [20] &quot;Activity.time&quot; ## [21] &quot;Activity.time..comments&quot; ## [22] &quot;substrate&quot; ## [23] &quot;substrate..comments&quot; ## [24] &quot;diet&quot; ## [25] &quot;diet..comments&quot; ## [26] &quot;foraging.mode&quot; ## [27] &quot;foraging.mode..comments.&quot; ## [28] &quot;reproductive.mode&quot; ## [29] &quot;clutch.size&quot; ## [30] &quot;smallest.clutch&quot; ## [31] &quot;largest.clutch&quot; ## [32] &quot;smallest.mean.clutch.size&quot; ## [33] &quot;largest.mean.clutch.size&quot; ## [34] &quot;breeding.age..months.&quot; ## [35] &quot;youngest.age.at.first.breeding..months.&quot; ## [36] &quot;oldest.age.at.first.breeding..months.&quot; ## [37] &quot;mean.body.temperature.of.active.animals.in.the.wild&quot; ## [38] &quot;minimum.mean.Tb&quot; ## [39] &quot;maximum.mean.Tb&quot; ## [40] &quot;Family&quot; ## [41] &quot;Phylogeny&quot; ## [42] &quot;phylogenetic.data&quot; ## [43] &quot;IUCN.redlist.assessment&quot; ## [44] &quot;IUCN.population.trend&quot; ## [45] &quot;Extant.Extinct&quot; ## [46] &quot;Remarks&quot; ## [47] &quot;References..Biology..all.columns.except.M..N.and.O..&quot; ## [48] &quot;References..SVL.of.unsexed.individuals..neonates.and.hatchlings&quot; ## [49] &quot;References..SVL.of.females&quot; ## [50] &quot;References..SVL.of.males&quot; names(dfReptiles)[1:10] ## [1] &quot;Binomial&quot; ## [2] &quot;Genus&quot; ## [3] &quot;epithet&quot; ## [4] &quot;valid..reptile.database..February.2018.&quot; ## [5] &quot;year.of.description&quot; ## [6] &quot;country.described.from&quot; ## [7] &quot;main.biogeographic.Realm&quot; ## [8] &quot;Geographic.Range&quot; ## [9] &quot;known.only.from.the.only.type.&quot; ## [10] &quot;Latitude.centroid..from.Roll.et.al..2017.&quot; # If you want to view your dataset. head(dfReptiles, 10) ## Binomial Genus epithet ## 1 Ablepharus bivittatus Ablepharus bivittatus ## 2 Ablepharus budaki Ablepharus budaki ## 3 Ablepharus chernovi Ablepharus chernovi ## 4 Ablepharus darvazi Ablepharus darvazi ## 5 Ablepharus deserti Ablepharus deserti ## 6 Ablepharus grayanus Ablepharus grayanus ## 7 Ablepharus kitaibelii Ablepharus kitaibelii ## 8 Ablepharus lindbergi Ablepharus lindbergi ## 9 Ablepharus pannonicus Ablepharus pannonicus ## 10 Ablepharus rueppellii Ablepharus rueppellii ## valid..reptile.database..February.2018. year.of.description ## 1 yes 1832 ## 2 yes 1996 ## 3 yes 1953 ## 4 yes 1990 ## 5 yes 1868 ## 6 yes 1872 ## 7 yes 1833 ## 8 yes 1960 ## 9 yes 1824 ## 10 yes 1839 ## country.described.from main.biogeographic.Realm ## 1 Azerbaijan Palearctic ## 2 Turkey Palearctic ## 3 Armenia Palearctic ## 4 Tajikistan Palearctic ## 5 Uzbekistan Palearctic ## 6 India Oriental ## 7 Greece Palearctic ## 8 Afghanistan Palearctic ## 9 Uzbekistan Palearctic ## 10 Jordan Palearctic ## Geographic.Range ## 1 Armenia, Azerbaijan, Turkmenistan, Iran, Turkey ## 2 Cyprus, Lebanon, Syria, Turkey ## 3 Armenia, Turkey ## 4 Tajikistan ## 5 Kazakhstan to Tadjikistan ## 6 India, Pakistan, Iran ## 7 Greece (Aegean islands: Paros, Antiparos, Despotiko, Strongylo, Tourlos, Preza, Glaropunta, Panteronisi, Cyprus, Rhodos, Peloponnes, Syphnos, Corfu, Lesbos, Samos, Samothraki, Milos, Tinos), Romania, Bulgaria, Yugoslavia, Hungary, Albania, Czechoslovakia, Russia, Turkey, Syria, Iraq ?, Egypt (Sinai), Lebanon? ## 8 Afghanistan ## 9 Georgia, Turkmenistan, Tajikistan, Uzbekistan, Kyrgyzstan, Azerbaijan, Iran, Iraq, Oman, Afghanistan, Pakistan, Jordan?, Syria, United Arab Emirates (UAE), India ## 10 Egypt, Israel, Jordan, Lebanon, Syria, ## known.only.from.the.only.type. Latitude.centroid..from.Roll.et.al..2017. ## 1 no 34.16 ## 2 no 36.44 ## 3 no 38.32 ## 4 no 38.96 ## 5 no 41.38 ## 6 no 27.09 ## 7 no 41.11 ## 8 no 34.28 ## 9 no 33.16 ## 10 no 31.39 ## Longitude.centroid..from.Roll.et.al..2017. insular.endemic maximum.SVL ## 1 53.20 no 61 ## 2 34.06 no 48 ## 3 37.95 no 54 ## 4 69.79 no 44 ## 5 68.40 no 58.8 ## 6 70.93 no 34.9 ## 7 25.77 no 58 ## 8 66.28 no 50 ## 9 58.67 no 55 ## 10 35.28 no 52 ## female.SVL hatchling.neonate.SVL Leg.development ## 1 56.5 18.5 four-legged ## 2 36.8 &lt;NA&gt; four-legged ## 3 44.4 &lt;NA&gt; four-legged ## 4 &lt;NA&gt; &lt;NA&gt; four-legged ## 5 46 17.5 four-legged ## 6 &lt;NA&gt; 14.5 four-legged ## 7 43.4 20 four-legged ## 8 &lt;NA&gt; &lt;NA&gt; four-legged ## 9 &lt;NA&gt; 20 four-legged ## 10 35.3 18.5 four-legged ## mass_equation..Feldman.et.al..2016.unless.stated. intercept slope ## 1 Legged Scincidae -5.125 3.229 ## 2 Legged Scincidae -5.125 3.229 ## 3 Legged Scincidae -5.125 3.229 ## 4 Legged Scincidae -5.125 3.229 ## 5 Legged Scincidae -5.125 3.229 ## 6 Legged Scincidae -5.125 3.229 ## 7 Legged Scincidae -5.125 3.229 ## 8 Legged Scincidae -5.125 3.229 ## 9 Legged Scincidae -5.125 3.229 ## 10 Legged Scincidae -5.125 3.229 ## Activity.time ## 1 Diurnal ## 2 Diurnal ## 3 Diurnal ## 4 Diurnal ## 5 Diurnal ## 6 Diurnal ## 7 Diurnal ## 8 Diurnal ## 9 Diurnal ## 10 Diurnal ## Activity.time..comments ## 1 Diurnal ## 2 Diurnal ## 3 Diurnal ## 4 Diurnal (Philipp Wagner Personal communication to Enav Vidan, October 2015) ## 5 Diurnal (Jablonski 2016 and inferred from Szczerbak 2003) ## 6 Diurnal (e.g., Karamiani et al. 2018) ## 7 Diurnal (e.g., pers. obs., Speybroeck et al. 2016, Stille and Stille 2017)/ crepuscular (Valakos et al. 2008) ## 8 Diurnal (Nasrullah Rastegar-Pouyani Personal communication to Enav Vidan, October 2015) ## 9 Diurnal (e.g., Karamiani et al. 2018) ## 10 Diurnal (e.g., pers obs. And Werner 2016) ## substrate substrate..comments ## 1 Saxicolous Saxicolous ## 2 Terrestrial Leaf litter ## 3 Saxicolous Saxicolous ## 4 &lt;NA&gt; &lt;NA&gt; ## 5 Terrestrial Terrestrial (Szczerbak, Jablonski 2016) ## 6 Terrestrial Terrestrial and Leaf Litter (e.g., Greer 1973) ## 7 Terrestrial Leaf Litter (e.g., pers. pbs. And Vergilov and Natchev 2017) ## 8 &lt;NA&gt; &lt;NA&gt; ## 9 Terrestrial Leaf Litter ## 10 Terrestrial Leaf Litter (e.g., pers. obs. And Werner 2016) ## diet ## 1 Carnivorous ## 2 Carnivorous ## 3 Carnivorous ## 4 &lt;NA&gt; ## 5 Carnivorous ## 6 Carnivorous ## 7 Carnivorous ## 8 &lt;NA&gt; ## 9 Carnivorous ## 10 Carnivorous ## diet..comments ## 1 Invertebrates (&quot;Food consists mainly of insects (beetles, hymenopterans, and cicadas), spiders, and snails&quot;, Szczerbak 2003) ## 2 Invertebrates ## 3 Arthropods (&quot;ants, flies, aphids, small beetles, and spiders.&quot;, Szczerbak 2003) ## 4 &lt;NA&gt; ## 5 Arthropods (&quot;beetles, caterpillars, and cicadas) and spiders&quot; Szczerbak 2003) ## 6 Insects (Mostly ants) ## 7 Invertebrates (spiders and earthworms, Street 1979, Rogner 1997b), small invertebrates (Stille and Stille 2017) ## 8 &lt;NA&gt; ## 9 Arthropods (beetles and spiders, Szczerbak 2003) ## 10 arthropods (Amitai and Bouskila 2001) ## foraging.mode foraging.mode..comments. reproductive.mode ## 1 &lt;NA&gt; &lt;NA&gt; Oviparous ## 2 &lt;NA&gt; &lt;NA&gt; Oviparous ## 3 &lt;NA&gt; &lt;NA&gt; Oviparous ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; Oviparous ## 6 &lt;NA&gt; &lt;NA&gt; Oviparous ## 7 &lt;NA&gt; &lt;NA&gt; Oviparous ## 8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 &lt;NA&gt; &lt;NA&gt; Oviparous ## 10 &lt;NA&gt; &lt;NA&gt; Oviparous ## clutch.size ## 1 3-5, usually 4 (Arakelyan et al. 2011), 4-5 (Szczerbak 2003) ## 2 &lt;NA&gt; ## 3 3 (n=1, Szczerbak 2003) ## 4 &lt;NA&gt; ## 5 1-8 (Szczerbak 2003) ## 6 &lt;NA&gt; ## 7 2-4 (Stojanov et al. 2011, Speybroeck et al. 2016, Kwet 2015), up to 4 (Stille and Stille 2017), max 5 (Vergilov et al. 2018) ## 8 4.5 (Myhrvold et al. 2015) ## 9 2-3 (Ataev et al. 1994), 3-4 (Gardner 2013), 3-6 (Szczerbak 2003) ## 10 up to 3 (Bar and Haimovitch 2012), 1-2, mean 1.5 (Goldberg 2013, TAU experimental zoo data), mean 2.1 (Werner 1995), 1-3, mean 2.1 (Werner 2016) ## smallest.clutch largest.clutch smallest.mean.clutch.size ## 1 2 5 4.0 ## 2 2 5 NA ## 3 3 4 NA ## 4 NA NA NA ## 5 1 8 NA ## 6 1 2 NA ## 7 2 6 NA ## 8 4 5 4.5 ## 9 2 6 NA ## 10 1 6 1.5 ## largest.mean.clutch.size ## 1 4.0 ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA ## 7 NA ## 8 4.5 ## 9 NA ## 10 2.1 ## breeding.age..months. ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 Szczerbak 2003: 10 months ## 6 &lt;NA&gt; ## 7 6 month for 35mm which is min adult size, fig 2 (Vergilov et al. 2018) ## 8 &lt;NA&gt; ## 9 Ataev et al. 1994: 10-11 months, Szczerbak 2003: 1 year ## 10 &lt;NA&gt; ## youngest.age.at.first.breeding..months. ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 10 ## 6 NA ## 7 6 ## 8 NA ## 9 10 ## 10 NA ## oldest.age.at.first.breeding..months. ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 10 ## 6 NA ## 7 6 ## 8 NA ## 9 12 ## 10 NA ## mean.body.temperature.of.active.animals.in.the.wild ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; ## 7 Shai pers. obs. (n=1, 27 deg) ## 8 &lt;NA&gt; ## 9 &lt;NA&gt; ## 10 Roll et al. 2013 (25.6-34.8) but it should be 29.5, for n=5 active animals (Meiri, pers. Obs. 15.4.2014) ## minimum.mean.Tb maximum.mean.Tb Family ## 1 NA NA Scincidae ## 2 NA NA Scincidae ## 3 NA NA Scincidae ## 4 NA NA Scincidae ## 5 NA NA Scincidae ## 6 NA NA Scincidae ## 7 27.0 27.0 Scincidae ## 8 NA NA Scincidae ## 9 NA NA Scincidae ## 10 29.5 29.5 Scincidae ## Phylogeny ## 1 &lt;NA&gt; ## 2 Pyron and Burbrink 2014, Skourtanioti et al. 2016 ## 3 Pyron and Burbrink 2014, Skourtanioti et al. 2016 ## 4 &lt;NA&gt; ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; ## 7 Pyron and Burbrink 2014, Skourtanioti et al. 2016 ## 8 &lt;NA&gt; ## 9 Pyron and Burbrink 2014, Skourtanioti et al. 2016 ## 10 Skourtanioti et al. 2016 ## phylogenetic.data IUCN.redlist.assessment ## 1 &lt;NA&gt; LC ## 2 2mt and 2 nuclear genes (Skourtanioti et al. 2016) LC ## 3 2mt and 2 nuclear genes (Skourtanioti et al. 2016) LC ## 4 &lt;NA&gt; DD ## 5 &lt;NA&gt; LC ## 6 &lt;NA&gt; NE ## 7 2mt and 2 nuclear genes (Skourtanioti et al. 2016) LC ## 8 &lt;NA&gt; LC ## 9 2mt and 2 nuclear genes (Skourtanioti et al. 2016) NE ## 10 2mt and 2 nuclear genes (Skourtanioti et al. 2016) LC ## IUCN.population.trend Extant.Extinct ## 1 decreasing extant ## 2 stable extant ## 3 stable extant ## 4 unknown extant ## 5 unknown extant ## 6 NE extant ## 7 stable extant ## 8 unknown extant ## 9 NE extant ## 10 unknown extant ## Remarks ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; ## 7 &lt;NA&gt; ## 8 &lt;NA&gt; ## 9 &lt;NA&gt; ## 10 elevational data (0-1660): Meiri, own data ## References..Biology..all.columns.except.M..N.and.O.. ## 1 Szczerbak 2003, Anderson 1999, Baran and Atatur 1998, Clark 1990, Arakelyan et al. 2011 ## 2 Gocmen et al. 1996, Baier et al. 2009, Schmidtler 1997, Franzen et al. 2008 ## 3 Szczerbak 2003, Baran and Atatur 1998, Schmidtler 1997, Arakelyan et al. 2011 ## 4 &lt;NA&gt; ## 5 Szczerbak 2003, Jablonski 2016 ## 6 Minton 1966, Tikader and Sharma 1992, Greene 1982, Khan 2006, Greer 1973, Karamiani et al. 2018 ## 7 Arnold and Ovenden 2004, Baran and Atatur 1998, Street 1979, Rogner 1997b, Atatur and Gocmen 2001, Herczeg et al. 2007, Valakos et al. 2008, Valakos et al. 2004, Arbel 1984, Kwet 2009, Kohler 2005, Schmidtler 1997, Stojanov et al. 2011, Tomovic et al. 2015, Speybroeck et al. 2016, Kwet 2015, Vergilov et al. 2016, Vergilov and Natchev 2017, Stille and Stille 2017, Vergilov et al. 2018 ## 8 &lt;NA&gt; ## 9 Smith 1935, Szczerbak 2003, Anderson 1999, Arnold 1972, Khan 2006, Clark 1990, Jongbloed 2000, Eremchenko 2007, Weber 1960, van der Kooij 2001, Fathinia et al. 2009, Grossmann et al. 2012, Ataev et al. 1994, Gardner 2013, Karamiani et al. 2018 ## 10 Amitai and Bouskila 2001, Arbel 1984, Disi et al. 2001, El Din 2006, Bar and Haimovitch 2012, Roll et al. 2013, Goldberg 2013, Werner 1995, Werner 2016, TAU experimental zoo data ## References..SVL.of.unsexed.individuals..neonates.and.hatchlings ## 1 Szczerbak 2003, Boulenger 1887, Arakelyan et al. 2011, ## 2 Gocmen et al. 1996, Franzen et al. 2008, ## 3 Szczerbak 2003, Arakelyan et al. 2011, ## 4 &lt;NA&gt; ## 5 Szczerbak 2003, Boulenger 1887, ## 6 Smith 1935, Minton 1966, Tikader and Sharma 1992, Boulenger 1890, Boulenger 1887, Khan 2006, Karamiani et al. 2015, Ali et al. 2017, ## 7 Arnold and Ovenden 2004,Herczeg et al. 2007, Foufopoulos and Ives 1999, Franzen et al. 2008, Stojanov et al. 2011, Vergilov et al. 2018 ## 8 Wettstein 1960, ## 9 Smith 1935, Szczerbak 2003, Anderson 1999, Minton 1966, Boulenger 1890, Boulenger 1887, Khan 2006, Jongbloed 2000, Arnold 1986, Werner 1930, Gardner 2013, Karamiani et al. 2015, ## 10 Meiri (own measurements), Maza 2008, TAU Herpetology collection, El Din 2006, Roll et al. 2013, ## References..SVL.of.females ## 1 Anderson 1999, Ahmadzadeh et al. 2008, Arakelyan et al. 2011, ## 2 Gocmen et al. 1996, Budak et al. 1998, Baier et al. 2009, Schmidtler 1997, ## 3 Schmidtler 1997, ## 4 &lt;NA&gt; ## 5 Jablonski 2016, ## 6 &lt;NA&gt; ## 7 Fitch 1981, Arnold and Ovenden 2004, Disi et al. 2001, El Din 2006, Schmidtler 1997, Stojanov et al. 2011, Vergilov et al. 2018 ## 8 &lt;NA&gt; ## 9 &lt;NA&gt; ## 10 Disi et al. 2001, Schmidtler 1997, Goldberg 2012, TAUM collection, Werner 1995, Werner 2016, ## References..SVL.of.males ## 1 Anderson 1999, ## 2 Gocmen et al. 1996, Budak et al. 1998, Baier et al. 2009, Schmidtler 1997, ## 3 Schmidtler 1997, ## 4 Yeriomchenko and Panfilov 1990, ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; ## 7 Fitch 1981, Arnold and Ovenden 2004, Disi et al. 2001, Schmidtler 1997, Stojanov et al. 2011, Vergilov et al. 2018 ## 8 &lt;NA&gt; ## 9 Fathinia et al. 2009, ## 10 Disi et al. 2001, Schmidtler 1997, Goldberg 2012, # We have a lot of columns. Let&#39;s subset to remove columns we don&#39;t need for now. dfTraits &lt;- dfReptiles %&gt;% ## tidyverse pipe # Select function allows us to choose the columns we want select(c(Binomial, Genus, Family, main.biogeographic.Realm, Latitude.centroid..from.Roll.et.al..2017., insular.endemic, maximum.SVL, hatchling.neonate.SVL, Leg.development, Activity.time, substrate, diet, foraging.mode, reproductive.mode, smallest.clutch, largest.clutch, youngest.age.at.first.breeding..months., IUCN.redlist.assessment, IUCN.population.trend, Extant.Extinct )) names(dfTraits) ## [1] &quot;Binomial&quot; ## [2] &quot;Genus&quot; ## [3] &quot;Family&quot; ## [4] &quot;main.biogeographic.Realm&quot; ## [5] &quot;Latitude.centroid..from.Roll.et.al..2017.&quot; ## [6] &quot;insular.endemic&quot; ## [7] &quot;maximum.SVL&quot; ## [8] &quot;hatchling.neonate.SVL&quot; ## [9] &quot;Leg.development&quot; ## [10] &quot;Activity.time&quot; ## [11] &quot;substrate&quot; ## [12] &quot;diet&quot; ## [13] &quot;foraging.mode&quot; ## [14] &quot;reproductive.mode&quot; ## [15] &quot;smallest.clutch&quot; ## [16] &quot;largest.clutch&quot; ## [17] &quot;youngest.age.at.first.breeding..months.&quot; ## [18] &quot;IUCN.redlist.assessment&quot; ## [19] &quot;IUCN.population.trend&quot; ## [20] &quot;Extant.Extinct&quot; # Let&#39;s clean up these column names! names(dfTraits) &lt;- tolower(names(dfTraits)) # Replace all &quot;.&quot; with &quot;_&quot; (personal preference) names(dfTraits) &lt;- gsub(&quot;\\\\.&quot;, &quot;_&quot;, names(dfTraits)) names(dfTraits) ## [1] &quot;binomial&quot; ## [2] &quot;genus&quot; ## [3] &quot;family&quot; ## [4] &quot;main_biogeographic_realm&quot; ## [5] &quot;latitude_centroid__from_roll_et_al__2017_&quot; ## [6] &quot;insular_endemic&quot; ## [7] &quot;maximum_svl&quot; ## [8] &quot;hatchling_neonate_svl&quot; ## [9] &quot;leg_development&quot; ## [10] &quot;activity_time&quot; ## [11] &quot;substrate&quot; ## [12] &quot;diet&quot; ## [13] &quot;foraging_mode&quot; ## [14] &quot;reproductive_mode&quot; ## [15] &quot;smallest_clutch&quot; ## [16] &quot;largest_clutch&quot; ## [17] &quot;youngest_age_at_first_breeding__months_&quot; ## [18] &quot;iucn_redlist_assessment&quot; ## [19] &quot;iucn_population_trend&quot; ## [20] &quot;extant_extinct&quot; # Make some of the names shorter. names(dfTraits)[5] &lt;- &quot;latitude&quot; names(dfTraits)[17] &lt;- &quot;age_first_breeding&quot; names(dfTraits)[17] # Rename species column. names(dfTraits)[1] &lt;- &quot;species&quot; # In order to properly count the number of missing values, replace blanks with NAs # I always do this just in case. dfTraits[dfTraits == &quot; &quot;] &lt;- NA # Make sure there are no species name duplications. # Note that using the sum() function on logical vector will count the number of TRUE values. duplicated(dfTraits$species) sum(duplicated(dfTraits$species)) # Are there any species that don&#39;t have ANY data? missRows &lt;- apply(dfTraits[, -(1:3)], MARGIN = 1, function(x) all(is.na(x))) # Let&#39;s break that down! First, we ed if &quot;x&quot; row had NAs. is.na(dfTraits[1, ]) ## logical vector ## species genus family main_biogeographic_realm latitude insular_endemic ## 1 FALSE FALSE FALSE FALSE FALSE FALSE ## maximum_svl hatchling_neonate_svl leg_development activity_time substrate ## 1 FALSE FALSE FALSE FALSE FALSE ## diet foraging_mode reproductive_mode smallest_clutch largest_clutch ## 1 FALSE TRUE FALSE FALSE FALSE ## age_first_breeding iucn_redlist_assessment iucn_population_trend ## 1 TRUE FALSE FALSE ## extant_extinct ## 1 FALSE # Then we ed if they were ALL NAs (it is ing if all of the elements are NA) dfTraits[1, ] ## species genus family main_biogeographic_realm latitude ## 1 Ablepharus bivittatus Ablepharus Scincidae Palearctic 34.16 ## insular_endemic maximum_svl hatchling_neonate_svl leg_development ## 1 no 61 18.5 four-legged ## activity_time substrate diet foraging_mode reproductive_mode ## 1 Diurnal Saxicolous Carnivorous &lt;NA&gt; Oviparous ## smallest_clutch largest_clutch age_first_breeding iucn_redlist_assessment ## 1 2 5 NA LC ## iucn_population_trend extant_extinct ## 1 decreasing extant all() ## [1] TRUE # Note you could use the &quot;any&quot; function to if ANY of the elements are NA any() ## There are a couple NAs ## [1] FALSE # Then we &quot;apply&quot; that function to all of the rows (MARGIN = 1) in the dataframe which returns: missRows sum(missRows) ## [1] 0 # If you wanted to remove species without any trait data from the dataframe: dfTraits &lt;- dfTraits[!missRows, ] Working with different data types. —- Let’s try to understand the type of data we are working with. names(dfTraits) ## [1] &quot;species&quot; &quot;genus&quot; ## [3] &quot;family&quot; &quot;main_biogeographic_realm&quot; ## [5] &quot;latitude&quot; &quot;insular_endemic&quot; ## [7] &quot;maximum_svl&quot; &quot;hatchling_neonate_svl&quot; ## [9] &quot;leg_development&quot; &quot;activity_time&quot; ## [11] &quot;substrate&quot; &quot;diet&quot; ## [13] &quot;foraging_mode&quot; &quot;reproductive_mode&quot; ## [15] &quot;smallest_clutch&quot; &quot;largest_clutch&quot; ## [17] &quot;age_first_breeding&quot; &quot;iucn_redlist_assessment&quot; ## [19] &quot;iucn_population_trend&quot; &quot;extant_extinct&quot; # First, let&#39;s ID which columns are taxonomic information so we don&#39;t include them in our summary stats. taxCols &lt;- c(&quot;species&quot;, &quot;genus&quot;, &quot;family&quot;) # The rest are traits traits &lt;- setdiff(names(dfTraits), taxCols) traits ## [1] &quot;main_biogeographic_realm&quot; &quot;latitude&quot; ## [3] &quot;insular_endemic&quot; &quot;maximum_svl&quot; ## [5] &quot;hatchling_neonate_svl&quot; &quot;leg_development&quot; ## [7] &quot;activity_time&quot; &quot;substrate&quot; ## [9] &quot;diet&quot; &quot;foraging_mode&quot; ## [11] &quot;reproductive_mode&quot; &quot;smallest_clutch&quot; ## [13] &quot;largest_clutch&quot; &quot;age_first_breeding&quot; ## [15] &quot;iucn_redlist_assessment&quot; &quot;iucn_population_trend&quot; ## [17] &quot;extant_extinct&quot; # the classes of the traits. sapply(dfTraits[traits], class) ## main_biogeographic_realm latitude insular_endemic ## &quot;character&quot; &quot;numeric&quot; &quot;character&quot; ## maximum_svl hatchling_neonate_svl leg_development ## &quot;character&quot; &quot;character&quot; &quot;character&quot; ## activity_time substrate diet ## &quot;character&quot; &quot;character&quot; &quot;character&quot; ## foraging_mode reproductive_mode smallest_clutch ## &quot;character&quot; &quot;character&quot; &quot;integer&quot; ## largest_clutch age_first_breeding iucn_redlist_assessment ## &quot;integer&quot; &quot;numeric&quot; &quot;character&quot; ## iucn_population_trend extant_extinct ## &quot;character&quot; &quot;character&quot; # Let&#39;s ID which traits are numerical and which are categorical. index &lt;- sapply(dfTraits[traits], is.numeric) index ## main_biogeographic_realm latitude insular_endemic ## FALSE TRUE FALSE ## maximum_svl hatchling_neonate_svl leg_development ## FALSE FALSE FALSE ## activity_time substrate diet ## FALSE FALSE FALSE ## foraging_mode reproductive_mode smallest_clutch ## FALSE FALSE TRUE ## largest_clutch age_first_breeding iucn_redlist_assessment ## TRUE TRUE FALSE ## iucn_population_trend extant_extinct ## FALSE FALSE # Wait! Something doesn&#39;t seem right..! class(dfTraits$maximum_svl) ## [1] &quot;character&quot; head(dfTraits) ## species genus family main_biogeographic_realm latitude ## 1 Ablepharus bivittatus Ablepharus Scincidae Palearctic 34.16 ## 2 Ablepharus budaki Ablepharus Scincidae Palearctic 36.44 ## 3 Ablepharus chernovi Ablepharus Scincidae Palearctic 38.32 ## 4 Ablepharus darvazi Ablepharus Scincidae Palearctic 38.96 ## 5 Ablepharus deserti Ablepharus Scincidae Palearctic 41.38 ## 6 Ablepharus grayanus Ablepharus Scincidae Oriental 27.09 ## insular_endemic maximum_svl hatchling_neonate_svl leg_development ## 1 no 61 18.5 four-legged ## 2 no 48 &lt;NA&gt; four-legged ## 3 no 54 &lt;NA&gt; four-legged ## 4 no 44 &lt;NA&gt; four-legged ## 5 no 58.8 17.5 four-legged ## 6 no 34.9 14.5 four-legged ## activity_time substrate diet foraging_mode reproductive_mode ## 1 Diurnal Saxicolous Carnivorous &lt;NA&gt; Oviparous ## 2 Diurnal Terrestrial Carnivorous &lt;NA&gt; Oviparous ## 3 Diurnal Saxicolous Carnivorous &lt;NA&gt; Oviparous ## 4 Diurnal &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 Diurnal Terrestrial Carnivorous &lt;NA&gt; Oviparous ## 6 Diurnal Terrestrial Carnivorous &lt;NA&gt; Oviparous ## smallest_clutch largest_clutch age_first_breeding iucn_redlist_assessment ## 1 2 5 NA LC ## 2 2 5 NA LC ## 3 3 4 NA LC ## 4 NA NA NA DD ## 5 1 8 10 LC ## 6 1 2 NA NE ## iucn_population_trend extant_extinct ## 1 decreasing extant ## 2 stable extant ## 3 stable extant ## 4 unknown extant ## 5 unknown extant ## 6 NE extant # The column has strings mixed with numbers, which returns a character vector. # We should replace the strings with NAs. # Here, we are using regex to match any letter and then replacing the matches with NAs. dfTraits$maximum_svl &lt;- gsub(pattern = &quot;[a-zA-Z]&quot;, replacement = NA, x = dfTraits$maximum_svl) dfTraits$hatchling_neonate_svl &lt;- gsub(&quot;[a-zA-Z]&quot;, NA, dfTraits$hatchling_neonate_svl) # Change both traits to numeric. # Note here I am using lapply to apply the function for the columns of dfTraits. # Note: lapply returns a list, sapply returns a vector. &quot;map&quot; would be the tidyverse equivalent of apply, lapply, sapply, etc. functions. dfTraits[, c(&quot;maximum_svl&quot;, &quot;hatchling_neonate_svl&quot;)] &lt;- lapply(dfTraits[, c(&quot;maximum_svl&quot;, &quot;hatchling_neonate_svl&quot;)], as.numeric) class(dfTraits$maximum_svl) ## [1] &quot;numeric&quot; # Let&#39;s try IDing our numerical traits again. index &lt;- sapply(dfTraits[traits], is.numeric) # Subset the column names using indexing. contTraits &lt;- traits[index] contTraits ## is this right? ## [1] &quot;latitude&quot; &quot;maximum_svl&quot; &quot;hatchling_neonate_svl&quot; ## [4] &quot;smallest_clutch&quot; &quot;largest_clutch&quot; &quot;age_first_breeding&quot; # Get the categorical traits. catTraits &lt;- setdiff(traits, contTraits) # Convert character to factor because this is helpful for summary stats and plotting. # It is also the data class that regression models require for categorical variables. dfTraits[catTraits] &lt;- lapply(dfTraits[catTraits], as.factor) # If you wanted to make a particular category within a variable the reference. # Important for statistical analyses with categorical variables. table(dfTraits$insular_endemic) ## ## no unknown yes ## 5 4612 1 2044 dfTraits$insular_endemic &lt;- relevel(dfTraits$insular_endemic, &quot;no&quot;) # One last . sapply(dfTraits, class) ## species genus family ## &quot;character&quot; &quot;character&quot; &quot;character&quot; ## main_biogeographic_realm latitude insular_endemic ## &quot;factor&quot; &quot;numeric&quot; &quot;factor&quot; ## maximum_svl hatchling_neonate_svl leg_development ## &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; ## activity_time substrate diet ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; ## foraging_mode reproductive_mode smallest_clutch ## &quot;factor&quot; &quot;factor&quot; &quot;integer&quot; ## largest_clutch age_first_breeding iucn_redlist_assessment ## &quot;integer&quot; &quot;numeric&quot; &quot;factor&quot; ## iucn_population_trend extant_extinct ## &quot;factor&quot; &quot;factor&quot; Summary stats. —- Some base R summary statistics to get a quick look at your data. summary(dfTraits) ## species genus family ## Length:6662 Length:6662 Length:6662 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## main_biogeographic_realm latitude insular_endemic maximum_svl ## Neotropic :2041 Min. :-50.65000 no :4612 Min. : 17.0 ## Oriental :1167 1st Qu.:-17.89500 : 5 1st Qu.: 56.0 ## Afrotropic:1009 Median : 1.66000 unknown: 1 Median : 75.9 ## Australia : 772 Mean : 0.03201 yes :2044 Mean : 95.7 ## Palearctic: 555 3rd Qu.: 17.21500 3rd Qu.: 105.0 ## (Other) :1117 Max. : 56.60000 Max. :1570.0 ## NA&#39;s : 1 NA&#39;s :31 NA&#39;s :29 ## hatchling_neonate_svl leg_development activity_time ## Min. : 8.10 : 5 : 5 ## 1st Qu.: 23.00 forelimbs only: 10 Cathemeral: 268 ## Median : 28.00 four-legged :5992 Diurnal :3578 ## Mean : 33.62 hindlimbs only: 64 Nocturnal :1247 ## 3rd Qu.: 36.00 leg-reduced : 188 NA&#39;s :1564 ## Max. :174.50 Limbless : 403 ## NA&#39;s :4589 ## substrate diet foraging_mode ## Terrestrial :1750 : 5 : 5 ## Arboreal :1083 Carnivorous:2685 active foraging: 514 ## Saxicolous : 848 Herbivorous: 159 mixed : 96 ## Arboreal&amp;Terrestrial: 486 Omnivorous : 515 Sit and Wait : 460 ## Arboreal&amp;Saxicolous : 295 NA&#39;s :3298 NA&#39;s :5587 ## (Other) :1187 ## NA&#39;s :1013 ## reproductive_mode smallest_clutch largest_clutch age_first_breeding ## : 5 Min. : 1.000 Min. : 1.000 Min. : 1.00 ## Mixed : 20 1st Qu.: 1.000 1st Qu.: 2.000 1st Qu.: 9.00 ## Oviparous :3309 Median : 2.000 Median : 3.000 Median : 12.00 ## unclear : 11 Mean : 2.534 Mean : 5.675 Mean : 18.53 ## Viviparous: 714 3rd Qu.: 3.000 3rd Qu.: 7.000 3rd Qu.: 24.00 ## NA&#39;s :2603 Max. :34.000 Max. :95.000 Max. :144.00 ## NA&#39;s :3067 NA&#39;s :3067 NA&#39;s :5961 ## iucn_redlist_assessment iucn_population_trend extant_extinct ## NE :3021 : 5 : 5 ## LC :2148 decreasing: 565 EW : 2 ## DD : 453 increasing: 18 extant :6612 ## EN : 322 NE :3097 extinct: 43 ## VU : 278 stable :1372 ## NT : 253 unknown :1605 ## (Other): 187 Time for some tidyverse magic! Use of the tidyverse pipe avoids us having to create several interim variables while also improving readability. # Say we only wanted to keep extant species: dfTraits &lt;- dfTraits %&gt;% # Filter function allows us to apply a condition to our data. filter(extant_extinct == &quot;extant&quot;) %&gt;% # Remove column using minus sign as we no longer need it select(-extant_extinct) # Remove trait from catTraits. catTraits ## [1] &quot;main_biogeographic_realm&quot; &quot;insular_endemic&quot; ## [3] &quot;leg_development&quot; &quot;activity_time&quot; ## [5] &quot;substrate&quot; &quot;diet&quot; ## [7] &quot;foraging_mode&quot; &quot;reproductive_mode&quot; ## [9] &quot;iucn_redlist_assessment&quot; &quot;iucn_population_trend&quot; ## [11] &quot;extant_extinct&quot; catTraits &lt;- catTraits[-11] # Other summary info. # How many families do we have? unique(dfTraits$family) ## [1] &quot;Scincidae&quot; &quot;Anguidae&quot; &quot;Agamidae&quot; ## [4] &quot;Lacertidae&quot; &quot;Gymnophthalmidae&quot; &quot;Eublepharidae&quot; ## [7] &quot;Gekkonidae&quot; &quot;Trogonophiidae&quot; &quot;Diplodactylidae&quot; ## [10] &quot;Iguanidae&quot; &quot;Teiidae&quot; &quot;Amphisbaenidae&quot; ## [13] &quot;Dibamidae&quot; &quot;Leiosauridae&quot; &quot;Anniellidae&quot; ## [16] &quot;Dactyloidae&quot; &quot;Pygopodidae&quot; &quot;Chamaeleonidae&quot; ## [19] &quot;Sphaerodactylidae&quot; &quot;Phyllodactylidae&quot; &quot;Corytophanidae&quot; ## [22] &quot;Bipedidae&quot; &quot;Blanidae&quot; &quot;Gerrhosauridae&quot; ## [25] &quot;Cadeidae&quot; &quot;Phrynosomatidae&quot; &quot;Carphodactylidae&quot; ## [28] &quot;Opluridae&quot; &quot;Cordylidae&quot; &quot;Xantusiidae&quot; ## [31] &quot;Crotaphytidae&quot; &quot;Liolaemidae&quot; &quot;Hoplocercidae&quot; ## [34] &quot;Tropiduridae&quot; &quot;Helodermatidae&quot; &quot;Lanthanotidae&quot; ## [37] &quot;Leiocephalidae&quot; &quot;Polychrotidae&quot; &quot;Rhineuridae&quot; ## [40] &quot;Shinisauridae&quot; &quot;Varanidae&quot; &quot;Xenosauridae&quot; length(unique(dfTraits$family)) ## [1] 42 # Top 10 Families with most species. # You can sort by increasing or decreasing number of species. head(sort(table(dfTraits$family), decreasing = T), n = 10) ## ## Scincidae Gekkonidae Agamidae Dactyloidae ## 1622 1159 487 426 ## Lacertidae Liolaemidae Gymnophthalmidae Sphaerodactylidae ## 328 308 265 216 ## Chamaeleonidae Amphisbaenidae ## 207 175 # Doing the same thing, but with tidyverse syntax dfTraits %&gt;% dplyr::count(family, sort = T) %&gt;% ## to make sure the count function isn&#39;t masked head(n = 10) ## family n ## 1 Scincidae 1622 ## 2 Gekkonidae 1159 ## 3 Agamidae 487 ## 4 Dactyloidae 426 ## 5 Lacertidae 328 ## 6 Liolaemidae 308 ## 7 Gymnophthalmidae 265 ## 8 Sphaerodactylidae 216 ## 9 Chamaeleonidae 207 ## 10 Amphisbaenidae 175 # Sample size (number of complete observations for this trait) head(na.omit(dfTraits$maximum_svl)) ## na.omit removes NAs from the vector ## [1] 61.0 48.0 54.0 44.0 58.8 34.9 length(na.omit(dfTraits$maximum_svl)) ## [1] 6588 # Mean of the data mean(dfTraits$maximum_svl, na.rm = T) ## has option for removing number of NAs in the function ## [1] 95.35199 # Range of the data range(dfTraits$maximum_svl, na.rm = T) ## [1] 17 1570 # Proportion of NAs sum(is.na(dfTraits$maximum_svl)) ## [1] 24 sum(is.na(dfTraits$maximum_svl)) / length(dfTraits$maximum_svl) ## [1] 0.003629764 # To speed things up, (l)apply these to all of the numerical traits using an anonymous function. # An anonymous function is a function without a name that you really only need temporarily # e.g., within the confines of this lapply call. l_contInfo &lt;- lapply(dfTraits[contTraits], function(x){ # Number of complete observations n &lt;- length(na.omit(x)) # Mean avg &lt;- mean(x, na.rm = T) # Number of NAs numNAs &lt;- sum(is.na(x)) # Proportion of NAs propNAs &lt;- sum(is.na(x)) / length(x) # Return in dataframe format return(data.frame(n, avg, numNAs, propNAs)) }) # View the first element of the list. head(l_contInfo[[1]]) ## n avg numNAs propNAs ## 1 6587 -0.005732503 25 0.003781004 # Bind list of dataframes together by using the do.call() function. # This lets you rbind() the entire list of dataframes. dfContInfo &lt;- do.call(rbind, l_contInfo) head(dfContInfo) ## n avg numNAs propNAs ## latitude 6587 -0.005732503 25 0.003781004 ## maximum_svl 6588 95.351988464 24 0.003629764 ## hatchling_neonate_svl 2072 33.623262548 4540 0.686630369 ## smallest_clutch 3582 2.528475712 3030 0.458257713 ## largest_clutch 3582 5.681183696 3030 0.458257713 ## age_first_breeding 699 18.466666667 5913 0.894283122 # Do the same thing for the categorical data. lapply(dfTraits[catTraits], table) ## $main_biogeographic_realm ## ## Afrotropic Australia Madagascar Nearctic Neotropic Oceania ## 0 1004 772 332 226 2011 548 ## Oriental Palearctic ## 1164 554 ## ## $insular_endemic ## ## no unknown yes ## 4610 0 1 2001 ## ## $leg_development ## ## forelimbs only four-legged hindlimbs only leg-reduced ## 0 10 5949 63 187 ## Limbless ## 403 ## ## $activity_time ## ## Cathemeral Diurnal Nocturnal ## 0 267 3570 1243 ## ## $substrate ## ## Arboreal ## 0 1080 ## Arboreal&amp;Saxicolous Arboreal&amp;Saxicolous&amp;Terrestrial ## 294 203 ## Arboreal&amp;Terrestrial Cryptic ## 486 62 ## Cryptic&amp;Fossorial Cryptic&amp;Terrestrial ## 12 7 ## Fossorial Fossorial&amp;Saxicolous ## 254 1 ## Fossorial&amp;Saxicolous&amp;Terrestrial Fossorial&amp;Terrestrial ## 6 270 ## marine Saxicolous ## 1 847 ## Saxicolous&amp;Terrestrial Saxisolous ## 243 1 ## Semi_Aquatic Terrestrial ## 121 1742 ## ## $diet ## ## Carnivorous Herbivorous Omnivorous ## 0 2681 157 509 ## ## $foraging_mode ## ## active foraging mixed Sit and Wait ## 0 512 96 460 ## ## $reproductive_mode ## ## Mixed Oviparous unclear Viviparous ## 0 20 3304 11 707 ## ## $iucn_redlist_assessment ## ## CR DD EN EW EX LC LR/lc LR/nt NE NT VU ## 0 139 451 322 0 0 2148 4 8 3009 253 278 ## ## $iucn_population_trend ## ## decreasing increasing NE stable unknown ## 0 561 18 3070 1371 1592 l_catInfo &lt;- lapply(dfTraits[catTraits], function(x){ n &lt;- length(na.omit(x)) # number of unique categories instead of mean for example cats &lt;- length(unique(x)) numNAs &lt;- sum(is.na(x)) propNAs &lt;- sum(is.na(x)) / length(x) return(data.frame(n, cats, numNAs, propNAs)) }) # Bind together list. dfCatInfo &lt;- do.call(rbind, l_catInfo) head(dfCatInfo) ## n cats numNAs propNAs ## main_biogeographic_realm 6611 9 1 0.0001512402 ## insular_endemic 6612 3 0 0.0000000000 ## leg_development 6612 5 0 0.0000000000 ## activity_time 5080 4 1532 0.2316999395 ## substrate 5630 18 982 0.1485178463 ## diet 3347 4 3265 0.4937991531 # Tidyverse has handy functions for getting summary data by group. # For example, if we wanted to get summary information grouped by family: summary_stats1 &lt;- dfTraits %&gt;% # Group by family group_by(family) %&gt;% # Get the mean max SVL for each group and put it into a new column called avg_length summarize(avg_length = mean(maximum_svl, na.rm = T)) %&gt;% # Arrange in descending order arrange(desc(avg_length)) %&gt;% # Print to console print() ## # A tibble: 42 × 2 ## family avg_length ## &lt;chr&gt; &lt;dbl&gt; ## 1 Varanidae 472. ## 2 Helodermatidae 441 ## 3 Rhineuridae 380 ## 4 Iguanidae 364. ## 5 Cadeidae 270 ## 6 Amphisbaenidae 267. ## 7 Bipedidae 223 ## 8 Blanidae 220. ## 9 Lanthanotidae 220 ## 10 Corytophanidae 184. ## # ℹ 32 more rows # Let&#39;s add some info on other traits summary_stats2 &lt;- dfTraits %&gt;% group_by(family) %&gt;% summarize( avg_length = mean(maximum_svl, na.rm = T), # Average largest clutch avg_lc = mean(largest_clutch, na.rm = T), # Most common diet in each family top_diet = names(sort(table(diet), decreasing = T)[1]), ) %&gt;% print() ## # A tibble: 42 × 4 ## family avg_length avg_lc top_diet ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Agamidae 110. 8.98 Carnivorous ## 2 Amphisbaenidae 267. 4.22 Carnivorous ## 3 Anguidae 151. 12.2 Carnivorous ## 4 Anniellidae 156. 2.33 Carnivorous ## 5 Bipedidae 223 4 Carnivorous ## 6 Blanidae 220. 2 Carnivorous ## 7 Cadeidae 270 2 Carnivorous ## 8 Carphodactylidae 109. 2.04 Carnivorous ## 9 Chamaeleonidae 94.8 19.5 Carnivorous ## 10 Cordylidae 104. 4.08 Carnivorous ## # ℹ 32 more rows Exploratory plots. —- Let’s perform some data visualization to identify patterns and variable associations in our dataset. # Base R histograms for numerical traits hist(dfTraits$maximum_svl) ## there are some VERY long species! hist(dfTraits$latitude) # ggplot to beautify the data # ggplot is a very powerful tool for visualizing data but you need to get used to the syntax ggplot(dfTraits) + ## note use of &quot;+&quot; over &quot;%&gt;% # Plot a histogram and make it blue. # geom_* indicates what type of plot you want. aes = aesthetic mapping geom_histogram(mapping = aes(x = latitude), fill = &quot;skyblue&quot;, colour = &quot;black&quot;) + # Add labels labs(title = &quot;Reptile Latitude&quot;, x = &quot;Latitude (°)&quot;, y = &quot;Count&quot;) + # Change the plot theme (here, making the background white) theme_minimal(base_size = 12) + # Change the theme and make some font adjustments theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;)) # barplot for categorical traits plot(dfTraits$diet) # ggplot version # Get rid of those pesky NAs for plotting ggplot(data = dfTraits %&gt;% filter(!is.na(diet))) + # Barplot geom_bar(mapping = aes(x = diet, fill = diet), width = 0.7) + # Custom colours scale_fill_brewer(palette = &quot;Paired&quot;) + labs(title = &quot;Diet Types in Reptiles&quot;, x = &quot;Diet&quot;, y = &quot;Count&quot;) + theme_minimal(base_size = 12) + theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;), legend.position = &quot;none&quot;) ?scale_fill_brewer ## Lots of options, including colour blind friendly options # Relationships between numerical variables plot(dfTraits[contTraits]) ## scatter plots for each pair of traits # Correlations # Ranges from -1 to 1 and gives insight about the strength of pairwise relationships ?cor # Note that you have the option to use different coefficients (Pearson, Kendall, Spearman) cor(dfTraits[contTraits], use = &quot;pairwise.complete.obs&quot;) ## latitude maximum_svl hatchling_neonate_svl ## latitude 1.000000000 -0.008342921 -0.08613691 ## maximum_svl -0.008342921 1.000000000 0.85991210 ## hatchling_neonate_svl -0.086136910 0.859912101 1.00000000 ## smallest_clutch -0.003149907 0.243497717 0.16379773 ## largest_clutch 0.094788426 0.472278560 0.27042185 ## age_first_breeding -0.111217181 0.406792723 0.52252633 ## smallest_clutch largest_clutch age_first_breeding ## latitude -0.003149907 0.09478843 -0.11121718 ## maximum_svl 0.243497717 0.47227856 0.40679272 ## hatchling_neonate_svl 0.163797729 0.27042185 0.52252633 ## smallest_clutch 1.000000000 0.55075538 0.10747271 ## largest_clutch 0.550755384 1.00000000 0.06566284 ## age_first_breeding 0.107472714 0.06566284 1.00000000 # Test for significant association between two traits cor.test(dfTraits$hatchling_neonate_svl, dfTraits$age_first_breeding) ## ## Pearson&#39;s product-moment correlation ## ## data: dfTraits$hatchling_neonate_svl and dfTraits$age_first_breeding ## t = 14.785, df = 582, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4609332 0.5791094 ## sample estimates: ## cor ## 0.5225263 # The output tells us there is a statistically significant correlation (p-value &lt; 0.05) between these two variables. # Quick ggplot to see relationship ggplot(data = dfTraits) + geom_point(mapping = aes(x = age_first_breeding, y = hatchling_neonate_svl)) # Are these data normally distributed? We can this using QQ plots. hist(dfTraits$hatchling_neonate_svl) # The x-axis is the quantiles from the theoretical distribution we are comparing to (i.e., normal distribution) and y-axis is the quantiles from our data qqnorm(dfTraits$hatchling_neonate_svl) qqline(dfTraits$hatchling_neonate_svl) ## skewed distribution # Let&#39; see if we can use a log-transformation to make our data resemble a normal distribution hist(dfTraits$hatchling_neonate_svl) hist(log(dfTraits$hatchling_neonate_svl)) qqnorm(log(dfTraits$hatchling_neonate_svl)) qqline(log(dfTraits$hatchling_neonate_svl)) ## it helps! # If you wanted to keep the original data in your dataset, you could use the mutate function to create a new column with the log-transformed data dfTraits &lt;- dfTraits %&gt;% mutate(log_hatchling_neonate_svl = log(hatchling_neonate_svl), log_age_first_breeding = log(age_first_breeding)) names(dfTraits) ## [1] &quot;species&quot; &quot;genus&quot; ## [3] &quot;family&quot; &quot;main_biogeographic_realm&quot; ## [5] &quot;latitude&quot; &quot;insular_endemic&quot; ## [7] &quot;maximum_svl&quot; &quot;hatchling_neonate_svl&quot; ## [9] &quot;leg_development&quot; &quot;activity_time&quot; ## [11] &quot;substrate&quot; &quot;diet&quot; ## [13] &quot;foraging_mode&quot; &quot;reproductive_mode&quot; ## [15] &quot;smallest_clutch&quot; &quot;largest_clutch&quot; ## [17] &quot;age_first_breeding&quot; &quot;iucn_redlist_assessment&quot; ## [19] &quot;iucn_population_trend&quot; &quot;log_hatchling_neonate_svl&quot; ## [21] &quot;log_age_first_breeding&quot; # Plot the transformed data. # Note I placed the x and y variables in the ggplot argument here so I don&#39;t have to do it twice for both geom_point and geom_smooth ggplot(data = dfTraits, aes(x = log_hatchling_neonate_svl, y = log_age_first_breeding)) + geom_point(# Adding some colour and transparency (alpha) to the points color = &quot;skyblue&quot;, size = 2, alpha = 0.7) + # Add linear regression line geom_smooth(method = &quot;lm&quot;, color = &quot;darkblue&quot;, linewidth = 0.5) + theme_minimal(base_size = 12) # Boxplots to show distributions of SVL by IUCN redlist assessment ggplot(data = dfTraits, mapping = aes(x = iucn_redlist_assessment, y = log(maximum_svl))) + # Boxplot geom_boxplot(aes(fill = iucn_redlist_assessment)) + scale_fill_brewer(palette = &quot;Blues&quot;) + labs(title = &quot;SVL Distribution by IUCN Redlist Assessment in Reptiles&quot;, x = &quot;IUCN Redlist Assessment&quot;, y = &quot;Log Maximum SVL&quot;) + # Flipping the coordinates for better visualization coord_flip() + theme_minimal(base_size = 12) + theme(legend.position = &quot;none&quot;) ## remove the legend # Associations between categorical variables. ggplot(data = dfTraits %&gt;% filter(!is.na(diet))) + # Using a variant of geom_point geom_count(mapping = aes(x = diet, y = iucn_redlist_assessment)) + labs(x = &quot;Diet&quot;, y = &quot;IUCN Redlist Assessment&quot;) + # Increasing the size of the points scale_size_continuous(range = c(2, 10), name = &quot;Count&quot;) + theme_minimal(base_size = 12) Based on this, you may ask yourself what variables/relationships you want to explore further and which you could possibly remove from your dataset. Outlier detection. —- Remember those really long species? Let’s look at them a bit more closely! But this time, we will group by family and highlight outliers using the outlier arguments available in geom_boxplot. # Boxplots to show distributions of maximum SVL by family ggplot(data = dfTraits %&gt;% filter(!is.na(maximum_svl))) + geom_boxplot(mapping = aes(x = family, y = maximum_svl), outlier.color = &quot;red&quot;) + coord_flip() You can see the outliers are a part of the Varanidae family. # Using the interquartile method to identify outliers. # Determine the 1st quartile using the quantile function. quantile(dfTraits[, &quot;maximum_svl&quot;], na.rm = T) ## 0% 25% 50% 75% 100% ## 17.0 55.7 75.3 104.0 1570.0 lowerQuantile &lt;- quantile(dfTraits[, &quot;maximum_svl&quot;], na.rm = T)[2] # Determine the 3rd quartile using the quantile function. upperQuantile &lt;- quantile(dfTraits[, &quot;maximum_svl&quot;], na.rm = T)[4] upperQuantile ## 75% ## 104 # Calculate the IQR by subtracting the 1st quartile from the 3rd quartile. iqr &lt;- upperQuantile - lowerQuantile # Calculate our upper threshold ((3 x the IQR) + upperQuantile). upperThreshold &lt;- (iqr * 3) + upperQuantile # Identify outliers based on whether they exceed the upper threshold. outliers &lt;- which(dfTraits[, &quot;maximum_svl&quot;] &gt; upperThreshold) # Subset the outliers with taxonomic information. dfOutliers &lt;- dfTraits[outliers, c(&quot;family&quot;, &quot;genus&quot;, &quot;species&quot;, &quot;maximum_svl&quot;)] head(dfOutliers) ## family genus species maximum_svl ## 106 Scincidae Acontias Acontias gracilicauda 260 ## 111 Scincidae Acontias Acontias meleagris 295 ## 115 Scincidae Acontias Acontias percivali 257 ## 116 Scincidae Acontias Acontias plumbeus 500 ## 117 Scincidae Acontias Acontias poecilus 382 ## 242 Iguanidae Amblyrhynchus Amblyrhynchus cristatus 560 Detect points that may be need to be ed out further. Should you remove them or not? E.g., look further into whether they might be human error or a legitimate biological observation! And of course, the choice to remove outliers will also depend on the model you end up using. Dimensionality reduction. —- If you have a high dimensional dataset, you may think about using PCA to make your data more manageable. Principal component analysis example. # Create a data subset for PCA analysis. colnames(dfTraits) ## [1] &quot;species&quot; &quot;genus&quot; ## [3] &quot;family&quot; &quot;main_biogeographic_realm&quot; ## [5] &quot;latitude&quot; &quot;insular_endemic&quot; ## [7] &quot;maximum_svl&quot; &quot;hatchling_neonate_svl&quot; ## [9] &quot;leg_development&quot; &quot;activity_time&quot; ## [11] &quot;substrate&quot; &quot;diet&quot; ## [13] &quot;foraging_mode&quot; &quot;reproductive_mode&quot; ## [15] &quot;smallest_clutch&quot; &quot;largest_clutch&quot; ## [17] &quot;age_first_breeding&quot; &quot;iucn_redlist_assessment&quot; ## [19] &quot;iucn_population_trend&quot; &quot;log_hatchling_neonate_svl&quot; ## [21] &quot;log_age_first_breeding&quot; # We need to remove NAs before doing this (but don&#39;t worry, we will deal with missing values this afternoon)! dfPCA &lt;- as.data.frame(na.omit(dfTraits[contTraits])) # Perform PCA analysis using prcomp. pcaRes &lt;- prcomp(dfPCA, center = T, scale = T) # Standard deviations for each principal component. pcaRes$sdev ## [1] 1.6122257 1.1844751 0.9899865 0.7776832 0.5604074 0.3143660 # The columns here are eigenvectors that correspond to each principal component, tells you how much each variable contributes to the component pcaRes$rotation ## PC1 PC2 PC3 PC4 ## latitude -0.06534507 -0.2630789 -0.94506001 -0.14425519 ## maximum_svl 0.56598861 0.1275942 -0.13916457 0.34929400 ## hatchling_neonate_svl 0.52114614 0.3287476 -0.12863572 0.31080620 ## smallest_clutch 0.31513640 -0.5967449 0.26188578 -0.29760116 ## largest_clutch 0.40091774 -0.5478159 0.03535006 0.08441005 ## age_first_breeding 0.37914065 0.3875919 -0.03337456 -0.81540919 ## PC5 PC6 ## latitude -0.11056817 0.01860709 ## maximum_svl -0.02582579 -0.72203640 ## hatchling_neonate_svl -0.28648279 0.65200589 ## smallest_clutch -0.62168772 -0.03063215 ## largest_clutch 0.69237714 0.22671986 ## age_first_breeding 0.19787267 -0.02941600 # The transformed variables in the PCA space. pcaRes$x ## PC1 PC2 PC3 PC4 PC5 ## 5 -1.277826741 -0.4472862383 -1.0958707255 -0.023995115 0.168222539 ## 7 -1.259683916 -0.6764409787 -0.9883394137 0.098568843 -0.279953819 ## 9 -1.147279583 -0.4949261722 -0.7168664307 -0.090289599 -0.192475942 ## 22 0.150771971 0.5781564425 -0.4946526657 -0.991001796 0.612947806 ## 53 -0.653580709 -0.6513218385 -0.5988712764 -0.152628583 -0.374492168 ## 56 -0.919430845 -0.1379941277 -0.5472989749 0.334414105 0.108776309 ## 59 -0.805705150 -0.1978411549 -0.7127529783 0.144478711 0.243969470 ## 60 -1.278424339 0.0008305589 -0.6508764593 0.320326644 -0.244054779 ## 61 -0.910484964 -0.2195549870 -1.0392571726 0.213150397 0.035936371 ## 78 -0.486987241 -0.0338655319 -0.7446133075 -0.558469351 -0.096497581 ## 81 -0.758758379 0.2201330846 -0.6485615808 0.260537749 -0.165355702 ## 83 -1.148660899 -0.1776018923 -0.5403909698 0.434130563 -0.012033892 ## 105 0.640244712 1.8581150478 0.8664701661 -0.101393271 -0.246736845 ## 108 0.671452491 1.6584988264 0.7620965030 -0.116222321 -0.055781286 ## 129 -0.407868320 0.7163980175 0.0073893518 0.578751427 -0.351791132 ## 162 0.264011020 -0.9573448264 0.2092278230 0.302099526 0.499605693 ## 171 0.736656041 -0.9002496377 0.7445303864 -0.282289357 -1.455657887 ## 180 0.306632730 -1.6787654462 -0.3100362016 -0.146699887 -0.112472137 ## 187 0.200507816 -0.7829374867 0.7213205575 -0.242807850 -0.533115850 ## 192 0.473671761 -1.0321036293 0.9242670614 -0.426936919 -1.302956628 ## 216 -1.181453185 -0.3216668649 -0.9745189675 -0.095493204 0.176086129 ## 226 -1.476658972 -0.1240863236 -0.9991074726 -0.204268949 -0.115570345 ## 228 -1.498280847 -0.2025386651 -1.2497501809 -0.244911207 -0.135579275 ## 230 -1.647402256 -0.2873460883 -1.0699027415 -0.030413164 -0.157573166 ## 236 -1.052951005 0.2271777831 -1.1446064778 -0.820477409 -0.004954233 ## 242 3.565051335 2.6375086173 -0.6911104353 0.868874521 -0.602943005 ## 244 -0.111379886 0.2413926806 0.3992373055 1.127550094 0.180502708 ## 269 -0.183191452 -0.1709335775 1.5486810607 0.542262233 -0.027375168 ## 270 -0.259392059 0.0872129835 1.6025311154 0.336321983 -0.190279879 ## 386 -0.159523223 -0.1361619571 -0.8994936235 -1.609339363 -0.045697507 ## 407 1.918418130 -1.0384145690 -1.4427335124 -1.024067174 0.045250527 ## 408 1.358029000 -0.6337184941 -1.6485992756 0.088740224 0.731222653 ## 418 0.312173209 0.8194694000 -1.2344733249 0.004442298 -0.334738628 ## 420 -1.443654475 0.0240875644 -0.3039635485 0.340502860 -0.205042440 ## 451 -1.313705760 0.0806478267 -0.0195571635 0.584518865 -0.194503419 ## 464 -1.219061423 0.1067067828 -0.4911788079 0.015198484 -0.091032423 ## 473 -1.437087493 0.1915696772 0.3735954139 0.490117714 -0.152130807 ## 491 -1.439817261 -0.1858191610 -0.8125923040 0.418863527 -0.254904648 ## 513 -1.356519847 0.0635641497 -0.3407827696 0.355580993 -0.221011138 ## 517 -1.605349981 -0.0559757636 -0.1140299174 0.476064606 -0.193986275 ## 536 -1.465387747 0.0160673418 -0.4856828789 0.089644862 -0.165866296 ## 545 -0.356337223 0.5218927043 -0.7041927446 0.767634563 -0.510503481 ## 563 -0.787424067 0.3899216235 -0.1057700235 0.813824211 -0.367216007 ## 569 -1.052811441 0.0775409014 -0.4133300092 0.708700806 -0.274238309 ## 571 -1.084708971 0.3913011658 0.2713838916 0.217480876 -0.083124975 ## 585 -1.404186319 0.0621194925 -0.3183127286 0.147897284 -0.128619376 ## 594 -1.593592399 -0.0547624684 0.0501425953 0.415577492 -0.091498291 ## 629 -1.649685820 -0.1229562119 -0.0315259077 0.443848506 -0.100667884 ## 631 -1.436965377 0.0056907036 -0.3243131392 0.415014417 -0.227652348 ## 634 -1.354983252 0.0955326292 -0.0143880918 0.738418569 -0.303196578 ## 642 -1.115606358 0.3410713947 0.0079930896 0.148368742 -0.116610477 ## 643 -1.310836863 0.0799975008 -0.4780125472 0.332360615 -0.266643458 ## 684 -1.459074619 0.0303353125 -0.4015322363 0.103281521 -0.144474102 ## 694 -1.319535232 -0.0181696363 -0.2543584917 0.628871676 -0.215664840 ## 727 -1.310330933 0.1371949050 0.0069307853 0.768284237 -0.330306069 ## 730 -1.555306832 0.0102458311 0.0273982912 0.501305370 -0.185367291 ## 732 -1.206989662 0.0680757830 -0.4755996321 0.241919454 -0.148521055 ## 761 -1.180057758 0.1719038648 -0.2296138722 0.169579357 -0.098154107 ## 770 -1.341884786 0.0331504627 -0.4765237784 -0.060151293 -0.035266522 ## 814 -1.402535164 0.1459398227 -0.0310196178 0.251200855 -0.144843401 ## 855 -0.415674297 -0.3612440345 -0.8387665949 -0.883070149 -0.281986353 ## 917 -1.009267061 0.1305556031 -0.3536749127 0.279196254 -0.087978375 ## 919 -1.047419179 0.0484066151 -0.3300906949 0.619311774 -0.182722704 ## 921 -0.683716660 0.0065197961 -0.9051734384 0.312018233 -0.058633287 ## 924 -0.693141398 -0.0548897307 -0.7710255450 0.380407734 0.023579266 ## 926 -1.066300208 0.0296408362 -0.7315714809 0.129308579 -0.109272451 ## 927 -0.888383324 -0.0177343953 -0.7772133122 0.282090934 -0.070255912 ## 929 -0.815061611 -0.0247410227 -0.7201600869 0.321205267 -0.022338092 ## 930 -0.730703513 -0.0965366439 -0.4590609821 0.594108713 0.072054704 ## 931 -0.722341848 0.1317204634 -0.8472878574 0.248474082 -0.141266809 ## 937 -0.907548126 0.0617684475 -0.9352013664 0.279143125 -0.223076516 ## 945 -0.741983368 0.1107677145 -0.6817762907 0.399959575 -0.126911562 ## 946 -0.833825546 -0.1654949453 -0.9877562678 0.267120220 0.017639509 ## 949 -0.448567128 0.1071198307 -0.9704582048 0.294093102 -0.065081569 ## 950 -0.665371388 -0.1360788233 -0.9816371986 0.520559040 -0.036175487 ## 951 -0.933629616 -0.1342134553 -0.8672586866 0.230867411 0.008469735 ## 956 -0.683996596 -0.1992426114 -1.0326224612 -0.857978138 -0.007337118 ## 966 -1.277349658 -0.0358255708 -0.7984700814 -0.033815189 -0.099224156 ## 1005 0.532653344 -0.3928848962 -0.0902934800 0.856483255 0.377867635 ## 1007 0.944338124 -0.6024636915 -0.0680175895 -0.009634179 -0.106769783 ## 1008 0.194476959 -0.5575175969 -0.3582815874 0.929127835 0.215536212 ## 1009 -0.230027192 0.4571171252 1.6753245421 -0.329805597 0.478098159 ## 1027 2.101063819 2.1423812295 -1.2837681586 -0.395420866 -0.572208004 ## 1028 1.970837300 2.1240793883 -0.9750092181 -0.442787561 -0.458462988 ## 1029 0.435772558 1.0406351048 -0.6549185453 0.359774310 -0.429132047 ## 1040 1.431653942 1.7010908534 -1.5824505583 -0.172318434 -0.711819155 ## 1043 1.878434674 1.5973465422 0.5620166380 -0.112696297 -0.815922041 ## 1044 1.053786810 0.9159964344 0.5900665628 1.269506843 -0.562012237 ## 1046 1.429183451 1.7776408560 0.3970717863 0.426317692 -0.215139591 ## 1101 0.876466770 -2.1253022173 2.4512598150 -0.961998363 -1.148801173 ## 1107 -0.453691659 -0.5929254468 1.5062400331 0.752609488 0.784749282 ## 1109 0.667248176 -2.1251137690 2.5824936893 -1.202293909 -1.739326441 ## 1111 0.353246520 -1.3605360680 1.8146001897 -0.672762552 -0.566053717 ## 1112 0.230887988 -0.7312132102 1.7327666523 -0.176916057 0.478293923 ## 1114 -0.418385639 0.2330221846 1.1724753520 0.245859524 0.104879906 ## 1115 -0.745289079 -0.1877156348 0.7378060689 0.715700948 -0.063426347 ## 1152 -1.140171084 -0.0906443385 0.9472675857 0.470279310 -0.144633985 ## 1183 -0.626683276 -0.4878048805 -0.8853494451 0.388898055 0.418511838 ## 1196 0.103557825 -0.2540160462 -0.2672027505 -0.559523591 -0.041147275 ## 1208 0.312463993 -1.7977945259 0.4210213997 0.037775941 -0.427848397 ## 1213 -0.085759232 -1.2629057009 -0.4313698865 0.758579669 1.452536057 ## 1239 3.827327513 -5.1501487453 2.4206278219 -1.070790839 -0.671714673 ## 1258 -0.772255454 0.7191047051 1.5784573091 -0.412630930 0.621129565 ## 1261 -0.256935824 1.2582820574 1.7224132525 -0.926538135 0.667721399 ## 1262 -0.403423629 1.0154026191 1.6858052563 -0.155401572 0.437397777 ## 1297 -1.013245289 0.2169461361 1.0069295534 0.344013858 -0.288400540 ## 1299 -1.092299993 0.1890259321 1.0427039450 0.261902006 -0.245661337 ## 1304 -1.209729629 0.1071857769 0.9400841734 0.176301753 -0.229261242 ## 1344 2.538451953 -1.8458624545 0.0932422176 -1.220510230 -0.111115502 ## 1370 0.805508640 -0.5805538609 -1.1397553815 -0.086700149 0.028597847 ## 1375 0.242348168 -0.2473311773 -0.8651709539 0.384421566 -0.686576284 ## 1376 0.995263541 0.0388685725 -0.6519717622 -2.154587116 -0.599061107 ## 1382 0.037571295 0.8379964319 -1.0115272390 -1.026374770 0.101013895 ## 1384 0.374052004 -0.3784933087 -0.7053682655 0.111659142 1.021936607 ## 1392 -0.187352394 0.1678646313 -0.6999908601 -0.509313102 -0.147289667 ## 1399 2.832228661 -5.2052037965 1.2978208686 -0.937664825 -1.018138365 ## 1403 3.525515245 -6.2079594536 0.8981103420 0.194843742 2.208458557 ## 1404 0.871470087 -2.4708083417 -0.5527846957 0.617454269 1.774485348 ## 1405 1.921957032 -2.8501274812 0.7516593730 1.133024299 3.613254311 ## 1409 0.697863840 -1.8654419313 1.6830503030 0.515004514 0.127399664 ## 1411 1.583518877 -3.7723590286 0.6819196324 0.347084024 1.378714941 ## 1431 1.699497422 -0.2651100659 0.9227892008 0.205393989 0.257395394 ## 1432 -0.574818920 0.8719896169 1.1201820471 0.595869350 -0.113558949 ## 1439 -0.910619471 0.8998453620 1.4794630475 -0.442743101 0.360475202 ## 1581 -1.000514499 0.0977473689 0.1142054648 0.613491538 -0.004969092 ## 1600 -1.152644807 -0.0058494308 -0.7047141233 0.005881193 -0.044130051 ## 1601 -0.622320503 0.3245151451 -0.4648228904 0.438983603 -0.206448370 ## 1606 -0.271153169 0.4922556284 -0.7692131873 -0.937033233 -0.321738678 ## 1607 -1.145234784 0.0580995113 -0.8611544900 0.164688369 -0.245486715 ## 1630 5.828750602 1.5234604082 0.2876054896 -3.572816650 -1.799010981 ## 1631 5.642031134 1.4100664831 -0.0094597721 -2.379497006 -0.501912902 ## 1641 -0.875487629 -0.3925457936 -0.7412847717 0.215380225 0.349905431 ## 1653 -0.178918783 1.3048908489 1.2890897756 -0.450944536 0.246156716 ## 1655 0.026350200 1.3728217970 0.9586627350 -0.805594216 0.308902421 ## 1672 -0.478274014 0.8648495341 0.9125945892 0.634624367 -0.162794847 ## 1674 3.117503495 2.8084724633 -0.4257911444 1.881289058 -1.468235698 ## 1679 -0.049790315 -0.1681146377 -0.0955977790 -0.611173841 -0.103117115 ## 1683 -1.424228608 0.4860783025 1.4764114274 0.099435870 0.231488577 ## 1688 1.646884544 0.9814184349 0.0814239372 0.534693333 -0.828776076 ## 1689 -1.231823965 -0.0369496986 -1.1408465868 -0.064686864 -0.202239452 ## 1692 -0.556686642 -0.3209354746 -1.1179307167 0.519241776 0.127048101 ## 1693 -0.033329148 -0.6990943314 -0.9272756246 0.601477745 0.786639717 ## 1698 -0.058230168 -0.8505733479 -0.4509122827 0.127894980 -0.621527888 ## 1757 -0.731236938 0.0218007592 1.2082997456 0.429597334 0.092760913 ## 1764 -1.091484200 0.4295717826 1.3834719878 0.547367588 0.186683136 ## 1766 -0.974108598 0.3424572430 1.1193961045 0.616437769 0.188772901 ## 1767 -0.744313841 0.3462948854 1.4453297425 0.267620034 -0.061465006 ## 1768 -0.928564148 0.5257415856 1.3128308297 0.497899434 0.169605317 ## 1772 -0.445415930 0.3022353574 1.1557267000 0.630020597 -0.158969665 ## 1773 -0.524167132 0.3593677162 1.4289115032 0.608392841 -0.136944312 ## 1794 1.432575969 0.3043007782 -0.4076779722 0.118768426 -0.939590111 ## 1796 4.210929362 -2.9374717339 0.2643322379 -1.006578223 -0.241076881 ## 1798 5.418836135 -5.1179203106 0.6171777032 0.158888425 1.385150435 ## 1845 -0.604571056 0.4084159029 1.4286848933 0.354450898 -0.098102835 ## 1870 -0.853797161 0.6158503681 1.2454710161 0.479173105 0.103443929 ## 1873 -0.103517557 0.0446817394 1.2858090490 0.386267304 -0.234110010 ## 1890 -0.380871885 0.8351341982 1.1434954034 -0.213589942 0.419969264 ## 1904 1.393024052 0.0622177261 0.9473795723 0.685811472 1.539192791 ## 1910 3.653212323 2.3881541059 -1.2056700036 -1.626439159 0.028700649 ## 1912 4.610727544 0.5373433445 -0.9119968447 0.765686850 -0.735579960 ## 1913 6.479721712 4.3736363770 -1.7272466760 -5.018558746 1.639206028 ## 1914 3.551699329 1.4353384751 -1.2481866923 0.948871302 0.272110767 ## 1915 4.467465805 0.7449951085 -1.3835199812 2.065197232 0.177570491 ## 1916 4.617183757 0.3790438518 -0.6222900482 -0.447821201 -1.205700829 ## 1917 2.953074241 0.5895556121 -0.9537619703 1.226740072 0.106556638 ## 1919 4.998253212 1.6276284389 -0.8843773983 -1.528100840 -0.493061241 ## 2073 -0.466263393 0.5372821898 0.8531177986 0.418469874 -0.430769233 ## 2094 -0.993383233 0.2829779659 -0.3673150061 0.169843379 -0.178359248 ## 2152 -0.772574833 0.5882955983 -0.2117730371 -0.541111231 0.052072597 ## 2175 -0.224460434 0.6271504490 0.7505359894 0.560665661 -0.504673350 ## 2225 -0.364986701 0.4339984811 -1.2274201755 -1.329932960 0.348384104 ## 2230 -0.656579025 -0.1360869612 -1.1074092123 -0.835770962 -0.098307743 ## 2234 -0.359940126 0.2503577740 -1.0976269953 -1.502164250 0.004233559 ## 2236 -1.101056962 -0.3301940782 -1.1550180670 -0.064015067 0.134940317 ## 2241 -1.074559846 -0.3831613270 -1.0413976292 -0.197856284 -0.349416401 ## 2245 -0.998852169 -0.1942764708 -1.0786373590 0.022065221 0.045567296 ## 2246 -0.283618936 0.8199470770 -1.2031705019 -2.103409349 0.406083037 ## 2249 -0.561541007 -0.1201683945 -1.1844806906 -0.783352345 -0.119388562 ## 2252 -0.266293798 0.3122127080 -1.2098308826 -1.305936474 0.508359501 ## 2254 -0.024192734 0.0142744503 -1.0580278392 -1.379353671 0.353250499 ## 2275 -0.333522431 1.2425673662 1.4405870685 -0.084921728 0.115211298 ## 2326 -0.211705938 0.1175794536 -1.1707850612 -1.455994683 0.154977434 ## 2332 -1.100091350 0.5650465227 1.1222734474 0.409166344 0.011201912 ## 2351 -1.112667186 0.2357933987 1.3465655285 0.390422866 -0.235522681 ## 2394 -0.975450887 0.3505615645 1.1729270399 0.282819352 0.365493513 ## 2396 0.444774595 0.4717614696 -0.9487880849 -0.728292494 -0.118051726 ## 2447 -0.796082891 -0.2246025809 -0.1584055086 0.409840095 -0.279269056 ## 2452 -0.924804864 0.1507965421 0.5268448153 0.606169700 0.153184483 ## 2465 1.913033954 2.2095051851 1.0222150928 -1.432624561 0.798388687 ## 2472 1.127399434 1.7747555915 0.5533932269 -0.183114790 -0.013322922 ## 2473 1.716029169 1.5095920066 1.0653797383 -0.035667681 0.034842115 ## 2480 2.072460386 2.5149840358 0.7178161040 -1.330801639 0.372613350 ## 2481 0.304050868 0.9728991803 1.1743487779 0.017034249 -0.145096373 ## 2485 0.216602776 -0.1998821463 -1.3379229796 -1.006614578 0.390722759 ## 2487 0.720534064 -1.3025026701 -1.0656767415 0.223373009 1.877492245 ## 2495 -0.932565215 0.3442204576 0.2148220843 0.475851776 -0.105373762 ## 2509 -1.242590905 0.3810727340 0.7899365013 0.356035727 0.006204751 ## 2515 -1.279209769 0.3676893667 0.7431013267 0.326925430 -0.010276410 ## 2598 -0.728281758 -0.4612700642 -1.4058308191 0.235205171 0.193845344 ## 2612 -1.283384437 -0.1230507558 -1.0975858503 0.037831511 -0.182515044 ## 2614 -0.735961692 0.1924413849 -1.2142201127 -0.667753991 0.083323703 ## 2618 0.174410142 0.4806441431 -0.8714050093 -1.613765681 0.200517859 ## 2619 -1.004869683 -0.4447244398 -0.9712647819 -0.173209352 -0.240167290 ## 2622 -1.114842390 -0.0573742102 -1.0183803483 -0.004648372 -0.104924349 ## 2624 -0.647704625 -0.7629261042 -0.9250710366 0.167160150 0.093693256 ## 2628 -0.787373822 -0.3763149036 -1.1009161401 -0.049209059 -0.307622346 ## 2643 -0.231664007 0.7716490507 1.0720912106 0.077702168 0.391168018 ## 2648 0.988632872 0.9314364501 -0.8545425013 1.077611963 -1.133414688 ## 2650 -0.154557501 0.5395251288 -0.9605487737 0.549069468 -0.442523097 ## 2660 -0.440953092 0.9854538158 1.3881155005 -0.198253298 0.341968937 ## 2661 0.186346337 0.6409587702 1.2582969675 -0.099147299 0.220072001 ## 2662 -0.126408775 0.9389562568 1.4670820334 -0.044960291 0.498322758 ## 2669 1.075221610 0.0387156777 -1.0571271222 -0.358278290 0.379221622 ## 2678 -0.812533327 0.4877626772 1.1378573901 -0.202304933 -0.129846510 ## 2680 0.743746399 0.2997882493 -0.7903270272 -0.593711272 -0.463001728 ## 2717 -0.353533776 0.1170231275 -0.3554665943 0.541149574 0.125704116 ## 2754 -0.342763814 -1.5399390792 1.4499243411 0.461056629 0.536309343 ## 2756 0.140011063 -1.7947497812 1.7686957683 0.198267374 -1.339062959 ## 2757 0.069339712 -1.2077294700 1.3126669844 0.819801024 0.198436979 ## 2761 5.980164620 -8.8152760362 3.9421682818 -2.077032355 -4.230125507 ## 2762 2.387761472 -3.5567680564 1.6980408503 0.381080573 0.022022034 ## 2768 2.607516864 -4.2162796493 2.0807949941 0.240727774 0.344208964 ## 2771 -0.472246975 0.3589334123 -0.8030139428 -0.469156282 0.136415298 ## 2772 0.428467539 0.3225937086 -0.8468555237 -0.145963055 -0.204099961 ## 2773 -0.445660786 0.4284753776 -0.8010253674 -0.366871094 0.032123008 ## 2774 0.058568237 0.1257389971 -0.7553173373 -0.384835332 -0.056940177 ## 2776 2.080595026 -0.0348285099 -0.9079449849 0.403397424 -0.581751946 ## 2777 1.797654611 -0.0909020869 -0.7716795583 -0.600866036 -0.154963494 ## 2779 -0.189950106 0.4088374448 -1.1565738432 -0.042147753 -0.139476178 ## 2780 -0.171238624 -0.2155843348 -1.1012176728 0.665181009 0.152404488 ## 2818 -1.506778505 -0.1316217340 -0.0500335184 0.662978358 -0.137685824 ## 2835 -0.862371849 0.8484545611 1.2799703384 -0.145999062 0.197951714 ## 2854 -0.163555766 0.5385501535 -0.4510006987 0.891549840 -0.375890717 ## 2906 1.332304769 -1.4984926882 -0.3711607341 -0.479133506 0.366164099 ## 2944 -1.541447218 -0.0194762229 -0.0055253054 0.369725135 -0.103110529 ## 2947 -1.438500186 0.1789979433 -0.0781785802 -0.066437625 -0.063661594 ## 2957 -1.356072640 0.2937108852 0.5117415546 0.037104886 0.070169639 ## 2974 -1.496706738 0.0921469090 0.0192614461 -0.033324889 0.017334069 ## 2976 -0.257924611 0.6980379581 -0.7072979425 0.161900884 -0.358585993 ## 2979 -0.586482891 0.1797820277 -0.3779964026 0.208195748 -0.568445112 ## 2982 -0.715757883 0.0315931774 -0.6269642796 0.100423623 -0.537939721 ## 2999 0.218027653 1.0062071976 0.1479095459 -0.808376027 0.441115399 ## 3028 -1.453760397 -0.0541999357 0.0424928082 0.258895640 0.046883725 ## 3046 5.354521215 2.7668874619 -1.4132806282 -1.624075540 0.373028196 ## 3047 3.381391190 2.0445589157 -1.7668940739 1.034047568 -0.750009558 ## 3061 -1.287783155 0.1532958713 -0.0230430633 0.239272208 -0.083315716 ## 3075 -1.209770421 0.0284633932 -0.5294920455 0.075851186 -0.057237433 ## 3091 -1.073288889 0.0813248169 -0.5775847616 0.228830801 -0.136119543 ## 3095 -1.476913879 -0.0423613518 -0.0726306308 0.584339456 -0.179100563 ## 3097 -1.456406058 -0.0510083982 -0.1045243248 0.701872802 -0.233572316 ## 3111 -1.202855216 -0.2170830549 -0.4600639867 -0.044120700 -0.400118475 ## 3136 -1.098334145 0.3741565920 0.4132008769 0.262401817 -0.023616121 ## 3164 -1.205733773 0.1989819304 -0.2029597309 0.054233345 -0.074535185 ## 3193 -1.283413451 -0.0712409534 -0.7541977802 0.177117135 -0.150434971 ## 3203 -0.526158785 0.8706983514 1.3854211707 -0.265280967 0.443232117 ## 3231 -0.238645114 0.6385630432 -0.3061214483 0.639672481 -0.345776223 ## 3233 -0.301047931 -0.0677549278 -0.9175429409 -0.471728449 -0.077617778 ## 3235 -0.858839065 -0.3228612593 -0.9845203885 0.226173813 0.184108639 ## 3236 0.106420498 1.3856157514 1.4361566811 -1.772361505 0.353981592 ## 3241 -1.298808839 0.4516876801 1.1980780290 0.364926489 0.078465932 ## 3244 -1.160562286 0.1872275803 1.2056649899 0.236811777 -0.195233591 ## 3250 -0.985815643 -0.4678570134 -0.8422779176 0.112829224 0.372986159 ## 3251 -0.928461260 -0.3866868424 -0.5744746376 0.049108965 -0.200441594 ## 3255 -0.560772338 -0.0691730419 -0.1977087055 1.006418395 0.044794976 ## 3259 -1.062616203 0.0972812344 -0.0656178766 0.710532937 -0.171320122 ## 3266 -0.544370266 1.4168890444 1.7692571547 -1.035989137 0.447567618 ## 3281 1.316596726 2.3786742055 1.4781254555 -2.723889250 0.495657104 ## 3304 -0.694518585 -0.1112770590 -1.1271924644 -0.776335499 -0.178681071 ## 3305 -0.796769024 0.2175431258 -1.2468919121 -0.624953514 -0.010903766 ## 3306 -0.883741157 0.1617904028 -1.2286239378 -0.677165590 0.036605955 ## 3307 -0.788996078 -0.5705545042 -1.0377064456 0.061235095 -0.149584829 ## 3309 -0.670579953 -0.0909696375 -1.3229725729 -0.705666283 0.355309817 ## 3310 -0.126397013 -0.1979073310 -0.8632514016 -1.747753670 -0.550860902 ## 3311 -0.815940517 -0.6480018894 -1.0950838273 -0.047601358 -0.059918943 ## 3313 -0.767392239 -0.6108361582 1.1316439775 0.344331769 0.019506994 ## 3318 3.248779146 -0.0331118047 -0.5507165424 0.076670763 0.252687680 ## 3319 5.425173126 -3.0745594344 0.6065078252 1.037798636 0.988919817 ## 3324 1.233011107 -0.2651683298 1.2284618474 1.028508065 0.419071810 ## 3346 0.244266791 -2.3247412689 0.1389559884 -1.316395004 -1.414354357 ## 3359 -0.878533743 -0.4038937075 -0.4189025817 -0.029334522 -0.042480728 ## 3411 -0.424099405 -0.8682255069 -1.5461901291 0.147946756 0.721053654 ## 3413 -0.093376562 -0.3855993576 -1.1399875953 0.153169445 0.601208549 ## 3415 0.354479005 -1.4470576918 -0.8816045237 -0.265784883 0.162286242 ## 3416 0.134484835 -1.2286769878 -0.9146453557 -0.024127401 -0.277498648 ## 3418 0.371813441 -1.5493504227 -1.0527601250 0.016632605 0.064865233 ## 3426 0.576823607 0.0058051707 -0.4070171618 -0.261651323 -0.252344481 ## 3429 -0.615485324 0.3075321451 0.0907627363 0.622130796 -0.038551321 ## 3436 -1.066379959 0.2931901848 1.3771191421 0.234194308 0.484360582 ## 3438 -1.200736531 0.3347714674 1.5388868543 0.482667129 0.324357098 ## 3492 -0.643094754 0.4296796408 -0.5730693174 -0.078208991 -0.109066507 ## 3552 -1.627627298 -0.2009880349 -0.0753598423 0.583866334 -0.104008825 ## 3697 -0.168628746 1.0987114126 1.1979490008 -0.047774300 0.210990834 ## 3739 -0.144090431 -0.3614593994 1.5428876481 -0.292876843 -0.748252249 ## 3861 -0.839461983 0.5805178691 1.0487682227 0.394914583 0.104684195 ## 3904 -0.006173191 1.2183342350 1.5837179441 -0.809094374 0.720408940 ## 3948 -0.062274067 -0.4400587770 1.3493635075 -0.274916207 -0.718523974 ## 3995 -0.405996975 1.0215824355 1.1607093797 -0.188217057 0.220131875 ## 3996 1.090181837 1.5629322092 0.6773425666 0.661421839 -0.185252067 ## 3998 -0.071537956 1.1416146197 1.1402317903 0.006969754 0.165585778 ## 4000 -0.059577658 1.4496025224 1.2937168445 -0.809004118 0.342183458 ## 4005 -0.141777556 1.1327292166 1.3175684603 -0.017318133 0.230455304 ## 4034 -1.176325553 0.1338713651 -0.1698929873 0.048042252 0.009048041 ## 4037 -0.009799564 1.1292513218 1.4306584990 0.067034339 0.318686217 ## 4047 -0.091498297 -0.4254537259 0.8372593063 0.488035734 0.650778532 ## 4049 -0.189669721 0.6296912794 0.7265497544 0.731859332 0.091736503 ## 4051 -0.296616029 0.2690978773 1.3465579383 0.488591875 0.100570372 ## 4064 -1.365428221 -0.2534518588 0.1884950081 0.127823496 -0.253184130 ## 4071 -1.415189272 0.3436920973 1.3502306090 0.742081471 0.006689978 ## 4076 -0.925637838 0.3885356771 1.2770904719 0.174993776 -0.217069257 ## 4077 -1.005716523 0.2901665288 1.1925866868 0.260302996 -0.237026880 ## 4115 -1.518126478 0.2447222968 0.9166588822 0.268561912 0.098090698 ## 4151 -1.576212237 0.0487796327 0.1606352463 0.005318542 0.067257735 ## 4183 -0.367574288 0.0532562059 -0.0826513682 -0.715034849 -0.404625637 ## 4235 -0.592168027 -0.2214086731 -0.1852255362 0.133176364 -0.063942580 ## 4251 -1.407949029 -0.1935466365 -1.0259482961 -0.031909467 -0.106122085 ## 4253 -1.246450847 0.0518006776 -1.1432667462 -0.652239749 -0.008014237 ## 4255 -1.487708098 -0.1587783488 -0.9683623992 -0.062771592 -0.140389428 ## 4269 -1.559233440 0.1382253228 1.2780938616 0.544429028 0.197877993 ## 4272 -1.251159917 0.4120124154 1.0445949942 0.666260226 -0.078611936 ## 4274 -1.077355241 0.3857630174 1.1916692684 0.607276789 0.122894783 ## 4278 -0.568481830 -0.5093085170 1.1308325007 0.613312962 -0.116517574 ## 4285 -1.115694360 -0.0995588288 -0.7043798804 0.002141952 0.063193294 ## 4287 -1.103420461 -0.3547950279 -0.5019703700 0.268561490 0.274314594 ## 4291 -1.271883897 -0.3647085290 -0.6799336896 0.241077709 0.133869027 ## 4292 -1.096850825 -0.2368805130 -0.4215555192 -0.144802381 -0.266573080 ## 4309 -1.531771278 0.1985214381 0.9101062554 0.476051260 0.031480696 ## 4328 -0.891041354 -0.2350086051 0.5890324258 0.584715995 -0.089554895 ## 4339 -0.241332521 0.6549535297 0.9669863665 0.574393235 -0.449933850 ## 4358 -0.117835818 -0.0216703305 1.3011826470 0.365642020 -0.160775396 ## 4380 -1.231279626 0.2197791713 1.2245051346 0.537738536 0.260468226 ## 4422 -0.143854491 1.5820146845 1.7342452638 -0.801366278 0.353232767 ## 4431 -0.299011319 1.0509912104 0.8593781069 0.306894962 -0.123126904 ## 4432 -0.635768264 0.8880891137 1.2866428561 0.575107675 -0.081327168 ## 4433 -0.725132195 0.7929818067 1.1408830292 0.503429996 -0.057778470 ## 4434 -0.353764331 1.1293663625 1.0431230193 -0.140693696 0.074367129 ## 4436 -1.016733594 0.6563407929 1.3916851672 0.348802668 0.122281308 ## 4470 -0.660465156 0.7531927499 0.7562092286 0.501379618 -0.155139482 ## 4473 -0.247510726 1.1148720125 0.6702423419 -0.115219203 -0.035000779 ## 4479 -0.501936369 0.5565670551 1.0756239387 0.420986324 -0.369225000 ## 4480 -0.332023955 0.8188257388 1.1466376748 -0.331540198 -0.139847046 ## 4482 -0.250843001 0.9127286732 1.1989674728 -0.348159622 -0.166913886 ## 4491 0.304740550 1.3583545891 1.7950120877 -0.753642536 0.136568725 ## 4500 -0.083800903 1.1214805652 1.3985892652 -1.029055128 0.134669363 ## 4501 0.719997855 0.9625352542 1.9145655787 -0.912620053 -0.482840796 ## 4503 -0.415837860 0.8600149717 1.8235814575 -0.188274875 0.653857486 ## 4509 0.018893697 0.9891680113 1.5154162396 -0.097306320 -0.091263592 ## 4511 0.714380389 2.0223481791 1.6070767237 -1.166656408 0.488131428 ## 4513 -0.340922819 0.7908810106 1.7270180146 0.051336295 0.595583432 ## 4520 0.328491154 1.1609536164 1.4807980587 -0.798944687 0.230059676 ## 4526 0.686049069 2.0134703195 1.4244397151 -1.207009810 0.409248447 ## 4527 -0.337662468 0.6106444368 1.7761812637 -0.346940133 0.327642484 ## 4545 0.038860196 0.5421880272 -1.0662789077 -1.234192721 -0.197062516 ## 4552 1.807092435 -0.5224135262 -1.0205997871 0.405947295 -0.351043473 ## 4560 -1.070021295 -0.2680897059 -0.9457778624 -0.021363765 0.160301370 ## 4628 -0.853531262 0.3821924595 1.1719023402 0.210036560 -0.230692021 ## 4637 -0.542704620 1.1461918695 0.9039372082 -1.142502694 0.393479741 ## 4638 -0.601158321 1.1663015121 1.1587110556 -1.151114313 0.460369393 ## 4656 -1.384393367 -0.0603960851 -0.0007339364 0.371280924 0.020531804 ## 4660 -1.326490283 0.0060577448 0.0297264365 0.409718171 -0.028827624 ## 4661 -0.921376339 -0.4595811264 0.7005544449 0.408089853 0.907349020 ## 4670 -1.041358189 0.1425598791 0.6028671734 0.101186170 -0.286011936 ## 4692 0.449466167 -0.3942400865 -0.9456899995 -0.296518700 -0.145997342 ## 4693 1.825978893 -2.2910689330 0.0323443420 -1.833378370 -2.443416277 ## 4694 0.111875964 -0.4039286973 -1.0500502026 -0.115404110 0.219184430 ## 4695 0.167419372 -1.0330944896 -1.0299282504 0.452426895 0.580786004 ## 4697 0.869242840 -0.3368374194 -1.1090093418 -0.083778745 -0.843027551 ## 4700 -1.043295411 0.5808061581 1.0390832653 0.292393246 0.052176711 ## 4706 -0.822667204 0.2890602459 0.7806069121 0.366738406 -0.373312166 ## 4710 -1.113326378 0.4391268284 1.0357181706 0.826196858 -0.107818463 ## 4719 -0.828655202 0.1776079151 -1.1067771072 -0.715000553 0.117197903 ## 4753 -0.434947361 -0.4857397284 1.5945747373 -0.248829044 -0.111377060 ## 4764 -0.299077335 0.1217621146 -0.8008578472 -0.576324091 -0.161615119 ## 4767 0.639794076 -0.8733069144 -0.4246521267 -0.559059400 0.102197635 ## 4772 -1.213787159 0.3812816735 0.7852084213 0.517694820 -0.063908904 ## 4781 -1.376322154 0.3570147501 1.0515440845 0.300935681 0.105193214 ## 4784 -1.001337631 0.6154951055 0.7584423629 -0.145616462 0.133116962 ## 4787 -0.815740955 0.5724503377 0.6932970861 0.615626850 -0.127841113 ## 4798 -1.310937405 0.3560245828 0.9652729580 0.476852184 0.013284213 ## 4799 -0.753205754 0.6776801150 0.8556245259 0.456217046 -0.049415702 ## 4805 -1.283668978 0.4172148186 1.0297373789 0.356232793 0.052549178 ## 4832 -0.967982126 -0.6132741839 -0.8349434104 0.273145766 -0.213080410 ## 4875 -0.798887940 0.1991358274 -1.1292364545 -0.324586504 -0.092369575 ## 4876 -1.226419406 -0.2909083639 -1.3302697657 0.094422311 -0.117370171 ## 4877 -0.848240645 -0.6735077676 -1.2447939272 -0.157366954 -0.066219732 ## 4878 -1.395139586 -0.0460214264 -1.1234884787 -0.153213092 -0.228318854 ## 4882 -0.224823095 0.1845732297 -1.3603808790 -0.414286900 0.160133767 ## 4884 -1.065329537 -0.5445336998 -0.8299920677 0.007673272 -0.202234042 ## 4885 -0.959101399 -0.1419958949 -1.1589146330 0.052280653 -0.030079329 ## 4893 -1.130618787 -0.4895707087 -1.0025747885 -0.026165516 -0.346770023 ## 4894 -1.237710359 -0.1298380303 -0.8489163157 0.225248230 -0.137824356 ## 4899 0.474474982 -1.7173332266 -0.4294530413 -0.822461244 -0.380003459 ## 4903 1.800587120 -3.9986651865 0.2281780689 -1.092619186 0.229111365 ## 4904 1.479399587 -2.7625958050 -0.2793543561 -0.836237482 1.177332496 ## 4906 -0.687699006 -0.3692945468 -0.7543507086 0.088462954 0.474141786 ## 4907 0.324415302 -1.7300634140 -0.8996756698 -0.810229789 1.327754665 ## 4909 1.303146145 -2.5360439178 -0.5393163490 -0.745387758 1.385230421 ## 4910 -0.445959303 -0.3549680331 -0.8074320459 0.144340444 -0.132147994 ## 4911 -0.179109676 -1.9992836667 -0.1428308100 -0.454422533 -0.493932791 ## 4914 -0.326195625 -0.6050177326 -0.9424527320 -0.380560603 0.366266082 ## 4916 1.187835275 -2.2557918714 -0.1127870213 -1.004297953 0.043259480 ## 4981 -0.183716791 0.9675710576 1.4268643774 -0.207473741 -0.150797856 ## 5011 1.907000638 3.3107236394 0.9287949271 -3.935023873 0.883204121 ## 5018 1.186083932 2.6640427937 1.4390306626 -2.612007597 0.721235787 ## 5028 1.096890102 -0.9043793666 -0.3515879599 0.132866573 0.063640491 ## 5054 -0.509990309 -1.2033467494 -0.5913816804 -0.344797125 -0.343885146 ## 5062 -0.593809817 0.1417708149 -0.3116317040 -0.695630062 -0.061592737 ## 5066 -1.247382209 -0.4695732847 -0.7288260737 0.455095395 0.123661246 ## 5068 -0.449293593 -0.9796611637 -0.8577865170 0.003439467 0.511214496 ## 5072 -0.291154774 -0.5935533912 -0.6768942153 -0.636906051 -0.050848185 ## 5077 0.233001513 -1.5399144815 -0.5493575365 -0.165516456 -0.371495475 ## 5086 -0.329131175 -1.1428009844 -0.8688810754 0.023758733 0.706371748 ## 5088 1.354148301 -1.4213709104 -0.5415517110 -1.078892815 0.502603001 ## 5095 -1.033499440 -0.1304698846 -0.6108799991 -0.098588826 -0.438330166 ## 5096 0.058325918 -1.0940564428 -0.7630716031 -0.938848111 0.050623991 ## 5097 -0.426019029 -0.2644178390 -1.1072517043 -0.743169106 0.096348373 ## 5113 -1.136330049 -0.4673421101 -1.1712794422 0.187884521 0.119883511 ## 5116 -0.977023632 -0.2334529988 -1.1408673976 0.245801157 -0.042074881 ## 5118 -0.936010464 -0.0764247683 -1.1179950787 0.157188264 -0.113537108 ## 5120 -1.204743479 -0.2524237783 -1.1292473091 0.202755292 -0.108297594 ## 5123 -1.188596614 -0.2716659860 -1.1814417043 0.279674945 -0.140604097 ## 5126 -0.846066284 -0.5211076059 -1.2582147225 0.018883551 0.359373995 ## 5128 -0.776088786 0.0586735030 -1.1277079434 -0.263479969 0.029899994 ## 5130 -0.646095735 -0.6792778726 -1.1017861861 0.103769303 -0.026567762 ## 5131 -0.489226739 -0.3840894501 -1.1641297439 -0.355761048 -0.034514433 ## 5136 2.349200228 -1.9725131764 1.7198238981 -0.077716973 1.360605541 ## 5139 0.620034011 0.2766918665 1.1865866670 0.048296101 0.691098953 ## 5142 2.738564579 -2.2016526363 1.4851650296 0.332990190 2.784326813 ## 5143 0.719168201 -1.2995981271 1.0276395364 0.620608849 0.574640946 ## 5226 -0.732017680 -0.2782703262 -1.0345813773 0.069980005 0.255658980 ## 5229 -1.396313021 -0.4297594007 -1.0442037041 0.294437095 -0.043634675 ## 5236 -0.873971518 0.4672838015 1.6233429966 0.381884366 0.442084600 ## 5237 -0.675670575 0.0543597430 1.6676668609 0.256005407 0.356684526 ## 5238 -0.511981953 0.4271545129 1.7151155150 -0.476224370 0.458152444 ## 5239 -0.607900569 0.9972270816 1.5261997189 -0.275589757 0.341767739 ## 5277 0.205001593 1.1862701750 1.0833331205 -0.282833079 0.390832574 ## 5298 2.867671389 0.7802948515 -1.7703534866 1.209715782 -0.419726720 ## 5301 -0.443765047 0.8616109997 0.6455131646 0.625118451 -0.264251350 ## 5307 -0.316637404 -0.8245291721 -0.3841516001 -0.145876824 -0.613509725 ## 5312 -0.736267652 0.4681821718 0.8280901946 0.480574407 0.121949478 ## 5314 -1.110210112 0.5904705000 1.1693144590 0.263637251 0.074449593 ## 5334 -0.774320396 0.5286499230 0.1945004771 0.369716442 -0.161837718 ## 5335 -0.868118152 0.3462014323 -0.3491053261 0.251312786 -0.206017182 ## 5343 -0.718932643 0.4606952591 -0.8375723651 -0.575195747 -0.045860489 ## 5344 -0.714356326 0.4591062845 -0.5456590910 -0.106863991 -0.147039772 ## 5365 -1.052113277 0.3146756035 -0.7889230040 -0.781240838 0.042830729 ## 5370 0.860495631 1.5998246784 0.6060727883 0.581864427 -0.286278282 ## 5380 0.821899257 -2.0763468673 1.9474544270 -1.458477047 -1.194744608 ## 5407 -1.162163106 0.5449199594 1.1677878234 0.374840710 0.034940959 ## 5448 0.542600774 1.4281180223 0.9987561396 -0.664271815 -0.196449542 ## 5452 5.088623817 -1.9189654281 0.9820937915 0.277594292 1.545830681 ## 5453 5.449908986 -2.6059089832 1.9458848982 0.168448497 -2.403593111 ## 5472 1.053813484 -0.1787295023 -0.9677483733 -0.088396808 -0.160754193 ## 5480 -0.691691325 -0.4268116568 -0.2622950598 -0.216972466 0.218580062 ## 5490 0.211188476 -0.4380335339 -0.7672870126 -0.206579106 0.464533663 ## 5498 0.301646012 -0.6010758018 -0.7980242815 -0.128520084 1.277778161 ## 5499 -0.570493635 -1.0216787738 -0.5256788499 -0.113630715 0.030763195 ## 5505 0.712819242 -1.0691223975 -0.2685881162 -1.002159902 -0.058633502 ## 5512 -0.248306752 -0.5346265587 -0.2434872131 -0.080375768 0.481850358 ## 5513 -1.143383747 -0.0336416591 -0.3511625534 0.326389718 -0.017390196 ## 5516 -0.877600402 -0.3388170070 -1.1419460092 0.052261883 0.206010775 ## 5517 -0.792629067 -0.9668349267 -0.3704171791 0.386623256 0.352863324 ## 5521 0.250094693 -1.5525979314 -0.1351296158 0.005792943 0.393483115 ## 5526 -0.591125131 -0.8463607793 -0.5253007198 0.582461960 0.176295009 ## 5534 0.158111542 -0.4853614607 -0.8011108991 -0.243220112 0.490305470 ## 5535 -0.579139034 -0.2686968662 -0.1086602221 0.137008693 0.021945486 ## 5538 -0.992643161 -0.4296365866 -0.5832751391 -0.063670743 -0.134101818 ## 5540 -0.111480836 -0.0922852932 -0.3166759217 -0.569901109 0.343362994 ## 5543 -0.228495735 -0.9699856523 -0.9100283515 -0.472045068 0.164691962 ## 5545 0.870590286 -2.9959838851 0.0713271811 -0.400194718 -0.272262343 ## 5547 0.466377918 -0.1629796821 -0.6480514417 -1.280439858 0.235717227 ## 5551 0.364071884 -1.2839002119 -0.4299257965 -0.124913610 0.133357898 ## 5555 -0.704348694 -1.0801866304 -0.2251301683 0.124219690 0.034331221 ## 5559 -0.753386873 -1.0751060745 -0.4291119369 0.011733587 -0.028585550 ## 5570 0.097607183 -0.6635175000 -0.3237888722 -0.280628106 0.195762187 ## 5571 -0.308443230 -1.3864487479 -0.5015512320 -0.337320439 -0.688738469 ## 5572 -0.454780412 -1.1409421047 -0.6878179764 0.023543887 0.067221877 ## 5575 -0.682735469 -0.4569097539 -0.3080662853 0.252669608 0.005323723 ## 5576 -1.169209454 -0.2363178548 -0.4258292987 0.564322023 0.015599976 ## 5577 -0.539952885 -1.1748210240 -0.4517052669 -0.054434576 0.192301274 ## 5578 -1.012142268 -0.5056702475 -0.5412477132 -0.012342919 -0.077627202 ## 5607 -1.315902499 -0.0576140605 -0.2328692258 0.316442981 -0.022599340 ## 5617 -1.270359354 -0.3295349756 -0.8463349850 0.093271175 0.106203692 ## 5663 -0.843714293 -1.4312606027 -0.1299378411 -0.378174906 -0.736760587 ## 5666 -0.295592846 -1.5118973703 0.3426049108 -0.167404033 0.374732060 ## 5668 -0.694197427 -1.7204527683 -0.0508456879 -0.577015986 -0.916600963 ## 5682 -1.715569854 -0.1232945036 -0.3542967436 0.162861350 -0.137149666 ## 5703 -1.628789062 -0.0230929952 -0.2925326926 0.007474925 -0.088339735 ## 5708 -1.514535328 -0.0299477073 -0.3857861006 -0.016336033 -0.053784985 ## 5738 -1.841639594 -0.1675430226 -0.2225874786 0.098079050 -0.067500870 ## 5739 -1.515123866 -0.0511118301 -0.3935689867 0.126687541 -0.108750547 ## 5752 -1.562610846 -0.0072687716 -0.3549502552 0.114312184 -0.151571371 ## 5760 -1.232156728 0.3303891993 -0.3113049455 -0.468026321 -0.011736672 ## 5781 -1.372265117 0.2586831078 -0.1578415711 -0.466377251 0.057952799 ## 5813 -0.678804721 -0.1677863849 0.0554168573 0.350384115 -0.149103556 ## 5832 -0.989629406 -0.3804580257 -0.3521477363 0.481754160 -0.353456053 ## 5908 0.609585581 -0.6824185714 -0.8377908655 -0.405250632 0.326192942 ## 5979 -1.143311248 0.0988670414 -0.5708785546 0.342363360 -0.245636856 ## 5993 -0.097825979 1.0838235639 1.0627786192 -1.060661824 0.040124664 ## 5995 -0.929063842 0.8526625002 1.2059113636 -0.485663315 0.292642125 ## 5997 -0.487348216 0.8086673050 1.4465941092 -0.396914253 -0.048680553 ## 6004 -0.835846786 0.8755090157 1.3730057610 -0.118627407 0.214280890 ## 6005 -0.487201300 1.1530414030 1.1139666479 -0.650999507 0.263293913 ## 6007 -0.480490152 0.7674478682 1.2038173499 -0.419745479 -0.101742229 ## 6010 -0.701957917 0.9614703838 1.1246778510 -0.351519418 0.199549121 ## 6025 -0.995781639 -0.0632883740 -0.8140293614 -0.005687036 0.049857459 ## 6031 -1.104618138 -0.4170086601 -1.0076096023 0.297692159 0.101266530 ## 6037 -0.643473911 0.4243659667 -0.4382005231 0.384442102 -0.265332828 ## 6044 -1.319101718 -0.0151701716 -0.6399964026 0.152399481 -0.155684770 ## 6055 -1.211517152 -0.0766044653 -0.9888894471 -0.047307288 -0.090232418 ## 6066 -0.832495263 0.1084899840 -0.9098051524 -0.045371381 -0.061120525 ## 6067 -0.261626865 0.3839753416 1.3958259463 0.599836498 -0.036407415 ## 6071 -1.288064496 -0.0922426661 -1.0673046327 0.053294043 -0.198142873 ## 6074 -1.158439596 0.0413292711 -1.0949540861 -0.229703058 -0.137749831 ## 6075 -1.057554437 0.1663176609 -0.9076238779 -0.146918547 -0.176222028 ## 6083 -0.681674702 0.2261408101 -1.3137963320 0.263183452 -0.416675883 ## 6095 -0.154590486 1.1567327499 1.3011789385 0.274030881 0.044716814 ## 6096 2.650871223 0.7736171796 0.0973987975 1.289502309 -1.182313728 ## 6098 2.493965325 1.7570401452 0.8721064314 1.897781020 -0.180711563 ## 6100 2.900646319 2.6915513686 0.4337786985 2.102382333 -1.196909904 ## 6101 2.925631362 1.4665352873 0.4815201344 0.695524820 0.745750924 ## 6103 0.872972068 -0.6850805237 -1.2670296801 0.225767225 0.635265917 ## 6116 -1.058524382 -0.2459753281 0.2008261159 0.441413524 -0.251506802 ## 6131 -0.677541570 -0.8442327359 0.2962336310 0.333430736 -0.057000787 ## 6158 -0.586191592 -0.7451544856 0.5055298807 0.413919923 -0.056898912 ## 6177 -0.055614718 -0.2769557316 -0.0810430509 0.188371132 0.285311541 ## 6183 -0.159993950 -0.4687975727 0.6697912508 0.353191927 0.646446462 ## 6189 -0.879227304 -0.1766480763 0.7273953854 0.490454435 0.625852990 ## 6201 -0.075563572 -0.3411248031 -0.5572629674 -0.843094363 -0.708881797 ## 6204 -0.038717095 -0.2633280679 -0.8175078431 -0.518828216 0.311062889 ## 6205 0.525336094 -0.6475899409 -1.1390765192 -0.466158262 0.143655839 ## 6229 1.972615480 -3.4303227829 1.5284375955 -0.274954990 -0.739871366 ## 6230 -0.188131910 -1.0730847592 0.5098321041 0.644339790 0.996807636 ## 6236 0.007243476 -1.4645552117 0.6355052514 0.462062657 0.293423707 ## 6239 1.418827880 -2.9585613923 0.7994412276 0.656201130 1.740411676 ## 6262 1.193071510 0.9603618610 -1.2591531131 -0.105503124 -0.649950101 ## 6327 -0.602167269 -0.0741708332 0.8775535386 0.900792868 0.542893755 ## 6350 -0.597227391 -0.5819201389 1.3839663382 0.221269795 0.236270505 ## 6399 -0.860503753 -0.0238914630 -0.6752312179 0.528046664 -0.124731871 ## 6401 -0.567717988 0.1409618844 -0.8902615040 0.328161953 -0.129624685 ## 6402 -0.115208004 -1.9942432999 0.0868168060 -0.241127929 -1.795799119 ## 6403 -0.597317056 0.1576361312 -1.0487782226 0.305038183 -0.219142385 ## 6404 -0.712793127 0.7885468080 1.2705084303 0.742418340 -0.107575397 ## 6414 2.895383622 -0.4019820006 -0.7997922301 -1.227760485 0.315383603 ## 6415 3.326727082 -0.2663460444 -0.8604501653 -0.704678749 1.431448834 ## 6424 1.028005413 -0.2701350385 -0.5781748951 -0.102806870 0.082208995 ## 6450 -0.923856015 -0.6440653894 -0.7445862160 0.138569229 -0.064294910 ## 6453 -0.801129429 -0.9223531514 -0.6508611583 0.110452305 0.318345225 ## 6459 -0.936267522 0.0454078134 -0.7405261485 0.124959665 -0.060398029 ## 6461 -1.095902257 -0.3728295414 -0.8766547251 0.173787220 0.184515303 ## 6467 1.195703016 0.4378597777 0.7730731958 1.030170441 0.167951774 ## 6468 6.553962670 1.1441028879 -0.3058571848 1.484845138 1.571183118 ## 6472 2.402364509 2.4196839856 -0.1484513426 -0.786901488 0.031938281 ## 6473 5.584764411 -0.4573560719 -1.1493591631 1.562791127 0.299518036 ## 6477 -0.240534903 0.6534169130 0.9406428656 0.706686701 0.127912194 ## 6480 -0.074235486 0.1674373370 1.2240181877 0.942595942 -0.642622837 ## 6490 3.270663456 -0.2921534038 -0.9685862442 0.483418008 0.165584419 ## 6493 0.763924594 0.4635681096 0.6933970756 1.271763518 -0.878922825 ## 6496 4.482971887 -0.3321606051 -1.1174937668 1.396173746 -0.364643187 ## 6498 3.266312549 1.6701180117 -0.3014681937 2.045447046 -0.883004497 ## 6499 2.267581619 1.4662360660 -0.1703601581 1.479884392 -0.870311431 ## 6503 10.141910469 3.8460532080 -1.6644407247 3.214740367 0.162120106 ## 6510 3.685693691 1.6615549584 0.1958235332 0.929665048 -0.582100890 ## 6512 5.575877161 1.2822040248 -1.1610676453 1.916377754 -0.316183331 ## 6514 7.365820821 -0.6243825400 -0.5419682249 2.902272793 0.920772967 ## 6517 4.727858117 1.5326222116 -1.2357107073 2.211116308 -1.146540818 ## 6519 4.453115348 1.8099501598 0.1298636188 1.928045544 -0.808266821 ## 6520 0.225260396 0.6033345046 0.8806481092 1.094567841 -0.463029670 ## 6521 2.090859487 1.5939334949 -0.0293114262 0.309652259 -0.490431421 ## 6529 6.735694547 1.3208926589 -1.6652936087 3.564318393 0.658468193 ## 6542 1.890648034 0.6397109972 0.8541112846 0.625468304 -0.101879403 ## 6543 5.772809058 3.5783894271 0.2181139180 -1.060587435 0.312125194 ## 6554 0.214251146 1.5880037143 1.7706902728 -1.654650553 0.297325111 ## 6556 0.113179344 1.8219479024 1.5854339977 -1.522053809 0.543640412 ## 6563 -0.288010613 0.9311287793 -0.9554986260 -1.619209179 0.164196869 ## 6565 0.286750470 0.4577600070 -0.9014387133 -1.558714167 0.243277616 ## 6569 -0.671478957 0.5225506074 -0.9488573137 -1.221785604 0.183095696 ## 6578 0.258152086 0.8239782281 -0.5626487267 -0.588744444 0.202965678 ## 6579 0.234534536 1.0842765289 -0.7615755197 -0.862468315 -0.016800168 ## 6583 0.365839624 1.0318645625 -0.7681241877 -0.807017959 0.068501844 ## 6604 -0.948938030 -0.8792790468 -1.6305625933 -0.053459313 0.533947800 ## PC6 ## 5 -0.1133342368 ## 7 -0.0777347086 ## 9 -0.0770771052 ## 22 0.1161075153 ## 53 -0.0824282666 ## 56 -0.0371975657 ## 59 -0.0143610530 ## 60 -0.0227393725 ## 61 0.0942088107 ## 78 0.0973253172 ## 81 0.0546702792 ## 83 -0.0817066761 ## 105 0.6582654347 ## 108 0.2442922549 ## 129 0.2969886665 ## 162 0.0700376120 ## 171 -0.0017446083 ## 180 0.0263451574 ## 187 -0.2288134741 ## 192 -0.2957969549 ## 216 0.0034584440 ## 226 -0.1167947988 ## 228 -0.1528052015 ## 230 -0.1347599285 ## 236 -0.1030088556 ## 242 0.0511208309 ## 244 -0.1839746912 ## 269 -0.0373040438 ## 270 -0.0828363228 ## 386 -0.1262068999 ## 407 -0.2276333316 ## 408 -0.0510127381 ## 418 0.3130349910 ## 420 -0.1975533886 ## 451 -0.1156106555 ## 464 -0.2504987962 ## 473 -0.1274183928 ## 491 -0.1666748864 ## 513 -0.2073572911 ## 517 -0.2189248879 ## 536 -0.1764096414 ## 545 -0.0812470258 ## 563 -0.1859031211 ## 569 -0.3258034465 ## 571 -0.0187819412 ## 585 -0.3348923957 ## 594 -0.2103987023 ## 629 -0.2457697782 ## 631 -0.2093540884 ## 634 -0.0752704805 ## 642 -0.2589041064 ## 643 -0.0790349561 ## 684 -0.2366253230 ## 694 -0.2593498881 ## 727 0.0147823152 ## 730 -0.2051792895 ## 732 -0.2764643141 ## 761 -0.2384486983 ## 770 -0.3555715081 ## 814 -0.1689890208 ## 855 -0.1178199255 ## 917 -0.0080155160 ## 919 0.0199894112 ## 921 0.1281015214 ## 924 0.0413900155 ## 926 0.0287572157 ## 927 0.1156514873 ## 929 0.1417204102 ## 930 0.0733901096 ## 931 0.0810073983 ## 937 0.1491684169 ## 945 -0.0609325372 ## 946 0.1170255350 ## 949 0.3343496493 ## 950 -0.0407482953 ## 951 0.0493597664 ## 956 -0.0697602892 ## 966 -0.0581935569 ## 1005 -0.1988559865 ## 1007 -0.3191347391 ## 1008 -0.1730627269 ## 1009 -0.0181201539 ## 1027 1.2279420066 ## 1028 1.0361335393 ## 1029 0.6944743568 ## 1040 1.0503732747 ## 1043 0.6558965245 ## 1044 0.5432382907 ## 1046 0.5611811331 ## 1101 -0.0497822907 ## 1107 -0.0483925910 ## 1109 -0.2041105314 ## 1111 -0.0920862419 ## 1112 -0.0040196255 ## 1114 0.0924321270 ## 1115 0.0607285173 ## 1152 -0.0695250676 ## 1183 0.0835686846 ## 1196 -0.0097276455 ## 1208 -0.1760182056 ## 1213 0.0520749695 ## 1239 -0.2904123504 ## 1258 -0.1869081549 ## 1261 -0.1112210249 ## 1262 -0.0170538628 ## 1297 -0.1013492421 ## 1299 -0.1345439857 ## 1304 -0.1268500402 ## 1344 -0.2242963172 ## 1370 0.1041531976 ## 1375 0.1040570305 ## 1376 0.1089767137 ## 1382 0.2041234979 ## 1384 0.2349246085 ## 1392 0.1847577128 ## 1399 0.1225215842 ## 1403 0.6435640024 ## 1404 0.3506874766 ## 1405 0.8119626180 ## 1409 -0.0090528515 ## 1411 0.4879393066 ## 1431 -0.1313359563 ## 1432 0.0607512362 ## 1439 -0.3951129549 ## 1581 -0.2112301763 ## 1600 -0.0614821567 ## 1601 0.0960235862 ## 1606 0.1072941066 ## 1607 -0.0247815260 ## 1630 -0.1201624977 ## 1631 0.0287288657 ## 1641 -0.0219846564 ## 1653 0.0139928758 ## 1655 0.1495387421 ## 1672 0.0410736901 ## 1674 2.1100365632 ## 1679 -0.0341740192 ## 1683 -0.2941216064 ## 1688 0.1351686295 ## 1689 -0.1043909633 ## 1692 0.2134738055 ## 1693 0.4090101899 ## 1698 0.0376044453 ## 1757 -0.2441416662 ## 1764 -0.0071575321 ## 1766 -0.0736818152 ## 1767 0.0300327071 ## 1768 0.0187020467 ## 1772 -0.0545316258 ## 1773 0.1209687498 ## 1794 -0.2099808947 ## 1796 -0.0496539563 ## 1798 0.1036826256 ## 1845 0.0151761093 ## 1870 -0.0187107804 ## 1873 0.0458486223 ## 1890 0.0093862958 ## 1904 0.1864152320 ## 1910 -0.5996426783 ## 1912 -0.2305251928 ## 1913 -0.9499688826 ## 1914 0.0339343942 ## 1915 -0.6197046159 ## 1916 -0.1519113883 ## 1917 0.0899940231 ## 1919 0.1154342601 ## 2073 0.1364582888 ## 2094 -0.0338583521 ## 2152 -0.1498631415 ## 2175 0.1147628250 ## 2225 0.0159552839 ## 2230 -0.0268865544 ## 2234 -0.0267115572 ## 2236 -0.0172349956 ## 2241 -0.0505900611 ## 2245 0.0169724428 ## 2246 -0.1147411990 ## 2249 -0.0996030909 ## 2252 0.0742936569 ## 2254 0.0885296463 ## 2275 0.0343412473 ## 2326 -0.0484538805 ## 2332 -0.0638658399 ## 2351 -0.1117150149 ## 2394 -0.1330975246 ## 2396 0.1747202976 ## 2447 -0.0531596993 ## 2452 -0.0766243248 ## 2465 -0.0392499144 ## 2472 0.6740648435 ## 2473 0.4228986214 ## 2480 0.6994194853 ## 2481 0.4236097696 ## 2485 -0.0369561103 ## 2487 0.2705968457 ## 2495 -0.0785624878 ## 2509 -0.1695485164 ## 2515 -0.0686595266 ## 2598 0.1244738812 ## 2612 0.0160428593 ## 2614 -0.0357317231 ## 2618 0.0190083898 ## 2619 0.0085762166 ## 2622 -0.0143413854 ## 2624 0.1942327205 ## 2628 -0.0306857399 ## 2643 -0.0747723380 ## 2648 0.6827429393 ## 2650 0.2031840031 ## 2660 -0.0117051056 ## 2661 0.1144795574 ## 2662 0.1426532791 ## 2669 0.4834426961 ## 2678 -0.1031314860 ## 2680 0.2091288965 ## 2717 0.1046195472 ## 2754 0.0554999943 ## 2756 -0.4328946739 ## 2757 0.0217781913 ## 2761 0.1057993517 ## 2762 0.1342834375 ## 2768 -0.0996180551 ## 2771 -0.0575198286 ## 2772 -0.1267911595 ## 2773 0.0011533879 ## 2774 0.0193656891 ## 2776 -1.2413892466 ## 2777 -0.7999050780 ## 2779 0.3025540676 ## 2780 0.3369523443 ## 2818 -0.1426159979 ## 2835 -0.1876524301 ## 2854 -0.1937050047 ## 2906 0.0593974283 ## 2944 -0.1561345975 ## 2947 -0.1478205805 ## 2957 -0.2131883683 ## 2974 -0.1871691544 ## 2976 0.2748564807 ## 2979 0.1759197195 ## 2982 0.0322413966 ## 2999 -0.3242152522 ## 3028 -0.1344851258 ## 3046 0.7387962796 ## 3047 1.8956878809 ## 3061 -0.1963860495 ## 3075 -0.2333488269 ## 3091 -0.1555272611 ## 3095 -0.1629229965 ## 3097 -0.0925166971 ## 3111 -0.1003718557 ## 3136 -0.2566838373 ## 3164 -0.1609579083 ## 3193 -0.0620867638 ## 3203 -0.0797331687 ## 3231 0.1599570368 ## 3233 -0.0339020610 ## 3235 -0.0277180546 ## 3236 -0.1557700307 ## 3241 -0.1871364233 ## 3244 -0.1882465387 ## 3250 -0.0254177312 ## 3251 0.0901886082 ## 3255 0.0704536583 ## 3259 0.0549358504 ## 3266 -0.2261841888 ## 3281 -0.3236697926 ## 3304 -0.0158923509 ## 3305 0.0669175793 ## 3306 -0.0424982999 ## 3307 -0.0034043088 ## 3309 0.0765634862 ## 3310 -0.1151861093 ## 3311 -0.0248029229 ## 3313 -0.0716182328 ## 3318 0.0363309680 ## 3319 0.0926052887 ## 3324 -0.1544260396 ## 3346 -0.1649909017 ## 3359 -0.1746723356 ## 3411 0.1296543484 ## 3413 -0.0156593920 ## 3415 0.1384290310 ## 3416 -0.0837999254 ## 3418 0.0796617778 ## 3426 -0.0054035361 ## 3429 0.0602492758 ## 3436 -0.1452188149 ## 3438 -0.1211591998 ## 3492 -0.0435451117 ## 3552 -0.1659645738 ## 3697 0.1540119798 ## 3739 -0.1082006822 ## 3861 -0.0711805704 ## 3904 -0.1492801272 ## 3948 -0.1676459735 ## 3995 0.0751237272 ## 3996 0.7618036470 ## 3998 0.2132675773 ## 4000 -0.0080478779 ## 4005 0.0992936157 ## 4034 0.0565103163 ## 4037 -0.1185268546 ## 4047 -0.0147252065 ## 4049 -0.1097331715 ## 4051 -0.1400830778 ## 4064 -0.1721744089 ## 4071 -0.1872288876 ## 4076 -0.0386959739 ## 4077 -0.0924131927 ## 4115 -0.2795677459 ## 4151 -0.3402549501 ## 4183 -0.3770633421 ## 4235 0.0709219340 ## 4251 -0.1525215605 ## 4253 -0.2380939916 ## 4255 -0.1627613345 ## 4269 -0.2431028840 ## 4272 0.0241777673 ## 4274 0.0644074683 ## 4278 0.1373445693 ## 4285 0.0052375215 ## 4287 -0.0388748373 ## 4291 0.0170565004 ## 4292 -0.0271106530 ## 4309 -0.2398819035 ## 4328 0.0135024040 ## 4339 0.0057500174 ## 4358 0.0346205767 ## 4380 -0.1213767156 ## 4422 0.0132009063 ## 4431 0.1830174239 ## 4432 0.0907741036 ## 4433 -0.0035068120 ## 4434 0.0040822559 ## 4436 -0.3494358930 ## 4470 0.1440700741 ## 4473 0.1638199118 ## 4479 0.0225584570 ## 4480 -0.0756111889 ## 4482 0.0884153931 ## 4491 -0.0011640990 ## 4500 -0.0587431094 ## 4501 0.2402700557 ## 4503 -0.0134945430 ## 4509 0.1072650181 ## 4511 -0.0117214092 ## 4513 0.1332435651 ## 4520 -0.0548881406 ## 4526 0.3013271711 ## 4527 -0.0340443005 ## 4545 0.1408430017 ## 4552 -0.2266680699 ## 4560 -0.0480097616 ## 4628 -0.1365753027 ## 4637 -0.2010012670 ## 4638 -0.2898365415 ## 4656 -0.2267475951 ## 4660 -0.0368914172 ## 4661 -0.0470389465 ## 4670 -0.0852593611 ## 4692 0.0527416404 ## 4693 0.1286566391 ## 4694 0.1448233176 ## 4695 0.2895882143 ## 4697 0.4966943505 ## 4700 -0.1949065592 ## 4706 0.0220291359 ## 4710 -0.1230115224 ## 4719 -0.0102698692 ## 4753 -0.1513128046 ## 4764 0.0129031059 ## 4767 -0.1281264826 ## 4772 -0.0653092788 ## 4781 -0.3137719264 ## 4784 -0.2256341115 ## 4787 -0.1341639616 ## 4798 -0.2031511582 ## 4799 -0.1959467324 ## 4805 -0.1885732870 ## 4832 -0.0491716572 ## 4875 0.1448006628 ## 4876 0.0108869721 ## 4877 0.0541370978 ## 4878 -0.0700674312 ## 4882 0.1050438656 ## 4884 0.0115589455 ## 4885 -0.0371634601 ## 4893 0.0558210117 ## 4894 0.0162963348 ## 4899 0.0386746976 ## 4903 0.2502910098 ## 4904 0.5504032019 ## 4906 0.0569094382 ## 4907 0.2634902944 ## 4909 0.4388505176 ## 4910 0.1332599739 ## 4911 -0.0055214782 ## 4914 0.0663706395 ## 4916 0.3112850539 ## 4981 -0.0349015489 ## 5011 0.2374200925 ## 5018 0.2146475603 ## 5028 -0.1354090834 ## 5054 0.0189758654 ## 5062 -0.0973322151 ## 5066 0.0219141961 ## 5068 0.1497885614 ## 5072 -0.0098401910 ## 5077 -0.0424020908 ## 5086 0.3110896128 ## 5088 0.3827625162 ## 5095 -0.0300813582 ## 5096 0.0334811373 ## 5097 -0.0038337249 ## 5113 0.0109365310 ## 5116 0.0777643842 ## 5118 -0.0525767832 ## 5120 -0.0338377736 ## 5123 -0.0307246125 ## 5126 0.0826403925 ## 5128 0.0536040043 ## 5130 0.1270211952 ## 5131 0.0830532471 ## 5136 0.1099908966 ## 5139 0.0672836234 ## 5142 0.5677018003 ## 5143 0.3600034305 ## 5226 0.0456640785 ## 5229 -0.0259848939 ## 5236 -0.0290769469 ## 5237 -0.0493665554 ## 5238 -0.1676061884 ## 5239 -0.0226108975 ## 5277 -0.0540578975 ## 5298 -0.6392910791 ## 5301 0.3272441400 ## 5307 -0.0603830792 ## 5312 0.1095945285 ## 5314 -0.1308453363 ## 5334 -0.0400942579 ## 5335 -0.0513116121 ## 5343 -0.1436962997 ## 5344 -0.0665174773 ## 5365 -0.1090681787 ## 5370 -0.1058886319 ## 5380 -0.1161970870 ## 5407 -0.0695462622 ## 5448 0.1697631220 ## 5452 0.2914375550 ## 5453 -0.6717826216 ## 5472 0.1607019309 ## 5480 0.0575438780 ## 5490 0.1547381683 ## 5498 0.1750521746 ## 5499 0.0093817945 ## 5505 -0.0986048346 ## 5512 0.2098960332 ## 5513 -0.0586764050 ## 5516 -0.0106868065 ## 5517 0.0254955664 ## 5521 0.1718035377 ## 5526 0.1082720096 ## 5534 0.0744506628 ## 5535 0.0004144472 ## 5538 -0.0593271704 ## 5540 -0.0589048002 ## 5543 -0.0012158368 ## 5545 0.1361781311 ## 5547 0.0782758115 ## 5551 0.1387178065 ## 5555 -0.0415994707 ## 5559 -0.0192326455 ## 5570 -0.0949250580 ## 5571 -0.0859482956 ## 5572 0.0643589026 ## 5575 0.1100765435 ## 5576 0.0044824858 ## 5577 0.0858018326 ## 5578 -0.0642431933 ## 5607 -0.1528117667 ## 5617 -0.0776393363 ## 5663 -0.1711133825 ## 5666 -0.0874358179 ## 5668 -0.1809142689 ## 5682 -0.2061805038 ## 5703 -0.2183631368 ## 5708 -0.1766360876 ## 5738 -0.2992792501 ## 5739 -0.1109104420 ## 5752 -0.1340952368 ## 5760 -0.1196125952 ## 5781 -0.2824802599 ## 5813 -0.1895275897 ## 5832 0.1425004795 ## 5908 -0.2903750098 ## 5979 -0.0406290567 ## 5993 -0.1145260807 ## 5995 -0.2107171521 ## 5997 -0.1263325781 ## 6004 -0.2437512028 ## 6005 -0.1007659083 ## 6007 -0.0296587980 ## 6010 -0.1023901604 ## 6025 -0.0584323135 ## 6031 0.0669305807 ## 6037 -0.1142228178 ## 6044 -0.1985176051 ## 6055 -0.2658869491 ## 6066 0.0940564401 ## 6067 0.1023839141 ## 6071 -0.2064438020 ## 6074 -0.1876789745 ## 6075 -0.0066129199 ## 6083 0.1182171733 ## 6095 0.2988075750 ## 6096 1.0509803059 ## 6098 0.9835162331 ## 6100 1.8509645015 ## 6101 0.6219509179 ## 6103 -0.1621928763 ## 6116 -0.1357159705 ## 6131 -0.0840412573 ## 6158 -0.0690908736 ## 6177 -0.0607766111 ## 6183 0.0912129234 ## 6189 0.0170743487 ## 6201 0.2318273879 ## 6204 -0.1006949741 ## 6205 -0.1150383261 ## 6229 0.3468531214 ## 6230 0.2670347029 ## 6236 0.1446201739 ## 6239 0.4801776599 ## 6262 0.4455944713 ## 6327 -0.1480868691 ## 6350 0.0691966520 ## 6399 0.1103645451 ## 6401 0.1138382816 ## 6402 -0.0127623674 ## 6403 0.1707205095 ## 6404 0.0068427705 ## 6414 -0.5819138932 ## 6415 -0.1587150656 ## 6424 0.2154711989 ## 6450 0.0649411115 ## 6453 0.0575187262 ## 6459 0.0331213664 ## 6461 -0.0702533206 ## 6467 0.3928259009 ## 6468 -0.3614222857 ## 6472 0.1738175944 ## 6473 -1.6131377437 ## 6477 0.2455059869 ## 6480 0.3133870272 ## 6490 -0.4485601698 ## 6493 0.2285821533 ## 6496 -0.0165537652 ## 6498 -0.0318060735 ## 6499 0.0232129932 ## 6503 -2.4707134252 ## 6510 -0.0053496661 ## 6512 0.2468795281 ## 6514 -0.4654193068 ## 6517 -0.0050417076 ## 6519 -0.6745863382 ## 6520 0.2855208653 ## 6521 0.5827556950 ## 6529 -1.7974869301 ## 6542 0.2712765535 ## 6543 -1.0578445618 ## 6554 -0.1228409796 ## 6556 -0.1515413024 ## 6563 -0.0034276062 ## 6565 -0.0388880373 ## 6569 -0.1768242087 ## 6578 0.2632692323 ## 6579 0.3682483470 ## 6583 0.4141896215 ## 6604 -0.0216725419 # How much variance is explained by each of the components? summary(pcaRes) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 1.6122 1.1845 0.9900 0.7777 0.56041 0.31437 ## Proportion of Variance 0.4332 0.2338 0.1633 0.1008 0.05234 0.01647 ## Cumulative Proportion 0.4332 0.6670 0.8304 0.9312 0.98353 1.00000 # This plot shows the percentage of variance explained by each of the components. fviz_eig(pcaRes, addlabels = T) # This plot visualizes both the principal components and the original variables. # Longer arrows indicate greater contribution fviz_pca_var(pcaRes, # colour by contribution col.var = &quot;contrib&quot;, gradient.cols = &quot;Paired&quot;, repel = T, xlab = &quot;Principal Component 1&quot;, ylab = &quot;Principal Component 2&quot;) + ## takes ggplot arguments theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;), plot.subtitle = element_text(hjust = 0.5)) # Next step would be to extract the components to use in an analysis down the road (covered in later modules) # From here, keep the variables you are interested in exploring for your analysis. # But beware! Imputation prep is next, and it&#39;s not so forgiving! dfTraits &lt;- dfTraits %&gt;% select(family, genus, species, latitude, insular_endemic, maximum_svl, hatchling_neonate_svl, activity_time, diet, foraging_mode, reproductive_mode, largest_clutch, age_first_breeding, iucn_redlist_assessment) # Write dataset to file for next module. write.csv(dfTraits, &quot;dfTraits.csv&quot;, row.names = F) "],["module-2-dealing-with-missingness.html", "Module 2: Dealing with Missingness Lecture Lab", " Module 2: Dealing with Missingness Lecture Lab Load libraries. —- library(tidyverse) library(mice) library(missForest) library(VIM) library(simputation) library(naniar) Read in data. —- Read in trait dataset we cleaned in the last module. dfTraits &lt;- read.csv(file = “dfTraits.csv”) Identify the taxonomy columns. taxCols &lt;- c(“family”, “genus”, “species”) ##### Identify the traits. traits &lt;- setdiff(names(dfTraits), taxCols) ##### Identify the numerical traits. contTraits &lt;- traits[sapply(dfTraits[traits], is.numeric)] ##### Identify the categorical traits. catTraits &lt;- setdiff(traits, contTraits) Missing data exploration. —- Let’s make a dataframe for plotting. dfMissing &lt;- dfTraits[traits] %&gt;% # Get number of NAs in each trait. # across() allows us to apply our function across multiple columns, everything() selects all columns and ~ sum(is.na(.)) is our anonymous function summarise(across(everything(), ~ sum(is.na(.)))) %&gt;% # Pivot longer for plotting. pivot_longer and pivot_wider are useful functions for getting your data into a format for plotting. It also help with tidying messy data. # Here, we are selecting all of the columns and flipping it longways so that trait name is one column and the number of NAs is the other columns. pivot_longer(cols = everything(), names_to = “trait”, values_to = “num_NAs”) %&gt;% # Finally, add a column for percentage of missing values. mutate(pct_missing = (num_NAs/nrow(dfTraits)) * 100) Note: it can help to make interim variables when you are trying to understand or debug code. One of the downsides of the tidyverse pipe! View(dfMissing) ggplot for missingness visualization ggplot(dfMissing) + # Barplot ordered by missingness # reorder function from R documentation: “The”default” method treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric.” geom_bar(aes(x = reorder(trait, desc(pct_missing)), y = pct_missing), stat = ‘identity’, fill = “seagreen”, width = 0.7) + coord_flip() + labs(title = “Percentage of Missing Values by Trait”, x = ‘Trait’, y = “% of Missing Values”) + theme_minimal(base_size = 12) Using the package “naniar” to visualize missing data. naniar::vis_miss(dfTraits[traits]) Clustering by missingness may help you see patterns naniar::vis_miss(dfTraits[traits], cluster = T, sort_miss = T) Upset plots to identify missingness combinations. naniar::gg_miss_upset(dfTraits[traits]) Here’s the kicker! How many species do we lose if we use complete case observations? dfCompleteCase &lt;- na.omit(dfTraits) cat(“We lose”, nrow(dfTraits) - nrow(dfCompleteCase), “species!”) Imputation prep. —- Only some traits are amenable to imputation and we need to do some more preprocessing before we impute our values. names(dfTraits) ##### Set row names as species so we have a form of ID. rownames(dfTraits) &lt;- dfTraits$species Imputing traits with high proportions of missingness could be problematic. Based on our visualizations, let’s set a threshold of 60% missingness as our cutoff. Note: this is an arbitrary threshold I am using right now. In reality, this is something you could perform sensitivity analyses on. There is no “right” answer for how much missingness is too much. It’s a combination of many factors (e.g., sample size, strength of relationships between variables, missingness mechanism, etc.). So you probably don’t want to impute a variable with only a few observations and 90% missingness, for example. dfFiltered &lt;- dfTraits %&gt;% select(-c(all_of(taxCols), ## leave out taxonomy also for imputation purposes age_first_breeding, foraging_mode, hatchling_neonate_svl)) ## traits with most amount of missingness)) Update our trait vectors. catTraits &lt;- catTraits[!catTraits %in% “foraging_mode”] contTraits &lt;- contTraits[!contTraits %in% c(“age_first_breeding”, “hatchling_neonate_svl”)] traits &lt;- c(catTraits, contTraits) Visualize the missingness of our subset. This also gives you an idea of which combination of traits would give you the largest complete-case dataset. naniar::vis_miss(dfFiltered, cluster = T, sort_miss = T) Imputation methods can be sensitive to class imbalance issues, as some categories may be over or underrepresented. Let’s check our categorical traits for severe class imbalances or lack of variation or just for categories we should fix in general. lapply(dfFiltered[catTraits], table) Look at diet. “Herbivorous” is a minor category. We check if this category makes up more than 10% of the observed data. sum(dfFiltered\\(diet == &quot;Herbivorous&quot;, na.rm = T)/sum(dfFiltered\\)diet != “Herbivorous”, na.rm = T) ##### Consider removing rare categories (those less than 10% of the data, for example). You could use a different threshold for your data if you had a large enough sample size. ##### Let’s remove the rare categories: dfFiltered &lt;- dfFiltered %&gt;% filter(diet != “Herbivorous” | is.na(diet), activity_time != “Cathemeral” | is.na(activity_time), insular_endemic != “unknown” | is.na(insular_endemic), reproductive_mode == “Oviparous” | reproductive_mode == “Viviparous” | is.na(reproductive_mode), iucn_redlist_assessment == “LC” | iucn_redlist_assessment == “NE” | is.na(iucn_redlist_assessment) ) ## need to specify that we are keeping NAs because filter will remove them! Check distributions of numerical traits once more. lapply(dfFiltered[contTraits], hist) Finally, let’s ensure we have the correct data classes. dfFiltered[contTraits] &lt;- lapply(dfFiltered[contTraits], as.numeric) ##### Factor is the class required for imputation of categorical traits. It is also the data class required for categorical variables in many statistical analyses. dfFiltered[catTraits] &lt;- lapply(dfFiltered[catTraits], as.factor) Missingness simulation. —- The first thing we are going to do is make a complete-case dataset. dfCompleteCase &lt;- na.omit(dfFiltered) We are going to log-transform our skewed numerical traits to make the distributions approximately normal. This can help with certain types of imputation methods, but isn’t always necessary. dfLog &lt;- dfCompleteCase ## making a copy of the complete-case dataframe dfLog[c(“maximum_svl”, “largest_clutch”)] &lt;- lapply(dfLog[c(“maximum_svl”, “largest_clutch”)], log) Now, set the seed so you can reproduce your results. This is VERY important! set.seed(123) ##### To simulate missingness, let’s introduce some NAs into the dataset using the prodNA function. Any data introduced using this function is missing completely at random (MCAR). If you are interested in simulated MAR or MNAR data, check out the mice::ampute function. ?prodNA ##### noNA = the proportion of missingness we want to introduce dfMissing &lt;- prodNA(dfLog, noNA = 0.1) ##### Let’s make sure it worked. sum(is.na(dfMissing))/prod(dim(dfMissing)) View(dfMissing) Mean/mode imputation. —- Mean/mode is the simplest imputation method and it is pretty easy to do. It’s a good baseline to compare other methods to. Make a copy of dfMissing. dfMeanModeImp &lt;- dfMissing Impute traits with mean for numerical traits and mode for categorical traits. Mean value mean(dfMeanModeImp$latitude, na.rm = T) Mode value table(dfMeanModeImp\\(activity_time) which.max(table(dfMeanModeImp\\)activity_time)) names(which.max(table(dfMeanModeImp$activity_time))) Use a for loop!: for(t in 1:length(traits)) { # Take the name of the tth trait. trait &lt;- traits[[t]] # Identify missing values in the trait. index &lt;- which(is.na(dfMeanModeImp[trait])) # If trait is numeric.. if(trait %in% contTraits){ # Replace all NA values with the mean of the known observations for the variable. dfMeanModeImp[index, trait] &lt;- mean(dfMeanModeImp[[trait]], na.rm = T) } else if(trait %in% catTraits){ # Replace all NA values with the mode of the known observations for the variable. dfMeanModeImp[index, trait] &lt;- names(which.max(table(dfMeanModeImp[[trait]]))) } } Back-transform the log-transformed data because we will need to compare it to the original data to get our error rates. dfMeanModeImp[c(“maximum_svl”, “largest_clutch”)] &lt;- lapply(dfMeanModeImp[c(“maximum_svl”, “largest_clutch”)], exp) K-Nearest Neighbour Imputation —- Let’s try using KNN (K-Nearest Neighbour Imputation). Using a distance matrix, determines k neighbours that are closest to the observation with missing values. It then uses the information from the nearest neighbours to fill in the missing values. ?kNN data = dataframe with missing values, k = number of nearest neighbours used. Rule of thumb for k is to take the square root of n (sample size). This might be a parameter you want to tune, however. neighbours &lt;- round(sqrt(nrow(dfMissing))) dfKNN &lt;- kNN(data = dfMissing, k = neighbours) View(dfKNN) ## This is nice because it lets you track your imputed values. Subset to only take the imputed values and not the indicator columns. dfKNN &lt;- dfKNN[, -grep(pattern = “_imp”, x = names(dfKNN))] ##### Back-transform the log-transformed data. dfKNN[c(“maximum_svl”, “largest_clutch”)] &lt;- lapply(dfKNN[c(“maximum_svl”, “largest_clutch”)], exp) Random forest imputation —- Builds a Random Forest model using the observed data to estimate those values that are missing. It is an iterative process that repeats until the imputed values stabilize. ?missForest xmix = dataframe with missing values, maxiter = max # of iterations performed, ntree = number of trees in the forest RFresult &lt;- missForest(xmis = dfMissing, maxiter = 10, ntree = 100) Look at the imputed dataframe. View(RFresult\\(ximp) class(RFresult\\)ximp) Assign to dataframe. dfRF &lt;- RFresult$ximp Again, back-transform the data. dfRF[c(“maximum_svl”, “largest_clutch”)] &lt;- lapply(dfRF[c(“maximum_svl”, “largest_clutch”)], exp) MICE. —- MICE is a framework for performing multiple imputation. Single imputation is performed several times, so it can provides a measure of uncertainty for your imputed values ?mice Default methods for numeric and categorical traits are predictive mean matching and logistic regression, respectively. But you can try different methods and see which works best for your data. sapply(dfMissing, class) myMethods &lt;- c(“norm.predict”, “cart”, “pmm”, “logreg”, “logreg”, “cart”, “pmm”, “cart”) ##### data = dataframe with missing values, m = number of multiple imputations. miceMids &lt;- mice(data = dfMissing, method = myMethods, m = 10) Check the class. class(miceMids) ##### mids is a special object that contains multiple imputed datasets. ?mids Number of imputed datasets miceMids$m What variables were used as predictors (you can alter this matrix) miceMids$predictorMatrix View imputed data. View(miceMids\\(imp\\)maximum_svl) ## This contains the imputed values for each of the 5 datasets. View(miceMids\\(imp\\)insular_endemic) If you are running a statistical analysis on data imputed using MICE, you will have to apply it to EACH of the imputed datasets. For example: Fitting a linear regression to the multiply imputed datasets using the with() function. fit &lt;- with(miceMids, lm(maximum_svl ~ latitude + insular_endemic)) What class is it? class(fit) ## special class called “mira” that contained results of analyses repeated across MI datasets ##### Let’s see what it contains. fit ## Results of all the different regressions ! Let’s pool the results together using the pool function. From the mice documentation: “The pool() function combines the estimates from m repeated complete data analyses.” pool.fit &lt;- pool(fit) summary(pool.fit) ?pool ## if you are interested in how the pooling is performed Get a list of the imputed dataframes using the mice::complete() function. Since we specified “all”, it will return us a list containing all of the imputed dataframes. Should be length “m”. l_dfMICE &lt;- mice::complete(miceMids, “all”) View(l_dfMICE[[1]]) Back-transform all the imputed values in this list. l_dfMICE &lt;- lapply(l_dfMICE, function(x){ x[c(“maximum_svl”, “largest_clutch”)] &lt;- lapply(x[c(“maximum_svl”, “largest_clutch”)], exp) return(x) }) Error rates. —- Let’s see how well the different imputation methods predicted the values. First back-transform dfMissing for comparison purposes dfMissing[c(“maximum_svl”, “largest_clutch”)] &lt;- lapply(dfMissing[c(“maximum_svl”, “largest_clutch”)], exp) We can use did the missForest::mixError() function for obtaining error rates. mixError is a useful function because it tracks which values were imputed for you, as long as you can provide it with the original and missing dataframes. missForest::mixError(ximp = dfMeanModeImp, xmis = dfMissing, xtrue = dfCompleteCase) NRMSE refers to the normalized root mean squared error for the numeric data and PFC refers to the proportion of falsely classified entries for the categorical variables. For both, lower is better. If we wanted the error for one variable? missForest::mixError(ximp = dfMeanModeImp[“maximum_svl”], xmis = dfMissing[“maximum_svl”], xtrue = dfCompleteCase[“maximum_svl”]) ## Remember it wants a dataframe as input! How about KNN? missForest::mixError(ximp = dfKNN, xmis = dfMissing, xtrue = dfCompleteCase) ##### A bit better than mean/mode. How about missForest? missForest::mixError(ximp = dfRF, xmis = dfMissing, xtrue = dfCompleteCase) ##### Better on the numerical traits but worse on categorical compared to KNN. What if we wanted to get the errors for the MICE datasets? Apply the mixError function across the imputed dataframes. l_MICEerrors &lt;- lapply(l_dfMICE, function(x) mixError(ximp = x, xmis = dfMissing, xtrue = dfCompleteCase)) Let’s average the errors using the Reduce() function, which can used to apply a function over a list and then return a single result. Here, we are adding the elements of l_Errors and dividing them by the length of l_Errors. Reduce(“+”, l_MICEerrors)/length(l_MICEerrors) ## probably need to pick better methods Impute the full dataset. —- From here, choose the best-performing method on the complete-case data and use it to impute your target dataset. Ideally, it would be the method that resulted in the lowest error rate for the majority of variables. imputeRes &lt;- missForest(xmis = dfFiltered) ## may take a while Extract imputed dataset. dfImputed &lt;- imputeRes$ximp Combine dataframes into list. l_dfAll &lt;- list(dfCompleteCase, dfRF, dfFiltered, dfImputed) Name the list according to dataframe. names(l_dfAll) &lt;- c(“CC”, “SIM”, “O”, “IMP”) We need to make some alterations for plotting. for(i in 1:length(l_dfAll)){ # Get name of dataframe. ID &lt;- names(l_dfAll)[[i]] # Add ID to column names of dataframe so we can identify which dataframe it came from. names(l_dfAll[[i]]) &lt;- paste(ID, colnames(l_dfAll[[i]]), sep = “_“) # Add species col so we can merge them together. l_dfAll[[i]]$species &lt;- rownames(l_dfAll[[i]]) } View(l_dfAll[[1]]) Merge all the dataframes by species. dfAll &lt;- Reduce(function(…) merge(…, by = “species”, all = T), l_dfAll) names(dfAll) From here, we can make plots pretty easily. dfSubset &lt;- select(dfAll, c(CC_largest_clutch, SIM_largest_clutch, O_largest_clutch, IMP_largest_clutch)) Log transform data for better visualization. dfSubset &lt;- as.data.frame(lapply(dfSubset, log)) ## sometimes dataframe format is easier to deal Get sample size counts for each column. sampleSizes &lt;- sapply(dfSubset, function(x) sum(!is.na(x))) sampleSizes Pivot dataframe to long form so we can more easily plot variables by group. dfPivot &lt;- pivot_longer(dfSubset, cols = colnames(dfSubset)) View(dfPivot) Convert to factor. dfPivot\\(name &lt;- as.factor(dfPivot\\)name) X-axis label containing sample size information. dataType &lt;- c(“Complete case”, “Sim-imputed”, “Original”, “Original - Post-imputation”) Paste sample size onto dataType vector. xlabel &lt;- paste(dataType, “”, “(n =”, sampleSizes, “)” , sep = ““) GGplot comparing trait distributions ggplot(dfPivot, aes(x = name, y = value, fill = name)) + geom_boxplot() + labs(title = trait, y = ““, x =”“) + scale_x_discrete(labels = xlabel) + scale_fill_brewer(palette =”Set3”) + theme_minimal(base_size = 12) + # Base font size theme(legend.position = “none”) + theme(axis.text = element_text(size = 10, face = “bold”)) Let’s plot a categorical trait now! names(dfAll) Same steps as above. dfSubset &lt;- select(dfAll, c(CC_diet, SIM_diet, O_diet, IMP_diet)) sampleSizes &lt;- sapply(dfSubset, function(x) sum(!is.na(x))) Create dataframe containing category counts for each group. dfPivot &lt;- pivot_longer(dfSubset, cols = colnames(dfSubset)) Convert to factor. dfPivot\\(name &lt;- as.factor(dfPivot\\)name) Group by name and value to prepare data for plotting. dfCount &lt;- dfPivot %&gt;% na.omit() %&gt;% group_by(name, value) %&gt;% # Get counts by group summarise(count = n()) %&gt;% mutate(prop = count/sum(count)) View(dfCount) Paste sample size onto dataType vector. xlabel &lt;- paste(dataType, “”, “(n =”, sampleSizes, “)” , sep = ““) Barplot comparing trait categories of complete-case, pre- and post-imputation. ggplot(data = dfCount, mapping = aes(x = name, y = prop, fill = value)) + geom_bar(stat = “identity”, position = “dodge”) + scale_x_discrete(labels = xlabel) + scale_fill_brewer(palette = “Blues”) + geom_text(aes(label = scales::percent(prop, accuracy = 0.1)), vjust = -.5, position = position_dodge(0.9), size = 3) + theme(axis.text=element_text(size = 10, face = “bold”)) + labs(title = trait, y = ““, x =”“) + theme_minimal() Compare inferences obtained from complete-case and imputed datasets. —- Finally, something you should consider after finishing your analysis..re-doing the analysis using the complete-case dataset and comparing it to the results obtained using your final imputed dataset. How do the results change? Logistic regression ccModel &lt;- glm(diet ~ activity_time + maximum_svl + reproductive_mode + insular_endemic + latitude, data = dfCompleteCase, family = “binomial”) summary(ccModel) simModel &lt;- glm(diet ~ activity_time + maximum_svl + reproductive_mode + insular_endemic + latitude, data = dfRF, family = “binomial”) summary(simModel) targetModel &lt;- glm(diet ~ activity_time + maximum_svl + reproductive_mode + insular_endemic + latitude, data = dfImputed, family = “binomial”) summary(targetModel) One last plot to compare how results change between complete-case and imputed models. cols &lt;- c(“#F3E79A”, “#ED7C97”, “#9F7FCD”) # Create dataframe of p-values from complete-case and imputed models. dfPvalues &lt;- data.frame( trait = c(“Activity time”, “Max. SVL”, “Repro. mode”, “Insular/endemic”, “Latitude”), p_cc = -log(c(2.37e-06, 0.00270, 0.00093, 1.37e-06, 9.66e-06)), p_sim = -log(c(3.10e-07, 0.000234, 0.007901, 1.95e-07, 0.000108)), p_imp = -log(c(2e-16, 1.11e-09, 0.143, 6.27e-15, 2e-16))) ##### Note that we use the -log to improve visualization and visibility of smaller p-values. Dumbbell plot for comparing p-values. Complete-case vs. sim-imputed. ggplot(dfPvalues) + geom_segment(aes(x = trait, xend = trait, y = p_cc, yend = p_sim), color=“#9F7FCD”, size = 6.5, alpha = .4) + geom_point(aes(x = trait, y = p_cc), colour = “#ED7C97”, shape = “triangle”, size = 6.5, show.legend = TRUE) + geom_pointrange(aes(x = trait, y = p_sim, ymin = p_sim, ymax = p_sim), colour = “#F3E79A”, size = 1.5, show.legend = TRUE) + theme_minimal() + geom_hline(yintercept = -log(0.05), linetype = “dashed”, color = “darkgray”, size = 1) + labs(title = “Complete-case vs. sim-imputed dataset”, x = “”, y = “-ln(P-value)”) + theme(axis.text.x = element_text(size = 12, vjust = 0.5, hjust = 0.5), axis.text = element_text(size = 12, face = “bold”), strip.text.x = element_text(size = 10, face = “bold”), axis.title=element_text(size = 14, face=“bold”)) Complete-case vs. full imputed dataset. ggplot(dfPvalues) + geom_segment(aes(x = trait, xend = trait, y = p_cc, yend = p_imp), color=“#9F7FCD”, size = 6.5, alpha = .4) + geom_point(aes(x = trait, y = p_cc), colour = “#ED7C97”, shape = “triangle”, size = 6.5, show.legend = TRUE) + geom_pointrange(aes(x = trait, y = p_imp, ymin = p_imp, ymax = p_imp), colour = “#F3E79A”, size = 1.5, show.legend = TRUE) + theme_minimal() + geom_hline(yintercept = -log(0.05), linetype = “dashed”, color = “darkgray”, size = 1) + labs(title = “Complete-case vs. full imputed dataset”, x = “”, y = “-ln(P-value)”) + theme(axis.text.x = element_text(size = 12, vjust = 0.5, hjust = 0.5), axis.text = element_text(size = 12, face = “bold”), strip.text.x = element_text(size = 10, face = “bold”), axis.title=element_text(size = 14, face=“bold”)) Reproductive mode did have a considerable amount of missingness (~40%) It is important to consider how imputed values impact your results every step of the way, from accuracy of imputed values to the downstream impacts on inferences. But, imputation is a very powerful tool if used in the right way! "],["module-3-modeling-part-1.html", "Module 3: Modeling Part 1 Lecture Lab", " Module 3: Modeling Part 1 Lecture Regression Generalized Linear Models Lab Loading the Libraries First, let’s load any necessary libraries library(tidyverse) # General functions and plots library(faraway) # Source of the gala dataset library(car) # For some model diagnostics library(arm) # For binned plot for logistic regression library(mgcv) # For beta regression library(microbiome) # Source of dietswap data library(microbiomeDataSets) # Source of baboongut data library(phyloseq) # to calculate diversity measures library(GUniFrac) # for throat dataset library(MGLM) # for rna seq dataset First, we will cover multiple linear regression modeling in R with the lm() function. In regression modeling, it is always important to consider the data available to you. What is your response of interest? Is it a continuous, roughly normally distributed variable? Which predictors should be included in your model? Do these predictors have a linear relationship with the outcome? Multiple Linear Regression and the Dietswap Data Supposed researchers are interested in how nationality and bmi group are related to diversity of bacteria in the gut microbiome. In particular, researchers are interested to know whether nationality moderates the relationship between bmi group and Shannon diversity. Loading the Data # load the dietswap data from the microbiome package data(dietswap) # look up description of dataset ?dietswap # given that this is a phylogenetic class format, we get the data as follows: otu_table(dietswap) # counts of different OTUs sample_data(dietswap) # the metadata # let&#39;s look at the summary of our sample data summary(sample_data(dietswap)) Since we do not have a column for the Shannon diversity, we must calculate it ourselves with the estimate_richness() function from the phyloseq package. # we don&#39;t have shannon diversity so we must calculate it given the OTU table Shannon &lt;- estimate_richness(otu_table(dietswap), measures = c(&quot;Shannon&quot;)) # add shannon diversity to our sample data dietswap_data &lt;- cbind.data.frame(Shannon=Shannon$Shannon, sample_data(dietswap)) # look at histogram of Shannon diversity hist(dietswap_data$Shannon) # look at boxplot of Shannon diversity boxplot(dietswap_data$Shannon) Another potential issue we notice when reviewing the dataset is that each sample was observed at multiple time points which means this data will violate our independence assumption. To overcome this, we can filter our data to only look at the first time point. # remove second time point for each group dietswap_data_t1 &lt;- dietswap_data %&gt;% filter(timepoint.within.group==1) Since the researchers are interested to know whether there is an interaction effect, we must include the interaction and main effects in our linear model. To include an interaction between two variables, we use ’*’ in the model which will automatically include both main effects and the interaction effect of the 2 variables. # fitting the full multiple linear regression in R mod1 &lt;- lm(Shannon~nationality*bmi_group+sex, data=dietswap_data_t1) summary(mod1) ## ## Call: ## lm(formula = Shannon ~ nationality * bmi_group + sex, data = dietswap_data_t1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.32963 -0.36395 0.09898 0.33401 1.44820 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.7540 0.2296 11.993 &lt;2e-16 *** ## nationalityAFR -0.3100 0.2212 -1.401 0.1640 ## bmi_groupoverweight 0.2642 0.2275 1.162 0.2481 ## bmi_groupobese -0.2012 0.2208 -0.911 0.3643 ## sexmale -0.0509 0.1377 -0.370 0.7125 ## nationalityAFR:bmi_groupoverweight -0.5859 0.2934 -1.997 0.0484 * ## nationalityAFR:bmi_groupobese -0.0190 0.2880 -0.066 0.9475 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5512 on 105 degrees of freedom ## Multiple R-squared: 0.2439, Adjusted R-squared: 0.2007 ## F-statistic: 5.645 on 6 and 105 DF, p-value: 4.112e-05 We see from the results of this linear model that there is very moderate evidence of an interaction effect between BMI group and nationality such that those with African nationality have a greater difference in diversity between the lean and overweight groups than those of African American nationality. Overall, the variables in the model accounted for 24.4% of variability in the diversity response. Model Diagnostics Again, after fitting the model, we must check for the following assumptions: Linearity - we already checked this prior to fitting the model Independence - we already fixed this by removing the second time point Homoscedasticity - we can check this with a scatterplot of model residuals vs. fitted values Normality - we can check this with a qq plot of model residuals No multicollinearity - we will ignore this for this model for now since we have interaction terms and look at it for the model without interactions # 3. homoscedasticity res &lt;- residuals(mod1) fit &lt;- fitted(mod1) mod_res &lt;- cbind.data.frame(res,fit) # create the residuals vs fitted values plot ggplot(data=mod_res,aes(x = fit, y = res)) + geom_point(color = &quot;blue&quot;, size = 2) + # Plot the residuals geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + # Add a horizontal line at 0 labs(title = &quot;Residuals vs Fitted Values&quot;, x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) + theme_minimal() # 4. normality # QQ-plot to assess normality qqPlot(mod_res$res) ## [1] 30 97 Comparing Nested Models When fitting multiple linear regression models, it’s often useful to explore whether a simpler, more parsimonious model can adequately explain the data. For example, when working with indicator variables, we can assess whether they should be included for a change in intercept (main effect), a change in slope (interaction effect), or excluded altogether. By comparing models without the indicator, with only the main effect, and with the interaction effect, we can evaluate the contribution of the indicator variable and determine its necessity in the final model. Here, we will use the anova() function in R to compare nested models. The null hypothesis of the test run by this anova function is H0: reduced (simpler model) is adequate and the alternative hypothesis is H1: Full (more complex) model is needed to explain the data. Therefore, if the p value is small, then we have evidence that we should include the extra variable(s). # fit model with only main effects for nationality mod2 &lt;- lm(Shannon~nationality+bmi_group+sex, data=dietswap_data_t1) summary(mod2) ## ## Call: ## lm(formula = Shannon ~ nationality + bmi_group + sex, data = dietswap_data_t1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.35941 -0.34599 0.06785 0.32141 1.51472 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.97433 0.19853 14.982 &lt; 2e-16 *** ## nationalityAFR -0.52438 0.11508 -4.557 1.39e-05 *** ## bmi_groupoverweight -0.04098 0.16653 -0.246 0.8061 ## bmi_groupobese -0.31923 0.16734 -1.908 0.0591 . ## sexmale -0.12343 0.13122 -0.941 0.3490 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5606 on 107 degrees of freedom ## Multiple R-squared: 0.2029, Adjusted R-squared: 0.1731 ## F-statistic: 6.81 on 4 and 107 DF, p-value: 6.374e-05 # fit model without nationality mod3 &lt;- lm(Shannon~bmi_group+sex, data=dietswap_data_t1) summary(mod3) ## ## Call: ## lm(formula = Shannon ~ bmi_group + sex, data = dietswap_data_t1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4473 -0.4295 0.1259 0.4826 1.3520 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.46753 0.17886 13.796 &lt;2e-16 *** ## bmi_groupoverweight 0.23528 0.16870 1.395 0.166 ## bmi_groupobese -0.07057 0.17206 -0.410 0.682 ## sexmale 0.02172 0.13846 0.157 0.876 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6098 on 108 degrees of freedom ## Multiple R-squared: 0.04824, Adjusted R-squared: 0.02181 ## F-statistic: 1.825 on 3 and 108 DF, p-value: 0.147 # compare nested models anova(mod3,mod2) ## Analysis of Variance Table ## ## Model 1: Shannon ~ bmi_group + sex ## Model 2: Shannon ~ nationality + bmi_group + sex ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 108 40.155 ## 2 107 33.629 1 6.5256 20.763 1.385e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod3,mod1) ## Analysis of Variance Table ## ## Model 1: Shannon ~ bmi_group + sex ## Model 2: Shannon ~ nationality * bmi_group + sex ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 108 40.155 ## 2 105 31.901 3 8.2543 9.0563 2.189e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod2,mod1) ## Analysis of Variance Table ## ## Model 1: Shannon ~ nationality + bmi_group + sex ## Model 2: Shannon ~ nationality * bmi_group + sex ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 107 33.629 ## 2 105 31.901 2 1.7287 2.8449 0.06263 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From comparing the nested models, we see that nationality should be included in the model but that we do not require the interaction effect. We can go ahead and check the model diagnostics for mod2. # 3. homoscedasticity res &lt;- residuals(mod2) fit &lt;- fitted(mod2) mod_res &lt;- cbind.data.frame(res,fit) # create the residuals vs fitted values plot ggplot(data=mod_res,aes(x = fit, y = res)) + geom_point(color = &quot;blue&quot;, size = 2) + # Plot the residuals geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + # Add a horizontal line at 0 labs(title = &quot;Residuals vs Fitted Values&quot;, x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) + theme_minimal() # 4.normality # QQ-plot to assess normality qqPlot(mod_res$res) ## [1] 30 97 # 5. no multicollinearity vif(mod2) ## GVIF Df GVIF^(1/(2*Df)) ## nationality 1.166324 1 1.079965 ## bmi_group 1.667090 2 1.136291 ## sex 1.521859 1 1.233637 Visualize our Model Now let’s plot the interaction between bmi_group and nationality on Shannon diversity to help us interpret our findings. ggplot(dietswap_data_t1, aes(x = bmi_group, y = Shannon, color = nationality, group = nationality)) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 3) + # Plot the means stat_summary(fun = mean, geom = &quot;line&quot;) + # Connect the means with lines labs(title = &quot;Interaction between Nationality, BMI Group, and Shannon Diversity&quot;, x = &quot;BMI Group&quot;, y = &quot;Shannon Diversity Index&quot;, color = &quot;Nationality&quot;) + theme_minimal() Logistic Regression Suppose researchers are interested to learn more about the association between shannon diversity of the throat bacteria and smoking. We can use logistic regression to help answer this question through looking at whether we can predict smoking status with shannon diversity of bacteria in the throat, age, and sex. Loading the Data # first, need to load in meta data and otu table data(throat.meta) data(throat.otu.tab) # look up descriptions of the data ?throat.meta ?throat.otu.tab # let&#39;s turn this data into a phyloseq object throat.physeq &lt;- phyloseq( otu_table(throat.otu.tab, taxa_are_rows = FALSE), sample_data(throat.meta) ) # check our phyloseq object otu_table(throat.physeq) sample_data(throat.physeq) # now we can estimate shannon diversity Shannon &lt;- estimate_richness(otu_table(throat.physeq), measures=&quot;Shannon&quot;) throat_data &lt;- cbind.data.frame(Shannon=Shannon$Shannon,sample_data(throat.physeq)) Fitting the Logistic Regression Model In R, we can use the glm() function to fit a logistic regression model by setting family = “binomial” as an argument. # now let&#39;s look at the effect of sex, age, and diversity on smoking log_mod &lt;- glm(SmokingStatus~Age+Sex+Shannon, data=throat_data, family=binomial) summary(log_mod) ## ## Call: ## glm(formula = SmokingStatus ~ Age + Sex + Shannon, family = binomial, ## data = throat_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.97623 2.54902 0.775 0.4382 ## Age 0.04757 0.02798 1.700 0.0891 . ## SexMale 0.80698 0.61972 1.302 0.1929 ## Shannon -1.29961 0.80217 -1.620 0.1052 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 82.911 on 59 degrees of freedom ## Residual deviance: 75.147 on 56 degrees of freedom ## AIC: 83.147 ## ## Number of Fisher Scoring iterations: 4 # get odds ratios exp(coef(log_mod)) ## (Intercept) Age SexMale Shannon ## 7.2154548 1.0487182 2.2411336 0.2726372 # 95% confidence intervals for odds ratios exp(confint(log_mod)) ## 2.5 % 97.5 % ## (Intercept) 0.05231538 1437.791848 ## Age 0.99472215 1.111987 ## SexMale 0.68252995 7.976087 ## Shannon 0.05015106 1.235533 It does not look like Shannon diversity has a significant association with smoking status. That is, the p-value for the Shannon diversity covariate is 0.105. We can interpret the coefficient on the logit scale as, increasing Shannon diversity by 1 unit is estimated to decrease log(p/1-p) of smoking by 1.2996. Alternatively, we can interpret the coefficient on the odds scale by exponentiating the coefficient. On the odds scale, we can say that increasing the Shannon diversity changes the odds of smoking by a factor of 0.273 (95% CI: 0.050, 1.236). Model Diagnostic Again, after fitting the model, we must check for the following assumptions: Linearity - we can check the relationship between covariates and logit outcome with component-residual plots Independence - we will assume the data observations are independent Proper fitting distribution - we can check this with a scatterplot of model DEVIANCE residuals vs. linear predictor No multicollinearity - we can check this with variance inflation factors # checking assumptions # 1. linearity assumption with component+residual plots crPlot(log_mod, variable=&quot;Shannon&quot;) crPlot(log_mod, variable=&quot;Age&quot;) # 3. proper fitting distribution (look for no pattern/constant variance) plot(residuals(log_mod)~predict(log_mod)) # notice it looks weird because we only have 2 response options binnedplot(predict(log_mod), residuals(log_mod)) # 4. no multicollinearity vif(log_mod) # a bit concerning - could be too high of a correlation ## Age Sex Shannon ## 1.074958 1.120760 1.095087 Visualization # first we need smoking status to be numeric throat_data$SmokingStatusNumeric &lt;- ifelse(throat_data$SmokingStatus==&quot;Smoker&quot;,1,0) # create a ggplot object with the data points p &lt;- ggplot(throat_data, aes(x = Shannon, y = SmokingStatusNumeric)) + geom_point(alpha = 0.5) + # Plot the actual data points stat_smooth(method = &quot;glm&quot;, method.args = list(family = binomial), se = FALSE, color = &quot;blue&quot;) + labs(title = &quot;Logistic Regression Curve&quot;, x = &quot;Shannon Diversity&quot;, y = &quot;Probability of Smoking&quot;) # display the plot p Binomial Regression Suppose researchers are interested in the number of surviving trout eggs in boxes across different locations at different weeks after placement. In particular, the outcome of interest is the number of trout eggs alive in the box/the total number of trout eggs in the box. Loading the Data We will use the troutegg data from faraway for this analysis. # first, let&#39;s load in the data data(troutegg) # from faraway # view description of the data ?troutegg # view data head(troutegg) ## survive total location period ## 1 89 94 1 4 ## 2 106 108 2 4 ## 3 119 123 3 4 ## 4 104 104 4 4 ## 5 49 93 5 4 ## 6 94 98 1 7 str(troutegg) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ survive : int 89 106 119 104 49 94 91 100 80 11 ... ## $ total : int 94 108 123 104 93 98 106 130 97 113 ... ## $ location: Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 1 2 3 4 5 ... ## $ period : Factor w/ 4 levels &quot;4&quot;,&quot;7&quot;,&quot;8&quot;,&quot;11&quot;: 1 1 1 1 1 2 2 2 2 2 ... Fitting the binomial regression We will fit the binomial regression using the glm function and setting family=“binomial”. However, we have to tell R how many successes we have in our outcome and how many failures. # fit binomial binom_mod &lt;- glm(cbind(survive,total-survive)~location+period, data=troutegg, family=&quot;binomial&quot;) summary(binom_mod) ## ## Call: ## glm(formula = cbind(survive, total - survive) ~ location + period, ## family = &quot;binomial&quot;, data = troutegg) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.6358 0.2813 16.479 &lt; 2e-16 *** ## location2 -0.4168 0.2461 -1.694 0.0903 . ## location3 -1.2421 0.2194 -5.660 1.51e-08 *** ## location4 -0.9509 0.2288 -4.157 3.23e-05 *** ## location5 -4.6138 0.2502 -18.439 &lt; 2e-16 *** ## period7 -2.1702 0.2384 -9.103 &lt; 2e-16 *** ## period8 -2.3256 0.2429 -9.573 &lt; 2e-16 *** ## period11 -2.4500 0.2341 -10.466 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1021.469 on 19 degrees of freedom ## Residual deviance: 64.495 on 12 degrees of freedom ## AIC: 157.03 ## ## Number of Fisher Scoring iterations: 5 Model diagnostics Again, after fitting the model, we must check for the following assumptions: Linearity - we do not have any continuous or numeric variables Independence - we will assume the data observations are independent Proper fitting distribution - we can check this with a scatterplot of model DEVIANCE residuals vs. linear predictor No multicollinearity - we can check this with variance inflation factors # checking assumptions # 3. proper fitting distribution (look for no pattern/constant variance) plot(residuals(binom_mod)~predict(binom_mod)) # 4. No multicollinearity vif(binom_mod) ## GVIF Df GVIF^(1/(2*Df)) ## location 1.33796 4 1.037064 ## period 1.33796 3 1.049721 Addressing Overdispersion in Binomial The binomial model assumes that the variance is related to the our outcome, p, as p(1-p). If the variance is greater than what is assumed by the model, it is called ‘overdispersion’. We can estimate overdispersion as deviance/df residuals. If this ratio is greater than 1 we have overdispersion, if it is (roughly) 1 we have no overdispersion, and if it is less than 1, we have underdispersion. If we have overdispersion of the binomial, we can try to fit a quasibinomial model instead which will estimate the dispersion parameter instead of assuming it to be 1. # estimating the dispersion binom_mod$deviance/binom_mod$df.residual ## [1] 5.374593 # if it is large, we can use &#39;quasibinomial&#39; as the family instead to account for this binom_mod2 &lt;- glm(cbind(survive,total-survive)~location+period, data=troutegg, family=&quot;quasibinomial&quot;) summary(binom_mod2) ## ## Call: ## glm(formula = cbind(survive, total - survive) ~ location + period, ## family = &quot;quasibinomial&quot;, data = troutegg) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.6358 0.6495 7.138 1.18e-05 *** ## location2 -0.4168 0.5682 -0.734 0.477315 ## location3 -1.2421 0.5066 -2.452 0.030501 * ## location4 -0.9509 0.5281 -1.800 0.096970 . ## location5 -4.6138 0.5777 -7.987 3.82e-06 *** ## period7 -2.1702 0.5504 -3.943 0.001953 ** ## period8 -2.3256 0.5609 -4.146 0.001356 ** ## period11 -2.4500 0.5405 -4.533 0.000686 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasibinomial family taken to be 5.330358) ## ## Null deviance: 1021.469 on 19 degrees of freedom ## Residual deviance: 64.495 on 12 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 Poisson Regression Loading the Data We will use the faramea.csv data. # read in the dataset faramea &lt;- read.csv(&#39;module3_faramea.csv&#39;) # look at first 6 rows head(faramea) ## X UTM.EW UTM.NS Precipitation Elevation Age Age.cat Geology ## 1 B0 625754.0 1011569 2530.0 120 3 c3 Tb ## 2 B49 626654.0 1011969 2530.0 120 3 c3 Tb ## 3 p1 614856.9 1031786 2993.2 20 2 c2 Tc ## 4 p2 613985.4 1030725 3072.0 100 3 c3 Tc ## 5 p3 614674.3 1023802 3007.4 180 1 c1 Tc ## 6 p4 615018.6 1023548 2999.8 180 1 c1 Tc ## Faramea.occidentalis ## 1 14 ## 2 7 ## 3 0 ## 4 0 ## 5 2 ## 6 1 # look at structure str(faramea) ## &#39;data.frame&#39;: 43 obs. of 9 variables: ## $ X : chr &quot;B0&quot; &quot;B49&quot; &quot;p1&quot; &quot;p2&quot; ... ## $ UTM.EW : num 625754 626654 614857 613985 614674 ... ## $ UTM.NS : num 1011569 1011969 1031786 1030725 1023802 ... ## $ Precipitation : num 2530 2530 2993 3072 3007 ... ## $ Elevation : int 120 120 20 100 180 180 40 30 60 50 ... ## $ Age : int 3 3 2 3 1 1 2 2 1 3 ... ## $ Age.cat : chr &quot;c3&quot; &quot;c3&quot; &quot;c2&quot; &quot;c3&quot; ... ## $ Geology : chr &quot;Tb&quot; &quot;Tb&quot; &quot;Tc&quot; &quot;Tc&quot; ... ## $ Faramea.occidentalis: int 14 7 0 0 2 1 0 0 2 0 ... # look at summary of variables summary(faramea) ## X UTM.EW UTM.NS Precipitation ## Length:43 Min. :600714 Min. : 962862 Min. :1888 ## Class :character 1st Qu.:625578 1st Qu.:1009776 1st Qu.:2406 ## Mode :character Median :637732 Median :1012428 Median :2516 ## Mean :635927 Mean :1013984 Mean :2643 ## 3rd Qu.:645611 3rd Qu.:1023675 3rd Qu.:2996 ## Max. :688165 Max. :1045987 Max. :4002 ## Elevation Age Age.cat Geology ## Min. : 10.0 Min. :1.000 Length:43 Length:43 ## 1st Qu.: 59.0 1st Qu.:1.000 Class :character Class :character ## Median :120.0 Median :2.000 Mode :character Mode :character ## Mean :192.3 Mean :2.093 ## 3rd Qu.:180.0 3rd Qu.:3.000 ## Max. :830.0 Max. :3.000 ## Faramea.occidentalis ## Min. : 0.000 ## 1st Qu.: 0.000 ## Median : 0.000 ## Mean : 3.884 ## 3rd Qu.: 4.500 ## Max. :42.000 # convert age to numeric (helps with crPlots) faramea$Age &lt;- as.numeric(faramea$Age) Fitting the Poisson Regression Again, we will be using the glm() function here to fit our Poisson regression model and this time, we will set family=“poisson”. # fitting the Poisson model glm.poisson = glm(Faramea.occidentalis ~ Elevation+Age+Precipitation, data = faramea, family = poisson) summary(glm.poisson) ## ## Call: ## glm(formula = Faramea.occidentalis ~ Elevation + Age + Precipitation, ## family = poisson, data = faramea) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.8957542 0.6178206 7.924 2.3e-15 *** ## Elevation -0.0018282 0.0006837 -2.674 0.007496 ** ## Age -0.3918512 0.1135936 -3.450 0.000561 *** ## Precipitation -0.0010039 0.0002904 -3.457 0.000546 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 414.81 on 42 degrees of freedom ## Residual deviance: 341.86 on 39 degrees of freedom ## AIC: 419.74 ## ## Number of Fisher Scoring iterations: 8 Model Diagnostic Again, after fitting the model, we must check for the following assumptions: Linearity - use component-residual plots to assess Independence - we will assume the data observations are independent Proper fitting distribution- we can check this with a scatterplot of model DEVIANCE residuals vs. linear predictor No multicollinearity - we can check this with variance inflation factors # check assumptions # 1. linearity crPlot(glm.poisson, variable=&quot;Elevation&quot;) crPlot(glm.poisson, variable=&quot;Age&quot;) crPlot(glm.poisson, variable=&quot;Precipitation&quot;) # 3. proper fitting distribution plot(resid(glm.poisson)~predict(glm.poisson)) # fan pattern - likely due to the overdispersion # 4. no multicollinearity vif(glm.poisson) ## Elevation Age Precipitation ## 1.193796 1.308440 1.397856 Addressing Overdispersion in Poisson The Poisson model assumes that the variance is equal to the mean (i.e., E(Y) = VAR(Y)). If the variance is greater than what is assumed by the model, it is called ‘overdispersion’. We can estimate overdispersion as deviance/df residuals. If this ratio is greater than 1 we have overdispersion, if it is (roughly) 1 we have equidispersion, and if it is less than 1, we have underdispersion. If we have overdispersion of the Poisson, we can try to fit a quasipoisson model instead which will estimate the dispersion parameter instead of assuming it to be 1. # estimate dispersion glm.poisson$deviance/glm.poisson$df.residual ## [1] 8.765551 # fit quasi-poisson to address overdispersion glm.qpoisson = glm(Faramea.occidentalis ~ Elevation+Age+Precipitation, data = faramea, family = quasipoisson) summary(glm.qpoisson) ## ## Call: ## glm(formula = Faramea.occidentalis ~ Elevation + Age + Precipitation, ## family = quasipoisson, data = faramea) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.895754 2.159927 2.267 0.029 * ## Elevation -0.001828 0.002390 -0.765 0.449 ## Age -0.391851 0.397128 -0.987 0.330 ## Precipitation -0.001004 0.001015 -0.989 0.329 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 12.22231) ## ## Null deviance: 414.81 on 42 degrees of freedom ## Residual deviance: 341.86 on 39 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 8 Modelin Rates with Poisson When modeling count data, there are scenarios where the counts observed may vary due to differences in the length of observation periods, the area covered, or the number of trials conducted. In such cases, it’s important to account for the differing observation levels using an offset in Poisson models. **While the Poisson model can seem similar to the binomial distribution, particularly when dealing with rates of “successes”, there is a key distinction to keep in mind. The Poisson model is generally preferred when the count of successes is unbounded—that is, there is no theoretical limit to the number of successes we might observe based on the denominator. However, if the probability of success is extremely low, so much so that the numerator (the count of successes) would never be expected to approach the denominator, the Poisson model remains appropriate. In contrast, the binomial distribution is typically used when there is a fixed upper limit on the number of successes (and the denominator is this upper limit). We will briefly review an example here where we have a rate instead of a count outcome. In this example, we will look at the dicentric dataset from faraway. The outcome of interest is the number of chromosomal abnormalities and the predictor of interest is the dose amount and dose rate. However, we also want to account for the number of cells observed as we would expect to see more abnormalities if we observed more cells. We will treat number of cells as our offset variable. # load in the data data(dicentric) # view description of the data ?dicentric # now let&#39;s look at the model without accounting for # of cells dicentric$dose_fac &lt;- as.factor(dicentric$doseamt) mod_pois &lt;- glm(ca ~ log(doserate)+dose_fac, data=dicentric, family=poisson) summary(mod_pois) # clearly overdispersed - we are not properly accounting for the variability ## ## Call: ## glm(formula = ca ~ log(doserate) + dose_fac, family = poisson, ## data = dicentric) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.53018 0.03439 131.740 &lt;2e-16 *** ## log(doserate) 0.20443 0.01662 12.301 &lt;2e-16 *** ## dose_fac2.5 -0.02830 0.04857 -0.583 0.56 ## dose_fac5 0.59294 0.04249 13.955 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 916.13 on 26 degrees of freedom ## Residual deviance: 460.69 on 23 degrees of freedom ## AIC: 644.1 ## ## Number of Fisher Scoring iterations: 4 # include log(cells) as covariate mod_pois &lt;- glm(ca ~ log(cells)+log(doserate)+dose_fac, data=dicentric, family=poisson) summary(mod_pois) # close to 1, can fix at 1 and treat as offset ## ## Call: ## glm(formula = ca ~ log(cells) + log(doserate) + dose_fac, family = poisson, ## data = dicentric) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.08452 0.38201 -8.075 6.77e-16 *** ## log(cells) 1.04271 0.05158 20.217 &lt; 2e-16 *** ## log(doserate) 0.21509 0.01674 12.850 &lt; 2e-16 *** ## dose_fac2.5 1.72742 0.10226 16.892 &lt; 2e-16 *** ## dose_fac5 2.89486 0.12107 23.910 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 916.127 on 26 degrees of freedom ## Residual deviance: 42.089 on 22 degrees of freedom ## AIC: 227.5 ## ## Number of Fisher Scoring iterations: 4 # include log(cells) as offset mod_pois &lt;- glm(ca ~ offset(log(cells))+log(doserate)+dose_fac, data=dicentric, family=poisson) summary(mod_pois) ## ## Call: ## glm(formula = ca ~ offset(log(cells)) + log(doserate) + dose_fac, ## family = poisson, data = dicentric) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.76958 0.03430 -80.74 &lt;2e-16 *** ## log(doserate) 0.21447 0.01672 12.83 &lt;2e-16 *** ## dose_fac2.5 1.65299 0.04857 34.03 &lt;2e-16 *** ## dose_fac5 2.80095 0.04251 65.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 4753.004 on 26 degrees of freedom ## Residual deviance: 42.776 on 23 degrees of freedom ## AIC: 226.18 ## ## Number of Fisher Scoring iterations: 4 We see that including the offset is important as it can explain a lot of the overdispersion in the model outcome. It is important to note that the GLMs reviewed here are not exhaustive and there are many scenarios you will encounter when these will not work for your data. However, this module is to provide you with the tools necessary to think critically about your data and model assumptions. If your outcome data is highly overdispersed, it might be better to use more complex models to account for this such as the negative binomial (see below) or the zero-inflated Poisson if you have a large number of zero’s in your outcome. Negative Binomial We will very briefly review negative binomial as it is important for overdispersed count data that often occurs in gene expression data and taxonomic compositions. Load the Data The data we are looking at is from a simulated experiment based on RNA-Seq data for differential expression. The first 6 columns are the gene expression of different genes and we have covariates for age, treatment, gender, sex. We additionally want to account for total number of reads as an offset. # load in dataset rnaseq data(rnaseq) # description of dataset ?rnaseq # view first 6 rows of dataset head(rnaseq) ## X1 X2 X3 X4 X5 X6 totalReads treatment gender age ## 1 295 65 19 114 54 20 28317494 0 0 57 ## 2 213 126 12 96 50 4 20015549 0 0 67 ## 3 322 147 23 245 42 19 35318251 0 1 37 ## 4 184 35 9 134 74 8 20421437 0 0 59 ## 5 376 104 4 74 39 7 21940693 0 1 57 ## 6 490 97 18 282 159 24 48645477 0 1 50 Fitting the Negative Binomial Regression First, we will fit a Poisson regression to show how overdispersed gene expression data can be and how a negative binomial is more accommodating of this kind of data. We can fit this model with glm.nb() which will estimate the extra parameter, theta for us. # treat X1 as outcome and fit poisson mod_pois &lt;- glm(X1~offset(log(totalReads))+treatment+gender+age, data=rnaseq, family=poisson) summary(mod_pois) ## ## Call: ## glm(formula = X1 ~ offset(log(totalReads)) + treatment + gender + ## age, family = poisson, data = rnaseq) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.171e+01 2.557e-02 -458.043 &lt; 2e-16 *** ## treatment -7.581e-01 1.020e-02 -74.337 &lt; 2e-16 *** ## gender -8.130e-02 9.645e-03 -8.429 &lt; 2e-16 *** ## age 2.407e-03 4.912e-04 4.901 9.55e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 10798.2 on 199 degrees of freedom ## Residual deviance: 4890.3 on 196 degrees of freedom ## AIC: 6313.2 ## ## Number of Fisher Scoring iterations: 4 # dispersion? sum_pois &lt;- summary(mod_pois) sum_pois$deviance/sum_pois$df.residual ## [1] 24.95035 # fit negative binomial mod_nb &lt;- glm.nb(X1~offset(log(totalReads))+treatment+gender+age, data=rnaseq) summary(mod_nb) ## ## Call: ## glm.nb(formula = X1 ~ offset(log(totalReads)) + treatment + gender + ## age, data = rnaseq, init.theta = 9.319170881, link = log) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -11.777840 0.126981 -92.753 &lt;2e-16 *** ## treatment -0.767008 0.047530 -16.137 &lt;2e-16 *** ## gender -0.043007 0.047568 -0.904 0.366 ## age 0.003463 0.002420 1.431 0.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(9.3192) family taken to be 1) ## ## Null deviance: 460.37 on 199 degrees of freedom ## Residual deviance: 203.51 on 196 degrees of freedom ## AIC: 2243.7 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 9.319 ## Std. Err.: 0.965 ## ## 2 x log-likelihood: -2233.659 Bonus: Beta Regression Beta regression is useful for modeling outcomes bounded between 0 and 1, such as proportions. However, it should be used when the denominator of the proportion is unknown. If the denominator is known, binomial regression is more appropriate. Additionally, beta regression cannot handle outcomes exactly equal to 0 or 1; all values must fall strictly between these bounds. For this example, we will look at whether there exists an association between DNA methylation at a specific CpG site and gender, age, and smoking status. Load the Data The dataset we will use is titled ‘smoker_Epigenetic_df.csv’ in our folder. # first, let&#39;s load in the data smok_dat &lt;- read.csv(&#39;module3_Smoker_Epigenetic_df.csv&#39;) # smoker_epigenetic_Df # view data head(smok_dat) ## GSM Smoking.Status Gender Age cg00050873 cg00212031 cg00213748 ## 1 GSM1051525 current f 67 0.6075634 0.4228427 0.3724549 ## 2 GSM1051526 current f 49 0.3450542 0.5686615 0.5005995 ## 3 GSM1051527 current f 53 0.3213497 0.3609091 0.3527315 ## 4 GSM1051528 current f 62 0.2772675 0.3044371 0.4752352 ## 5 GSM1051529 never f 33 0.4135991 0.1312511 0.3675446 ## 6 GSM1051530 current f 59 0.6228599 0.5016849 0.2632270 ## cg00214611 cg00455876 cg01707559 cg02004872 cg02011394 cg02050847 cg02233190 ## 1 0.6215619 0.2907773 0.2671431 0.1791439 0.4802517 0.3276078 0.2411204 ## 2 0.4986067 0.3745909 0.1902743 0.1559775 0.4180809 0.3464627 0.1754907 ## 3 0.3738240 0.2306740 0.3147052 0.1057448 0.6151030 0.2375392 0.2464092 ## 4 0.4862581 0.2951815 0.2957931 0.1112862 0.3010196 0.3045353 0.1770279 ## 5 0.7611667 0.2357703 0.2505265 0.1691084 0.3929746 0.3062257 0.3017014 ## 6 0.4157459 0.4751891 0.2539041 0.2607587 0.5097921 0.4052457 0.3852716 ## cg02494853 cg02839557 cg02842889 cg03052502 cg03155755 cg03244189 cg03443143 ## 1 0.06706958 0.246993368 0.4692396 0.4002466 0.4150313 0.2214331 0.4758258 ## 2 0.04693889 0.236742313 0.3074666 0.3770313 0.3973715 0.2171221 0.5444690 ## 3 0.03823712 0.244611725 0.3577526 0.3050442 0.5212775 0.1850495 0.5370600 ## 4 0.02671625 0.001641439 0.4457390 0.2714746 0.4344920 0.1654187 0.5079167 ## 5 0.03701636 0.334319727 0.3950396 0.3265530 0.4300966 0.1811352 0.4054791 ## 6 0.02583463 0.309210202 0.3218573 0.5333670 0.5715522 0.2109749 0.3778239 ## cg03683899 cg03695421 cg03706273 ## 1 0.2077242 0.2091974 0.12998255 ## 2 0.1844462 0.1937732 0.09853265 ## 3 0.3931231 0.2680030 0.04024808 ## 4 0.2812089 0.2178572 0.10151626 ## 5 0.3107944 0.2800708 0.07785712 ## 6 0.4693609 0.3433317 0.04577912 # look at structure str(smok_dat) ## &#39;data.frame&#39;: 683 obs. of 24 variables: ## $ GSM : chr &quot;GSM1051525&quot; &quot;GSM1051526&quot; &quot;GSM1051527&quot; &quot;GSM1051528&quot; ... ## $ Smoking.Status: chr &quot;current&quot; &quot;current&quot; &quot;current&quot; &quot;current&quot; ... ## $ Gender : chr &quot; f&quot; &quot; f&quot; &quot; f&quot; &quot; f&quot; ... ## $ Age : int 67 49 53 62 33 59 66 51 55 37 ... ## $ cg00050873 : num 0.608 0.345 0.321 0.277 0.414 ... ## $ cg00212031 : num 0.423 0.569 0.361 0.304 0.131 ... ## $ cg00213748 : num 0.372 0.501 0.353 0.475 0.368 ... ## $ cg00214611 : num 0.622 0.499 0.374 0.486 0.761 ... ## $ cg00455876 : num 0.291 0.375 0.231 0.295 0.236 ... ## $ cg01707559 : num 0.267 0.19 0.315 0.296 0.251 ... ## $ cg02004872 : num 0.179 0.156 0.106 0.111 0.169 ... ## $ cg02011394 : num 0.48 0.418 0.615 0.301 0.393 ... ## $ cg02050847 : num 0.328 0.346 0.238 0.305 0.306 ... ## $ cg02233190 : num 0.241 0.175 0.246 0.177 0.302 ... ## $ cg02494853 : num 0.0671 0.0469 0.0382 0.0267 0.037 ... ## $ cg02839557 : num 0.24699 0.23674 0.24461 0.00164 0.33432 ... ## $ cg02842889 : num 0.469 0.307 0.358 0.446 0.395 ... ## $ cg03052502 : num 0.4 0.377 0.305 0.271 0.327 ... ## $ cg03155755 : num 0.415 0.397 0.521 0.434 0.43 ... ## $ cg03244189 : num 0.221 0.217 0.185 0.165 0.181 ... ## $ cg03443143 : num 0.476 0.544 0.537 0.508 0.405 ... ## $ cg03683899 : num 0.208 0.184 0.393 0.281 0.311 ... ## $ cg03695421 : num 0.209 0.194 0.268 0.218 0.28 ... ## $ cg03706273 : num 0.13 0.0985 0.0402 0.1015 0.0779 ... # look at summary summary(smok_dat) # some missingness in our outcome, we will not impute ## GSM Smoking.Status Gender Age ## Length:683 Length:683 Length:683 Min. :18.00 ## Class :character Class :character Class :character 1st Qu.:47.00 ## Mode :character Mode :character Mode :character Median :56.00 ## Mean :53.82 ## 3rd Qu.:62.00 ## Max. :80.00 ## ## cg00050873 cg00212031 cg00213748 cg00214611 ## Min. :0.1186 Min. :0.00695 Min. :0.0000 Min. :0.01247 ## 1st Qu.:0.4131 1st Qu.:0.06317 1st Qu.:0.3635 1st Qu.:0.06946 ## Median :0.5052 Median :0.36554 Median :0.4713 Median :0.41575 ## Mean :0.5600 Mean :0.30960 Mean :0.5191 Mean :0.34106 ## 3rd Qu.:0.8144 3rd Qu.:0.45981 3rd Qu.:0.7278 3rd Qu.:0.49745 ## Max. :0.8989 Max. :0.70999 Max. :0.9236 Max. :0.80606 ## NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 ## cg00455876 cg01707559 cg02004872 cg02011394 ## Min. :0.05917 Min. :0.04333 Min. :0.00262 Min. :0.0000 ## 1st Qu.:0.29300 1st Qu.:0.11080 1st Qu.:0.04284 1st Qu.:0.4261 ## Median :0.37968 Median :0.23873 Median :0.14933 Median :0.5157 ## Mean :0.44718 Mean :0.21435 Mean :0.15542 Mean :0.6058 ## 3rd Qu.:0.66283 3rd Qu.:0.28061 3rd Qu.:0.24263 3rd Qu.:0.9412 ## Max. :0.85443 Max. :0.46999 Max. :0.47384 Max. :0.9792 ## NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 ## cg02050847 cg02233190 cg02494853 cg02839557 ## Min. :0.05234 Min. :0.00863 Min. :0.01162 Min. :0.00000 ## 1st Qu.:0.33963 1st Qu.:0.08838 1st Qu.:0.02865 1st Qu.:0.06384 ## Median :0.42754 Median :0.25982 Median :0.03695 Median :0.35042 ## Mean :0.54369 Mean :0.23250 Mean :0.04077 Mean :0.30088 ## 3rd Qu.:0.95558 3rd Qu.:0.33702 3rd Qu.:0.04677 3rd Qu.:0.45786 ## Max. :0.98320 Max. :0.51173 Max. :0.28947 Max. :0.82739 ## NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 ## cg02842889 cg03052502 cg03155755 cg03244189 ## Min. :0.01346 Min. :0.0000 Min. :0.2020 Min. :0.02972 ## 1st Qu.:0.05483 1st Qu.:0.4025 1st Qu.:0.4245 1st Qu.:0.11976 ## Median :0.39757 Median :0.4940 Median :0.4962 Median :0.20397 ## Mean :0.32362 Mean :0.5907 Mean :0.5895 Mean :0.19552 ## 3rd Qu.:0.47385 3rd Qu.:0.9631 3rd Qu.:0.8988 3rd Qu.:0.24921 ## Max. :0.85625 Max. :0.9902 Max. :0.9696 Max. :0.54074 ## NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 ## cg03443143 cg03683899 cg03695421 cg03706273 ## Min. :0.06496 Min. :0.00788 Min. :0.0949 Min. :0.01120 ## 1st Qu.:0.40963 1st Qu.:0.06159 1st Qu.:0.2566 1st Qu.:0.03413 ## Median :0.48314 Median :0.34422 Median :0.3208 Median :0.04961 ## Mean :0.56841 Mean :0.28442 Mean :0.3978 Mean :0.05769 ## 3rd Qu.:0.85436 3rd Qu.:0.41866 3rd Qu.:0.5965 3rd Qu.:0.06916 ## Max. :0.93589 Max. :0.65925 Max. :0.8433 Max. :0.34380 ## NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 NA&#39;s :62 # look closer at gender and smoking status table(smok_dat$Gender) ## ## f F m M ## 440 13 181 49 table(smok_dat$Smoking.Status) ## ## current never ## 490 193 # clean gender smok_dat_c &lt;- smok_dat %&gt;% mutate(Gender=case_when(Gender==&quot; f&quot;~&quot;F&quot;, Gender==&quot; F&quot;~&quot;F&quot;, Gender==&quot; m&quot;~&quot;M&quot;, Gender==&quot; M&quot;~&quot;M&quot;)) table(smok_dat_c$Gender) ## ## F M ## 453 230 Fitting Beta Regression To fit beta regression, we need to use the gam() function from the mgcv package in R. We set family=betar() in this function. # look at effect of smoking status, gender, and age on methylation modb &lt;- gam(cg00455876~Smoking.Status+Gender+Age, data=smok_dat_c, family=betar()) summary(modb) ## ## Family: Beta regression(31.518) ## Link function: logit ## ## Formula: ## cg00455876 ~ Smoking.Status + Gender + Age ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7581417 0.0731284 -10.367 &lt;2e-16 *** ## Smoking.Statusnever 0.0655020 0.0328325 1.995 0.046 * ## GenderM 1.6778920 0.0342145 49.040 &lt;2e-16 *** ## Age 0.0005844 0.0013288 0.440 0.660 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## R-sq.(adj) = 0.832 Deviance explained = 82.5% ## -REML = -670.56 Scale est. = 1 n = 621 # check assumptions with gam.check gam.check(modb) ## ## Method: REML Optimizer: outer newton ## full convergence after 6 iterations. ## Gradient range [4.752114e-09,4.752114e-09] ## (score -670.5585 &amp; scale 1). ## Hessian positive definite, eigenvalue range [317.2695,317.2695]. ## Model rank = 4 / 4 The mgcv package has a useful function, gam.check, that allows us to check all of our usual model assumptions. In the QQ plot, we want the dots to follow roughly along the diagonal line if we assumed the correct distribution (beta) for our data. In the residual vs fitted plot, we are looking for a random scattering of points as usual. Independent Work Exercise 1 - Baboon Diet and Social Group Size The baboongut dataset from the microbiomeDataSets contains data on baboons over a 14 year span to look at heritability of the gut microbiome within social groups. Let’s load in this dataset and take a closer look. First, we will filter the data to only keep the first observation for each baboon to make the data cross-sectional as opposed to longitudinal. # load in dataset ts &lt;- baboongut() sample_data &lt;- colData(ts) # colData gives us the metadata we need # look at description ?baboongut # we will only look at first sample per ID (so it is not longitudinal) baboon_first &lt;- sample_data[isUnique(sample_data$baboon_id), ] dim(baboon_first) ## [1] 62 35 # we will only focus on the diet variables baboon_first_c &lt;- baboon_first %&gt;% as.data.frame(.) %&gt;% dplyr::select(group_size, contains(&quot;diet_PC&quot;)) dim(baboon_first_c) ## [1] 62 14 Now, do the following: Create a new dataset called ‘baboon_first_c’ that only has ‘group_size’ and the first 7 “diet_PC’ columns. These are the variables you would get had you done a principal component analysis on diet variables to reduce the dimensionality of your data. We will be using these principal components as our new predictors. # we can use select from tidyverse to do this baboon_first_c &lt;- baboon_first %&gt;% as.data.frame(.) %&gt;% dplyr::select(group_size, diet_PC1:diet_PC7) dim(baboon_first_c) ## [1] 62 8 Fit a linear regression model with the lm() function with group_size as the outcome and the principal components of diet variables (diet_PC) as your predictors. # fit model mod1 &lt;- lm(group_size~., baboon_first_c) summary(mod1) ## ## Call: ## lm(formula = group_size ~ ., data = baboon_first_c) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.092 -12.943 -2.438 15.847 44.974 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.22209 3.16188 22.525 &lt; 2e-16 *** ## diet_PC1 0.07252 0.09040 0.802 0.4259 ## diet_PC2 0.17070 0.23158 0.737 0.4643 ## diet_PC3 -0.52114 0.32445 -1.606 0.1141 ## diet_PC4 0.12835 0.40988 0.313 0.7554 ## diet_PC5 0.80878 0.73445 1.101 0.2757 ## diet_PC6 1.13856 0.53488 2.129 0.0379 * ## diet_PC7 4.36778 0.98211 4.447 4.37e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21.67 on 54 degrees of freedom ## Multiple R-squared: 0.4013, Adjusted R-squared: 0.3237 ## F-statistic: 5.17 on 7 and 54 DF, p-value: 0.0001502 Check the model assumptions of this linear regression model. What violations can you find? Hint: think about all assumptions, not just those that we can test/visualize. Can you think of how you would try to fix these violations? # multicollinearity? vif(mod1) ## diet_PC1 diet_PC2 diet_PC3 diet_PC4 diet_PC5 diet_PC6 diet_PC7 ## 1.051217 1.084132 1.092934 1.177352 1.814580 1.386233 1.504968 # linearity? pairs(baboon_first_c) # normality? qqPlot(mod1) ## sample_12052-TAGATCCTCGGA-408 sample_12050-TATGCCAGAGAT-407 ## 38 52 # constant variance? plot(resid(mod1)~predict(mod1)) The key violation here would likely be that we do not have independent observations as those within the same social group are more likely to be similar to one another in how they eat (and in this case, will have the same outcome). Other potential violations would be that there seems to be a bit of increasing variance in model residuals which could be because our outcome is a discrete count and Poisson might be more appropriate. Exercise 2 - Healthcare demand in U.S.A For this example we will look at the demand of healthcare in the U.S.A based on certain risk factors such as age, sex, income, etc. We will first load in the dataset, ‘healthcare_demand_nmes1988.csv’ from our folder. # 1. Load in the dataset. health &lt;- read.csv(&#39;module3_healthcare_demand_nmes1988.csv&#39;) # Healthcare demand dataset # 2. view data str(health) ## &#39;data.frame&#39;: 4406 obs. of 18 variables: ## $ visits : int 5 1 13 16 3 17 9 3 1 0 ... ## $ nvisits : int 0 0 0 0 0 0 0 0 0 0 ... ## $ ovisits : int 0 2 0 5 0 0 0 0 0 0 ... ## $ novisits : int 0 0 0 0 0 0 0 0 0 0 ... ## $ emergency: int 0 2 3 1 0 0 0 0 0 0 ... ## $ hospital : int 1 0 3 1 0 0 0 0 0 0 ... ## $ health : chr &quot;average&quot; &quot;average&quot; &quot;poor&quot; &quot;poor&quot; ... ## $ chronic : int 2 2 4 2 2 5 0 0 0 0 ... ## $ adl : chr &quot;normal&quot; &quot;normal&quot; &quot;limited&quot; &quot;limited&quot; ... ## $ region : chr &quot;other&quot; &quot;other&quot; &quot;other&quot; &quot;other&quot; ... ## $ age : num 6.9 7.4 6.6 7.6 7.9 6.6 7.5 8.7 7.3 7.8 ... ## $ gender : chr &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;male&quot; ... ## $ married : chr &quot;yes&quot; &quot;yes&quot; &quot;no&quot; &quot;yes&quot; ... ## $ school : int 6 10 10 3 6 7 8 8 8 8 ... ## $ income : num 2.881 2.748 0.653 0.659 0.659 ... ## $ employed : chr &quot;yes&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ insurance: chr &quot;yes&quot; &quot;yes&quot; &quot;no&quot; &quot;yes&quot; ... ## $ medicaid : chr &quot;no&quot; &quot;no&quot; &quot;yes&quot; &quot;no&quot; ... head(health) ## visits nvisits ovisits novisits emergency hospital health chronic adl ## 1 5 0 0 0 0 1 average 2 normal ## 2 1 0 2 0 2 0 average 2 normal ## 3 13 0 0 0 3 3 poor 4 limited ## 4 16 0 5 0 1 1 poor 2 limited ## 5 3 0 0 0 0 0 average 2 limited ## 6 17 0 0 0 0 0 poor 5 limited ## region age gender married school income employed insurance medicaid ## 1 other 6.9 male yes 6 2.8810 yes yes no ## 2 other 7.4 female yes 10 2.7478 no yes no ## 3 other 6.6 female no 10 0.6532 no no yes ## 4 other 7.6 male yes 3 0.6588 no yes no ## 5 other 7.9 female yes 6 0.6588 no yes no ## 6 other 6.6 female no 7 0.3301 no no yes # fix income (notice that some incomes are negative, we will set those to 0) health$income[health$income&lt;0] &lt;- 0 Choose an appropriate regression model if visits is our outcome and our predictors are health, chronic, adl, income, gender, age, insurance, medicaid. Additionally include an interaction term between income and gender. Fit this model to the data. # we will choose a Poisson regression given that the outcome is a discrete count glm_mod &lt;- glm(visits~health+chronic+adl+income*gender+age+insurance+medicaid, family=&quot;poisson&quot;, data=health) summary(glm_mod) ## ## Call: ## glm(formula = visits ~ health + chronic + adl + income * gender + ## age + insurance + medicaid, family = &quot;poisson&quot;, data = health) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.699105 0.085126 19.960 &lt; 2e-16 *** ## healthexcellent -0.353853 0.030311 -11.674 &lt; 2e-16 *** ## healthpoor 0.259091 0.018087 14.325 &lt; 2e-16 *** ## chronic 0.163046 0.004534 35.964 &lt; 2e-16 *** ## adlnormal -0.089349 0.016605 -5.381 7.41e-08 *** ## income -0.003374 0.003104 -1.087 0.277 ## gendermale -0.137261 0.017031 -8.060 7.66e-16 *** ## age -0.062546 0.010478 -5.969 2.38e-09 *** ## insuranceyes 0.378897 0.018923 20.023 &lt; 2e-16 *** ## medicaidyes 0.248354 0.024639 10.080 &lt; 2e-16 *** ## income:gendermale 0.017089 0.004123 4.145 3.40e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 26943 on 4405 degrees of freedom ## Residual deviance: 23818 on 4395 degrees of freedom ## AIC: 36615 ## ## Number of Fisher Scoring iterations: 5 Interpret the interaction term between income and gender Income has a larger association with nummber of visits for males such that males with higher income are likely to have more visits while income does not have an effect on visits for females. Check model assumptions (also look for overdispersion if appropriate for the model you chose) - note that you cannot use crPlot since we included an interaction so we can assume the linearity assumption is met for now. # constant variance of deviance residuals? plot(resid(glm_mod)~predict(glm_mod)) # overdispersed? glm_summary &lt;- summary(glm_mod) glm_summary$deviance/glm_summary$df.residual # overdispersed ## [1] 5.419233 Fix any model violations you may have noticed. # we will log transform income (need to add 0.1 since we cannot take log of 0) health$income_log &lt;- log(health$income+0.1) # Fitting quasipoisson to account for overdisperion glm_mod1 &lt;- glm(visits~health+chronic+adl+income_log*gender+age+insurance+medicaid, family=&quot;quasipoisson&quot;, data=health) summary(glm_mod1) ## ## Call: ## glm(formula = visits ~ health + chronic + adl + income_log * ## gender + age + insurance + medicaid, family = &quot;quasipoisson&quot;, ## data = health) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.68935 0.22585 7.480 8.93e-14 *** ## healthexcellent -0.35470 0.08038 -4.413 1.05e-05 *** ## healthpoor 0.26340 0.04803 5.484 4.40e-08 *** ## chronic 0.16305 0.01203 13.555 &lt; 2e-16 *** ## adlnormal -0.08755 0.04408 -1.986 0.047067 * ## income_log -0.01862 0.02621 -0.710 0.477459 ## gendermale -0.16838 0.04704 -3.579 0.000348 *** ## age -0.06075 0.02784 -2.182 0.029129 * ## insuranceyes 0.37319 0.05050 7.390 1.74e-13 *** ## medicaidyes 0.24667 0.06602 3.736 0.000189 *** ## income_log:gendermale 0.10583 0.04323 2.448 0.014410 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 7.032885) ## ## Null deviance: 26943 on 4405 degrees of freedom ## Residual deviance: 23794 on 4395 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 Advanced exercise - Multivariate count data # we will log transform income (need to add 0.1 since we cannot take log of 0) health$income_log &lt;- log(health$income+0.1) # Fitting quasipoisson to account for overdisperion glm_mod1 &lt;- glm(visits~health+chronic+adl+income_log*gender+age+insurance+medicaid, family=&quot;quasipoisson&quot;, data=health) summary(glm_mod1) ## ## Call: ## glm(formula = visits ~ health + chronic + adl + income_log * ## gender + age + insurance + medicaid, family = &quot;quasipoisson&quot;, ## data = health) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.68935 0.22585 7.480 8.93e-14 *** ## healthexcellent -0.35470 0.08038 -4.413 1.05e-05 *** ## healthpoor 0.26340 0.04803 5.484 4.40e-08 *** ## chronic 0.16305 0.01203 13.555 &lt; 2e-16 *** ## adlnormal -0.08755 0.04408 -1.986 0.047067 * ## income_log -0.01862 0.02621 -0.710 0.477459 ## gendermale -0.16838 0.04704 -3.579 0.000348 *** ## age -0.06075 0.02784 -2.182 0.029129 * ## insuranceyes 0.37319 0.05050 7.390 1.74e-13 *** ## medicaidyes 0.24667 0.06602 3.736 0.000189 *** ## income_log:gendermale 0.10583 0.04323 2.448 0.014410 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 7.032885) ## ## Null deviance: 26943 on 4405 degrees of freedom ## Residual deviance: 23794 on 4395 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 Advanced exercise - Multivariate count data Often in gene expression data or compositional data, we have multiple counts. The package MGLM in R offers functions to fit multivariate count models such as the multinomial model and more advanced models to handle overdispersion of these counts. Let’s look again at the rna seq dataset we used for the negative binomial example. # load the dataset data(rnaseq) # look at description of the dataset ?rnaseq # look at first 6 rows head(rnaseq) ## X1 X2 X3 X4 X5 X6 totalReads treatment gender age ## 1 295 65 19 114 54 20 28317494 0 0 57 ## 2 213 126 12 96 50 4 20015549 0 0 67 ## 3 322 147 23 245 42 19 35318251 0 1 37 ## 4 184 35 9 134 74 8 20421437 0 0 59 ## 5 376 104 4 74 39 7 21940693 0 1 57 ## 6 490 97 18 282 159 24 48645477 0 1 50 # look at summary of variables summary(rnaseq) ## X1 X2 X3 X4 ## Min. : 29.0 Min. : 15.00 Min. : 3.0 Min. : 54.0 ## 1st Qu.:132.8 1st Qu.: 43.00 1st Qu.: 15.0 1st Qu.:138.8 ## Median :191.0 Median : 66.00 Median : 32.0 Median :201.5 ## Mean :215.6 Mean : 73.72 Mean : 68.1 Mean :242.6 ## 3rd Qu.:275.8 3rd Qu.: 96.25 3rd Qu.:109.2 3rd Qu.:320.5 ## Max. :711.0 Max. :186.00 Max. :259.0 Max. :684.0 ## X5 X6 totalReads treatment ## Min. : 18.00 Min. : 2.00 Min. :20015549 Min. :0.0 ## 1st Qu.: 40.00 1st Qu.: 8.00 1st Qu.:26675268 1st Qu.:0.0 ## Median : 58.00 Median :12.50 Median :33177688 Median :0.5 ## Mean : 65.25 Mean :14.69 Mean :33748425 Mean :0.5 ## 3rd Qu.: 81.00 3rd Qu.:19.00 3rd Qu.:40486800 3rd Qu.:1.0 ## Max. :183.00 Max. :41.00 Max. :49813815 Max. :1.0 ## gender age ## Min. :0.000 Min. :26.00 ## 1st Qu.:0.000 1st Qu.:43.00 ## Median :1.000 Median :50.00 ## Mean :0.525 Mean :49.69 ## 3rd Qu.:1.000 3rd Qu.:56.25 ## Max. :1.000 Max. :77.00 Look up the function MGLMreg to learn about it. What distributions can you fit with this function? Multinomial (dist=“MN”), Dirichlet-multinomial (dist=“DM”), generalized Dirichlet-multinomial (dist=“GDM”), and negative multinomial (dist=“NM”). Let’s fit a multinomial regression to the 6 gene expression columns with gender, age, and treatment as our covariates. We will include log(totalreads) as well. # the multinomial outcome model would assume NO overdispersion: mnreg &lt;- MGLMreg(formula = cbind(X1, X2, X3, X4, X5, X6) ~ log(totalReads) + treatment + age + gender, data = rnaseq, dist = &quot;MN&quot;) print(mnreg) ## Call: MGLMreg(formula = cbind(X1, X2, X3, X4, X5, X6) ~ log(totalReads) + ## treatment + age + gender, data = rnaseq, dist = &quot;MN&quot;) ## ## Coefficients: ## X1 X2 X3 X4 X5 ## [1,] 4.942732 5.009649 -3.792216 4.435434 4.027689 ## [2,] -0.112845 -0.170222 0.219277 -0.107260 -0.120928 ## [3,] -0.022655 -0.043099 2.745277 1.405742 0.092246 ## [4,] -0.006187 -0.009709 -0.005907 -0.010945 -0.009599 ## [5,] 0.032676 0.100389 0.020663 0.103859 0.009514 ## ## Hypothesis test: ## wald value Pr(&gt;wald) ## (Intercept) 144.88789 1.634268e-29 ## log(totalReads) 69.92572 1.061922e-13 ## treatment 18364.13260 0.000000e+00 ## age 79.91023 8.762650e-16 ## gender 52.33670 4.601575e-10 ## ## Distribution: Multinomial ## Log-likelihood: -7506.393 ## BIC: 15145.24 ## AIC: 15062.79 ## Iterations: 6 Each row of the coefficient matrix in this output corresponds to our covariates (intercept, log(totalReads), treatment, age, gender) and each column corresponds to our outcomes (X1,…,X6). For example, the coefficient in the first row and first column (4.943) estimates the intercept of X1 which gives the log of baseline expression levels. The second row/first column coefficient (-0.113) gives the estimated association between log(totalReads) and gene expression of X1. Given that this dataset is overdispersed, it is likely that we will need to use a different distribution to accommodate this. Choose one of the distributions available other than “MN” # we can change &quot;MN&quot; in the previous code to &quot;DM&quot; to fit a Dirichlet-multinomial regression dmreg &lt;- MGLMreg(formula = cbind(X1, X2, X3, X4, X5, X6) ~ log(totalReads) + treatment + age + gender, data = rnaseq, dist = &quot;DM&quot;) print(dmreg) ## Call: MGLMreg(formula = cbind(X1, X2, X3, X4, X5, X6) ~ log(totalReads) + ## treatment + age + gender, data = rnaseq, dist = &quot;DM&quot;) ## ## Coefficients: ## X1 X2 X3 X4 X5 X6 ## [1,] -0.895850 -1.096921 -8.997414 -1.736871 -1.774227 -5.646822 ## [2,] 0.221988 0.186919 0.536572 0.252679 0.216672 0.347271 ## [3,] -0.679291 -0.686881 1.835585 0.707954 -0.546469 -0.543134 ## [4,] 0.010245 0.005227 0.009134 0.004252 0.006090 0.011642 ## [5,] -0.026177 0.040244 -0.052842 0.023178 -0.058339 -0.039139 ## ## Hypothesis test: ## wald value Pr(&gt;wald) ## (Intercept) 14.579069 0.02379579 ## log(totalReads) 8.502549 0.20354699 ## treatment 1851.437449 0.00000000 ## age 13.131512 0.04099442 ## gender 4.133364 0.65863419 ## ## Distribution: Dirichlet Multinomial ## Log-likelihood: -4386.941 ## BIC: 8932.831 ## AIC: 8833.882 ## Iterations: 9 If we wanted to know which model provided a better fit, we could compare their information criterion. For instance, the Bayesian Information Criterion (BIC) is designed to balance model complexity and model fit. A lower BIC indicates a better fitting model. Let’s compare the BIC of our two models fit using the BIC() function in R. Which model fits better for this data? BIC(dmreg) ## [1] 8932.831 BIC(mnreg) ## [1] 15145.24 "],["module-4-modeling-part-2.html", "Module 4: Modeling Part 2 Lecture Lab", " Module 4: Modeling Part 2 Lecture Lab "],["module-5-putting-it-all-together.html", "Module 5: Putting it all Together Lecture Lab", " Module 5: Putting it all Together Lecture Lab "],["module-6-introduction-to-causal-intference.html", "Module 6: Introduction to Causal Intference Lecture Lab", " Module 6: Introduction to Causal Intference Lecture Lab "]]
